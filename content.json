{"meta":{"title":"君霖的blog","subtitle":"","description":"","author":"Bean","url":"http://example.com","root":"/"},"pages":[{"title":"关于我","date":"2021-05-18T12:28:45.000Z","updated":"2021-05-18T17:46:55.147Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"一个简单的打工人."}],"posts":[{"title":"浅谈快速排序","slug":"快速排序","date":"2022-09-13T19:09:00.000Z","updated":"2022-09-13T19:21:19.531Z","comments":true,"path":"2022/09/14/快速排序/","link":"","permalink":"http://example.com/2022/09/14/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","excerpt":"","text":"浅谈快速排序好久没有写文章了,今天颇有兴趣想写篇文章.现在越发觉得一个技术人员的技术水平的提高,是需要深度的思考的.比如我们学习算法,如果仅仅只是刷题,不去深度思考为什么这么写的话,可能在当时参考一些答案能写出来,过一段时间可能又忘记了,那实际上,还是没有真的搞懂,如果真的搞懂了,那其实应该过一段时间,也能很容易的写出来,所以对于一些技术知识,要反复思考,反复实践和总结,形成自己独特的知识和看法,这样理解起来更深刻,对于提升自己的技术水平是很有益处的.学习技术是不容易的一件事情,这跟其它领域的很多事情是一样的,修行的路上必定充满了汗水,只有不断的承受压力,苦中作乐,突破自己,才能取得更大的成功,一定要刻意的去关注自己的理解程度,对知识的掌握程度,形成深度思考的能力,这样必能在人生路上一路高歌猛进,无论是生活还是工作上. 快速排序代码12345678910111213141516171819202122public static void quickSort(int [] nums,int i,int j) &#123; if(i&gt;=j)&#123; return; &#125; int start = i; int end = j; int basic = nums[i]; while(i &lt; j) &#123; while(i&lt;j &amp;&amp; nums[j] &gt;= basic) &#123; j--; &#125; while(i&lt; j &amp;&amp; nums[i] &lt;= basic) &#123; i++; &#125; if(i&lt;j) &#123; ArrayUtil.swap(nums, i, j); &#125; &#125; ArrayUtil.swap(nums,i,start); quickSort(nums,start,i-1); quickSort(nums,i+1,end); &#125; 首先,上面是一段快速排序的代码,我认为快速排序最重要的是三件事 取基准点 找中心值 递归的结束条件 从最简单的说起,首先基准点通常取数组的首元素.其次,递归的结束条件是i&gt;=j,这表示此时要排序的数组只有一个元素,这是许多递归算法的一个结束条件.最后,i始终不能大于j,因为我们要从这个地方去取得中心值,即i=j的那个地方,和基准点交换.如果i遇到了j,就表明此次已经找到了一个中心点,这个点让左边的数小于中心值,右边的数大于中心值,之后和数组首元素调换,继续递归左右两边的数组. 快速排序看似简单,实则容易遗忘,但本质上还是没有理解透彻,对于复杂的知识,一定要反复练习,反复实践,直到烂熟于心.","categories":[],"tags":[]},{"title":"Kafka的ack机制","slug":"Kafka的ack机制","date":"2021-07-07T16:58:31.000Z","updated":"2021-07-07T17:09:18.152Z","comments":true,"path":"2021/07/08/Kafka的ack机制/","link":"","permalink":"http://example.com/2021/07/08/Kafka%E7%9A%84ack%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Kafka的ack机制在博客中有一篇关于Kafka消息丢失和消息重复的文章,已经有对ack进行了讨论,这里再把这个概念拿出来单独说说. Kafka的ack机制实际上指的是生产者的ack配置,不同的配置对消息的处理方式不同,配置得越严格消息越不容易丢失,主要有以下几种配置: ack=0,消息一旦发送出去,就认为是发送成功了,即使Broker没有接收到消息. ack=1,一旦首领接收到消息,那么会收到发送成功的响应.但是首领有可能在消息同步到其它副本前发生崩溃,其它副本成为新的首领(即使禁用了不完全的首领选举),所以这个配置还是有可能导致消息丢失的. ack=all,这里的all等于Broker端配置的min.insync.replicas的个数,如果有等于这个个数的副本接收到消息,才能收到成功响应,但是要保证消息不可能丢失,应该是要保证所有副本都能收到消息,所以要使得min.insync.replicas等于副本个数.","categories":[],"tags":[],"author":"John Doe"},{"title":"Kafka消息丢失和消息重复,解决方案","slug":"Kafka什么时候出现消息丢失和消息重复","date":"2021-06-30T07:17:00.000Z","updated":"2021-07-07T16:50:47.529Z","comments":true,"path":"2021/06/30/Kafka什么时候出现消息丢失和消息重复/","link":"","permalink":"http://example.com/2021/06/30/Kafka%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%87%BA%E7%8E%B0%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%E5%92%8C%E6%B6%88%E6%81%AF%E9%87%8D%E5%A4%8D/","excerpt":"","text":"Kafka什么时候出现消息丢失和消息重复,解决方案消息队列的参与者无非是三个:生产者、Broker以及消费者.消息丢失和消息重复在这三个参与者当中都会出现,本篇文章以这三者的角度来叙述消息丢失和消息重复问题. 生产者消息丢失如果生产者往Broker发送消息,没有等到Broker的回复,就认为是成功了,那么就可能存在生产者的消息丢失.可能由于网络原因导致消息没有到达Broker或是其它一些异常情况.生产者发送消息有一个发送确认的概念,使用ack来进行配置. 当设置ack=0时,消息一旦从生产者端发送出去就认为是成功了.这种情况就很有可能出现消息丢失. 当设置ack=1时,会等到首领收到消息,并返回结果才算是发送成功.这种情况认为生产者已经发送成功了,对于生产者端来说不会出现消息丢失,而对于Broker来说可能存在消息丢失,主要和同步副本有关系,这个将在下面讲解. 当设置ack=all时,会等到首领及首领配置的最小同步副本都接收到消息,那么才算是发送成功,Broker通过min.insync.replics来进行配置,比如这个属性配置成2,那么要至少有2个Broker接收到消息才算成功,不会出现消息丢失,除此之外更加严格的确保消息在副本之间的一致性. 所以如果要让消息不丢失,那么可以设置ack=1或者ack=all. 消息重复消息重复是无法避免的情况,即使消息真正存放到Broker之中,Broker返回的响应结果也有可能由于网络原因出现丢失或者超时的可能性.这个时候生产者误以为自己没有发送成功,那么就有可能会重试发送消息,Broker再次接收到消息,那么消息就发生了重复. 这种消息重复只能在Broker端或者消费者端做逻辑上的去重处理. Broker消息丢失在Broker端存在三个很重要的概念:复制系数,不完全的首领选举及最小同步副本. 复制系数复制系数表示一个分区有多少个副本,通过replication.factor来进行配置,很明显这个值配置得越大,消息越不容易丢失,但是复制带来的性能损耗也越大. 不完全的首领选举不完全的首领选举表示在进行首领选举时,是否允许非同步副本成为首领,一般通过unclean.leader.election进行配置,如果配置成true,那么可能出现首领宕机,但其它副本并没有同步完成时成为新的首领,那么就可能出现消息丢失.如果配置成false,那么分区在旧首领重启之前就是不可用的,这种情况不会出现消息丢失. 但是可能出现这么一种情况,消息写入到首领之后,还没有同步到其它副本中,此时首领宕机了,但是其它副本还是认为自己是同步的,还是会进行首领选举产生新首领,所以使用这个配置并不能完全防止消息丢失,只能认为它具有一定的防止消息丢失的作用. 最小同步副本最小同步副本是在Broker端对消息同步副本个数的约束,一般通过min.insync.replicas进行配置.比如配置成2,那么要保证至少有两个同步副本时分区才能对外进行写服务,否则只能提供读服务,将最小同步副本设置成分区副本的总个数,那么在Broker端就不会出现消息丢失. 所以要让消息不丢失,可以禁用不完全的首领并且或者并且最小同步副本个数为分区副本个数. 消息重复在Broker端出现消息重复的根本原因是生产者重复发送导致的.对于Broker来说,可以根据Broker本身提供的幂等功能来进行去重. 消费者消息丢失消费者一般通过轮询的方式来获取消息,消息消费成功后就提交偏移量.如果消费者没有消费成功,但是提交了偏移量,那么就存在消息丢失的可能性,下一次拉取的消息就不会有本次消费失败的消息. 对于这种情况,要确保消息处理成功再提交偏移量,或者对于没有处理成功的消息,保存到数据库或者缓存中,稍后再进行处理. 消息重复消费者如果没有成功提交偏移量或者提交了已处理成功的消息的偏移量,那么就有可能导致消息重复.对于这种情况,要确保消息处理和偏移量提交的原子性(将偏移量作为数据库表的列,通过事务处理),或者在处理消息前判断数据库中是否已经有该消息对应的记录. 除此之外,如果Broker没有对重复的消息进行去重,消费者也置之不理,那么也会出现消息重复.对于这种情况,要根据业务标识做幂等处理.","categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"author":"John Doe"},{"title":"Zookeeper节点类型及特点","slug":"Zookeeper节点类型及特点","date":"2021-06-29T17:35:00.000Z","updated":"2021-06-30T09:34:17.850Z","comments":true,"path":"2021/06/30/Zookeeper节点类型及特点/","link":"","permalink":"http://example.com/2021/06/30/Zookeeper%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B%E5%8F%8A%E7%89%B9%E7%82%B9/","excerpt":"","text":"Zookeeper节点类型及特点Zookeeper共有四种节点类型: 持久节点.持久节点指的是在Zookeeper上进行持久化的节点,除非主动进行删除,否则节点会一直存在. 持久顺序节点.在持久节点的基础上,添加了顺序.比如创建一个持久节点,那么会自动的在路径的末尾添加一个序列号.比如连续创建两次/A/B,并表示创建的是顺序节点,那么会创建/A/B0000000000和/A/B0000000001这两个持久顺序节点,如果创建的是/A/C,那么会创建/A/C0000000002,这是因为有一个父节点在维护这些顺序节点的顺序(从0开始递增),只要是创建带有顺序的子节点,都会使用这个顺序来作为节点的后缀. 临时节点.临时节点指的是会话结束后会被删除的节点.临时节点不能有子节点,所以临时节点一定是叶子节点. 临时顺序节点.在临时节点的基础上,添加了顺序,其创建出来的节点路径规律和持久顺序节点是类似的.","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/categories/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/tags/zookeeper/"}],"author":"John Doe"},{"title":"CAP定理","slug":"CAP定理","date":"2021-06-29T08:36:00.000Z","updated":"2021-06-30T09:34:05.177Z","comments":true,"path":"2021/06/29/CAP定理/","link":"","permalink":"http://example.com/2021/06/29/CAP%E5%AE%9A%E7%90%86/","excerpt":"","text":"CAP定理CAP定理指的是在分布式环境中,只可能满足一致性、可用性、分区容错性中的其中两个,不可以三个都满足. 一致性:这里的一致性指的是强一致性,如果节点有副本,那么一旦一个节点的数据进行更新之后,那么在另外一个节点能立即获取到更新后的值. 可用性:可用性指的是在有限的时间返回正确的结果.对于不同系统来说,有限的时间是根据具体的应用场景来定义的一个合理的指标,比如对于搜索引擎而言,这个有限的时间就是一个比较短暂的时间,比如100ms.对于一个离线处理任务来说,可能长达几分钟或者几个小时.正确的结果是一个对用户来说看的懂或者说有意义的结果,比如下单成功或失败,而不是NullPointer Exception这种结果. 分区容错性:网络分区指的是两个网络之间由于网络问题,导致不同节点无法进行通信,各自形成一个子网络.分区容错性指的是即使出现网络分区,系统也能提供具有一致性和可用性的服务. 因为是在分布式环境下,那么网络分区是一个必然会出现的问题,所以在设计系统时,一般考虑的是系统的一致性和可用性.","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"author":"John Doe"},{"title":"分布式环境中的问题","slug":"分布式环境中的问题","date":"2021-06-29T08:24:00.000Z","updated":"2021-06-29T08:36:11.904Z","comments":true,"path":"2021/06/29/分布式环境中的问题/","link":"","permalink":"http://example.com/2021/06/29/%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"分布式环境中的问题现在几乎所有的软件都是分布式环境,那么分布式环境有什么问题呢? 通信问题.如果是单体应用,那么所有的程序逻辑都会在一台机器上进行处理,一般不会出现网络通信问题.但是在分布式环境,会涉及到不同机器之间的通信,所以可能会出现类似网络丢包等问题. 机器故障.故障一般指的是机器宕机或者僵死.如果是单体应用,那么故障之后会造成服务不可用,重启之后又可以继续进行服务.在分布式环境下,故障问题就可能发生在所有的机器上. 网络分区.在分布式环境中,机器可能部署在不同的网络环境中,有可能出现机器本身运行正常但网络出现分区的情况,即两个网络之间的通信链路出现了问题. 三态问题.如果是单体应用,那么一般来说一个请求的处理结果要么是成功或者失败.如果是分布式环境,那么可能由于网络原因出现第三种处理结果-超时.","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"author":"John Doe"},{"title":"Zookeeper和Redis实现分布式锁的区别","slug":"Zookeeper和Kafka实现分布式锁的区别","date":"2021-06-29T03:23:00.000Z","updated":"2021-06-29T03:34:27.545Z","comments":true,"path":"2021/06/29/Zookeeper和Kafka实现分布式锁的区别/","link":"","permalink":"http://example.com/2021/06/29/Zookeeper%E5%92%8CKafka%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Zookeeper和Redis实现分布式锁的区别在网上看了关于Zookeeper和Redis实现分布式锁的区别的一些文章,感觉可能有的文章写的这两者的区别可能跟我的理解有点出入,所以这里按照自己的理解来谈谈它们的区别: Zookeeper实现分布式锁时,除了可以使用和redis类似的独占锁的思路,还可以监听节点变更事件,在锁可能可以获取到的情况下通知客户端再次获取锁. Redis在获取锁时,可能需要设置过期时间,而Zookeeper通常是设置一个临时节点,在会话过期的时候自动释放锁. 还有一些其它区别暂时还没有学习到,可能是一些算法设计或者一致性方面的内容,如果以后涉及到这部分内容,再进行补充.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"John Doe"},{"title":"观察者模式","slug":"观察者模式","date":"2021-06-25T11:49:00.000Z","updated":"2021-06-25T12:01:22.011Z","comments":true,"path":"2021/06/25/观察者模式/","link":"","permalink":"http://example.com/2021/06/25/%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"观察者模式观察者模式解决的是一种关于发布/订阅的场景,是观察者和通知者的一种交互方式.举个例子:现在很流行订阅APP的消息,那么APP就相当于通知者,我们的手机相当于观察者,当APP有消息要发布时,会遍历所有它的观察者进行消息的发送.在实际编码中,我们在描述这种关系时可能要注意将观察者抽象化,或者将通知者抽象化,如果不抽象化我们也能描述这种发布/订阅的关系,只不过代码的扩展性比较差. 代码示例https://github.com/CodeShowZz/code-repository/tree/master/design-pattern-demo/src/main/java/com/observer 这里有一个简单的代码示例,将观察者和通知者进行了一定的抽象,实际应用场景中可以是使用接口进行抽象,或者使用抽象类进行抽象,总之就是代码的具体实现可能是多种多样的,重要的事情在于要把模式体现出来和要使程序具有扩展性即可,不能拘泥于某种代码写法.","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"单例模式","slug":"单例模式","date":"2021-06-25T09:11:00.000Z","updated":"2021-06-25T10:26:32.824Z","comments":true,"path":"2021/06/25/单例模式/","link":"","permalink":"http://example.com/2021/06/25/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"单例模式单例模式是最简单的设计模式,它用来解决一个类只可以有一个唯一实例的问题,该类提供一个访问该实例的方法.单例模式需要注意的一个点是要保证在多线程的环境下,也能保证单例. 懒汉式单例如果我们在需要单例的情况再去初始化它,则叫做懒汉式单例. 通常我们会这么写一个单例模式: 123456789101112131415161718public class Singleton &#123; private static Singleton singleton; private Singleton() &#123; &#125; private Singleton getSingleton() &#123; synchronized (Singleton.class) &#123; if(singleton == null) &#123; singleton = new Singleton(); &#125; &#125; return singleton; &#125;&#125; 这么做有一个坏处,就是每次都会有锁的开销,进而有人发明了双重检查的写法: 1234567891011121314151617181920public class Singleton2 &#123; private static Singleton2 singleton; private Singleton2() &#123; &#125; private Singleton2 getSingleton() &#123; if(singleton ==null) &#123; synchronized (Singleton2.class) &#123; if (singleton == null) &#123; singleton = new Singleton2(); &#125; &#125; &#125; return singleton; &#125;&#125; 但是这种写法是有问题的,这里主要的问题在于可见性问题,有可能A线程执行了new操作,但是对象没有完全被构建,B线程获取到了这个没有完全被构建完成的对象,这样是不安全的.进而有大神发明了一个更牛逼的写法: 12345678910111213141516public class SingletonFactory &#123; public Singleton3 getSingleton() &#123; return SingletonHolder.singleton3; &#125; private static class SingletonHolder &#123; public static Singleton3 singleton3 = new Singleton3(); &#125; private static class Singleton3 &#123; private Singleton3() &#123; &#125; &#125;&#125; 这种写法能做到懒加载方式,又能保证线程安全,堪称单例模式的最佳写法. 恶汉式如果我们在程序的初始阶段就实例化单例,则叫做恶汉式单例. 写法也是简单粗暴: 123456789101112public class Singleton4 &#123; private Singleton4 singleton = new Singleton4(); private Singleton4() &#123; &#125; public Singleton4 getSingleton() &#123; return singleton; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"Mac Os Open JDK编译","slug":"Mac Os Open JDK编译","date":"2021-06-25T03:47:00.000Z","updated":"2021-06-29T07:33:34.652Z","comments":true,"path":"2021/06/25/Mac Os Open JDK编译/","link":"","permalink":"http://example.com/2021/06/25/Mac%20Os%20Open%20JDK%E7%BC%96%E8%AF%91/","excerpt":"","text":"Open JDK编译我们经常都会看一些源码,但有没有想过动手修改源代码呢?在平时的开发环境中,是无法直接修改源码的,如果要修改源码,那么就要获取源代码进行编译,想象一下我们可以修改源码,然后在看源码的时候加上一些注释,仿佛成为JDK的开发人员那样,是不是很有意思?编译JDK是一个比较繁琐的过程,这里以Mac OS系统为例. 第一步:下载Open JDK11 进入页面 https://adoptopenjdk.net/installation.html?variant=openjdk11&amp;jvmVariant=hotspot#x64_mac-jdk 下载 tar.gz 包 解压到/Library/Java/JavaVirtualMachines,如果你有其它版本的JDK则不需要设置环境变量,否则需要设置. 第二步:下载XCode编译过程需要使用到XCode.在我的系统里面,App Store的XCode版本并不兼容我的系统,所以要找到兼容本系统的历史版本进行下载.可以在https://developer.apple.com/download/all/?q=Xcode%2011.7下载. 第三步:下载Open JDK11源码在http://hg.openjdk.java.net/jdk-updates/jdk11u/下载zip包,解压到一个英文目录中. 第四步:编译进入到下载的jdk源码目录中,使用命令sh configure --with-target-bits=64 --enable-ccache --with-jvm-variants=server --with-boot-jdk-jvmargs=&quot;-Xlint:deprecation -Xlint:unchecked&quot; --disable-warnings-as-errors --with-debug-level=slowdebug 2&gt;&amp;1 | tee configure_mac_x64.log 进行编译. 在我的实际编译中,遇到了这样一个错误: 12configure: error: No xcodebuild tool and no system framework headers found, use --with-sysroot or --with-sdk-name to provide a path to a valid SDK/Users/huangjunlin/IdeaProjects/jdk11u-113c646a33d2/build/.configure-support/generated-configure.sh: line 82: 5: Bad file descriptor 此时应执行sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer后再运行上面的命令.如果编译成功,将出现类似如下代码输出: 1234Build performance summary:* Cores to use: 4* Memory limit: 8192 MB* ccache status: Active (3.7.1) 之后在这个目录使用make命令进行编译,整个过程可能会耗费一定时间,如果执行命令有问题,记得先加上sudo进行尝试,还有就是如果卡在某个地方,请耐心等待,大部分情况最终都会向下执行的,不要一开始就认为它挂了.等到出现 1Finished building target &#x27;default (exploded-image)&#x27; in configuration &#x27;macosx-x86_64-normal-server-slowdebug&#x27; 在build目录出现macosx-x86_64-normal-server-slowdebug/jdk文件夹,那么编译就完成了. 第五步:指定Idea的JDK创建一个Idea项目,在Idea的Project Structure指定SDK为刚刚编译出来的jdk,路径类似为/jdk11u-113c646a33d2/build/macosx-x86_64-normal-server-slowdebug/jdk. 第六步:下载CLionClion是一个C/C++的一个开发工具,我们要修改JDK的源码,可以借助这个工具来进行修改,这个工具跟Idea的风格很像,下载也很简单.下载完成之后,使用它打开jdk11u-113c646a33d2/src目录,我们就是在导入的文件中修改源码. 第七步:修改源码并测试找到java.c文件,添加一条输出语句,片段如下: 12345678910intJavaMain(void* _args)&#123; JavaMainArgs *args = (JavaMainArgs *)_args; printf(&quot;修改open jdk&quot;); int argc = args-&gt;argc; char **argv = args-&gt;argv; int mode = args-&gt;mode; char *what = args-&gt;what; InvocationFunctions ifn = args-&gt;ifn; 重新运行make命令,进行重新编译,这次编译速度相对来说会比较快.然后在Idea用main方法进行测试: 123456public class MyTest &#123; public static void main(String[] args) &#123; &#125;&#125; 结果输出了修改open jdk; 在网上还有一些关于在Clion打断点的文章,但好像对.java上打断点没有过多的描述,我认为对看源码参考价值不大,这部分内容后续还要再研究一下. 感觉整个过程遇到了好多坑,编译JDK还是比较麻烦的.","categories":[{"name":"其它","slug":"其它","permalink":"http://example.com/categories/%E5%85%B6%E5%AE%83/"}],"tags":[{"name":"其它","slug":"其它","permalink":"http://example.com/tags/%E5%85%B6%E5%AE%83/"}],"author":"John Doe"},{"title":"Java内存模型","slug":"Java内存模型","date":"2021-06-24T17:39:00.000Z","updated":"2021-06-25T10:24:38.506Z","comments":true,"path":"2021/06/25/Java内存模型/","link":"","permalink":"http://example.com/2021/06/25/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Java内存模型Happen-Before规则Happen-Before规则可以让我们不考虑Java内存模型的细节,通过这些规则我们能让程序实现类似于串行化的语义(包括可见性).这样可以不需要去过多关注内存模型的复杂性.但是我发现单单在字面上去理解这些规则有点困难,如果能通过一个程序来对这些规则进行一个验证,那么理解起来可能更深刻. 程序顺序规则规则:如果程序中操作A在操作B之前,那么在线程中A操作将在B操作之前. 我认为这个规则字面上看来确实不好理解,其实可以换一种说法:在一个线程执行的代码顺序跟源代码中执行的代码顺序是一致的(可能不相关的操作还是会重排序,这里不考虑这种情况).比如有下面这样一个程序: 123456789101112131415161718192021222324252627282930313233343536373839public class JmmDemo &#123; private static Object lock = new Object(); private static int i = 0; public static void main(String[] args) throws InterruptedException &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while (true) &#123; synchronized (lock) &#123; System.out.println(&quot;线程1操作A&quot;); System.out.println(&quot;线程1操作B&quot;); System.out.println(&quot;线程1操作C&quot;); System.out.println(&quot;线程1操作D&quot;); &#125; &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; while (true) &#123; synchronized (lock) &#123; System.out.println(&quot;线程2操作A&quot;); System.out.println(&quot;线程2操作B&quot;); System.out.println(&quot;线程2操作C&quot;); System.out.println(&quot;线程2操作D&quot;); &#125; &#125; &#125; &#125;).start(); &#125;&#125; 这个程序有两个线程,在具体执行过程中,要么做线程1的ABCD操作,要么做线程2的ABCD操作,不会存在交错输出(比如线程1的ACBD)的可能性,但是可能会存在连续多次线程A或线程B的操作. 监视器锁规则规则:在监视器锁上的解锁操作必须在同一个监视器锁上的加锁操作之前执行. 这个规则从字面上看还是无法看出它想表达的意思,在我看来,这个规则其实是在强调一种可见性,可以理解成一个线程获取到了另一个线程释放的锁,那么另一个线程做的所有操作对于这个线程来说都是可见的.这里通过一个程序证明: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class JmmDemo &#123; private static Object lock = new Object(); private static int i = 0; public static void main(String[] args) throws InterruptedException &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for (int x = 0; x &lt; 10; x++) &#123; synchronized (lock) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; i++; &#125; &#125; System.out.println(&quot;a:&quot; + i); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int x = 0; x &lt; 10; x++) &#123; synchronized (lock) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; i++; &#125; &#125; System.out.println(&quot;b:&quot; + i); &#125; &#125;).start(); &#125;&#125; 这里还是有两个线程,两个线程都在执行完之后打印i的值,两个都执行i的递增操作10次.这个程序无论怎么执行,第一次打印的值不确定,但是第二次打印的值肯定是20,第二次打印的值可能来自第一个线程,也可能来自第二个线程.这也证明了这两个线程会交替执行,由于都在各自的递增操作中加了同步,所以能看到各自对i的修改,进而保证了可见性. volatile变量规则规则:对volatile变量的写入操作必须在对该变量的读操作之前执行. 这个规则描述得就更让人费解了,这里还需要再强调一点,线程本来就是交互执行的,写入操作为什么一定会在读操作之前执行呢?应该理解成在对volatile变量进行读写操作时,后续的读操作对前面写操作的值是可见的. 在写验证程序的时候,我发现很难写一个程序去验证这一点,因为volatile无法保证原子性,平时也只用在类似于对boolean值更新的场景下. 传递性如果操作A在操作B之前执行,并且操作B在操作C之前执行,那么操作A必须在操作C之前执行. 这个还是要换种说法,如果A的操作对B可见,B的操作对C可见,那么操作A对C也是可见的. 线程启动规则/线程结束规则/中断规则/终结器规则我发现这四个规则用得比较少,而且规则本身也非常难以理解,所以我认为可以先忘记这几个规则,等哪一天真的遇到这方面的问题了,再深入研究它. 总结在我看来,Happens-Before规则本身就比较拗口难懂,规则中用了许多的’之前’,其实它表达的意思是如果程序是这么执行的话,那么前者对后者是可见的,而不是说前者一定比后者先被CPU处理.多写一些程序去验证这些规则比从字面上去理解会好得多.","categories":[{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"}],"author":"John Doe"},{"title":"Java锁的优化","slug":"锁的优化","date":"2021-06-24T11:05:00.000Z","updated":"2021-06-25T10:24:54.402Z","comments":true,"path":"2021/06/24/锁的优化/","link":"","permalink":"http://example.com/2021/06/24/%E9%94%81%E7%9A%84%E4%BC%98%E5%8C%96/","excerpt":"","text":"Java锁的优化在JDK6,Java对synchronized进行了大量的改进,包括适应性自旋,锁膨胀,轻量级锁,偏向锁. 自适应自旋自旋指的是当线程获取不到锁的时候,不用直接进入挂起状态,而是执行一个忙循环,如果在忙循环结束之后能获取到锁,那么就可以减少线程切换的开销.自适应自旋通过统计自旋相关的一些参数信息,从而动态的调整执行忙循环的次数,甚至有可能跳过自旋过程. 锁消除如果在一个方法内部声明一个对象,并且这个对象不可能被外部的方法所访问到,但是这个对象的一些方法可能有关于锁的一些操作,这样可能会降低程序运行的性能.所以在即时编译器检查到这类操作时,会将这些关于锁操作的代码消除掉. 锁粗化通常来说,将锁的粒度控制得小一点是一个不错的做法,但是如果某些操作,频繁的用同一个对象进行加锁和解锁,那还不如将锁的范围扩大,这就是锁粗化. 锁升级这部分内容还有一些疑问没有搞清楚,留待以后补充 偏向锁轻量级锁重量级锁","categories":[{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"}],"author":"John Doe"},{"title":"ReentrantLock和Synchronized的区别","slug":"ReentrantLock和Synchronized","date":"2021-06-24T03:56:00.000Z","updated":"2021-06-25T10:24:28.976Z","comments":true,"path":"2021/06/24/ReentrantLock和Synchronized/","link":"","permalink":"http://example.com/2021/06/24/ReentrantLock%E5%92%8CSynchronized/","excerpt":"","text":"ReentrantLock和Synchronized的区别有了Synchronized,为什么还要有ReentrantLock呢?这是因为在某些特定的场景,Synchronized无法提供很好的灵活性,而ReentrantLock提供了一些更高级的功能,但是同时也有一些缺点. 两者主要的区别如下: ReentrantLock可以在获取锁失败时立即退出或者在一段时间内等待锁的获取,可以防止锁顺序死锁,而Synchronized不能. ReentrantLock可以在等待获取锁的时候响应中断,而Synchronized不能. ReentrantLock要手动释放锁,而Synchronized能自动释放锁. 在JDK5时,ReentrantLock的性能比Synchronized好.从JDK6开始,两者性能差不多. Synchronized是非公平锁,ReentrantLock既可以是公平锁,也可以是非公平锁. 如何选择应该优先考虑使用Synchronized,因为它能够自动释放锁,这样能降低危险性.只有在需要一些高级功能时,才应该考虑ReentrantLock.","categories":[{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"}],"author":"John Doe"},{"title":"JVM运行时数据区域","slug":"JVM运行时数据区域","date":"2021-06-24T03:09:00.000Z","updated":"2021-06-25T12:10:23.690Z","comments":true,"path":"2021/06/24/JVM运行时数据区域/","link":"","permalink":"http://example.com/2021/06/24/JVM%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/","excerpt":"","text":"JVM运行时数据区域JVM运行时数据区域如下图所示: 程序计数器程序计数器是线程私有的,指向当前线程执行的字节码行号,当线程挂起后恢复,就是通过这个计数器来知道下一条指令的执行位置. Java虚拟机栈Java虚拟机栈是线程私有的,在方法执行的时候,会在线程中创建一个栈帧,存放局部变量表、操作数栈、动态连接、方法出口等信息,每个方法的调用到执行完毕对应栈帧的入栈和出栈过程. 本地方法栈本地方法栈是线程私有的,和Java虚拟机栈相似,区别在于它为Native方法服务. Java堆堆是线程共享的,几乎所有的对象实例都是分配在Java堆上,另外从JDK7开始,本位于方法区的字符串常量池已经移动到了堆上. 方法区方法区是线程共享的,用来存放虚拟机加载的类型信息.运行时常量池是方法区的一部分,编译期生成的各种字面量和符号引用在类加载后会存放到运行时常量池中. 直接内存这部分区域并不属于JVM运行时数据区域,但是JAVA里的部分技术可能使用到这部分内存,如NIO,通过Native函数库直接分配堆外内存,所以在使用时要考虑本机总内存的大小.","categories":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"}],"author":"John Doe"},{"title":"JVM垃圾收集器","slug":"jvm垃圾收集器","date":"2021-06-23T16:34:00.000Z","updated":"2021-06-23T18:24:21.702Z","comments":true,"path":"2021/06/24/jvm垃圾收集器/","link":"","permalink":"http://example.com/2021/06/24/jvm%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"JVM垃圾收集器JVM垃圾收集器中比较经典的收集器如下图所示: 从图中可以看到,经典垃圾收集器目前有7种,图片米色部分的是新生代收集器,浅绿色部分是老年代收集器,横跨两种颜色的G1既是新生代收集器又是老年代收集器.除了这几个收集器之外,还有一些新版本的低延迟垃圾收集器,比如Shenandoah和ZGC收集器.接下来介绍一下各个收集器. SerialSerial是新生代垃圾收集器,具有如下特点: 采取标记复制算法进行垃圾回收 垃圾回收过程会暂停用户线程,并使用单个线程对垃圾进行回收 Serial是串行的意思,名字很好的反映了该收集器的特点. Serial OldSerial Old是老年代垃圾收集器,具有如下特点: 采取标记整理算法进行垃圾回收 垃圾回收过程会暂停用户线程,并使用单个线程对垃圾进行回收 和上面的Serial收集器对比,该收集器不同的点就是工作在老年代,垃圾回收的算法不一样. ParNew从名字中的New可以看出来这是一个新生代垃圾收集器,而名字中的Par代表的意思是并行,代表该收集器会同时开启多个线程来进行垃圾回收,所以可以把它认为是Serial收集器的并行回收版本,特点如下: 采取标记复制算法进行垃圾回收 垃圾回收过程会暂停用户线程,并使用多个线程对垃圾进行回收 Parallel Scavenge这个收集器也是新生代收集器,其基本特点与ParNew一模一样,除此之外,还有其它特别之处,可以理解为ParNew的加强版.除了ParNew的两个特点之外,特别之处在于这是一个关注吞吐量的收集器,它可以设定两个参数来控制垃圾收集器的吞吐量.首先吞吐量 = 运行用户代码时间/(运行用户代码时间+运行垃圾收集时间.)第一个参数-XX:MaxGCPauseMillis来让垃圾收集器尽可能保证垃圾收集的停顿时间小于这个指定的时间.第二个参数-XX:GCTimeRatio指定了GC时间占垃圾回收时间的比值,计算方式为GC占用时间比例=1/1+指定的值).除此之外,该收集器还提供了-XX:UseAdaptiveSizePolicy参数来根据运行情况收集性能信息来调整垃圾收集器的参数,这个叫做自适应调节策略. Parallel Old从Old可以看出这是一个老年代收集器,其特点和Parallel Scavenge是一样的,不同之处在于它采用了标记整理算法来对老年代进行收集. CMSCMS是老年代收集器,是以低停顿为目标的垃圾收集器,采用标记清除算法来清除垃圾,运行过程如下: 初始标记,标记GC Roots能直接关联到的对象,这个过程需要停顿用户线程. 并发标记,遍历整个对象图来标记能回收的对象,这个过程可以和用户线程一起并发执行 重新标记,这个过程标记的是在并发标记时产生的新的垃圾,相当于做一个修正,这个过程需要停顿用户线程 并发清除,这个过程清理标记了的对象,可以与用户线程一起并发执行. CMS被称为低停顿并发收集器,在上面耗时比较长的2和4过程中可以做到和用户线程一起执行,在耗时比较短的1和3过程需要停顿用户线程,总体来说可以认为整个过程是和用户线程一起并发执行,其思想是非常优秀的.但是由于这种设计,也有如下几个缺点: 占用CPU资源.因为与用户线程一起并发执行,那么肯定会和用户线程一起抢占CPU的执行权,这样就会导致用户线程的吞吐量下降,尤其是在CPU核心数较少的情况下. 无法处理浮动垃圾.因为在垃圾清除阶段,用户线程还在运行,这个时候老年代可能会产生新的对象,如果这个时候无法找到足够的内存空间进行对象的分配,将会出现Concurrent Mode Failure,此时将会停顿用户线程,改用Serial Old垃圾收集器来进行垃圾回收,这样停顿的时间就更长了.默认情况下,CMS会预留一部分内存空间来分配新对象,可以通过参数-XX:CMSInitiatingOccupancyFraction来指定占用内存的比例为多少时触发CMS的垃圾回收. 内存碎片导致无法分配新对象.因为CMS是基于标记清除的垃圾收集器,这会导致它产生大量的内存碎片,可能导致无法找到一块内存去分配新的对象,那么就很容易导致触发FULL GC.所以CMS提供了一个CompactAtFullCollection参数来使得进行Full GC前先进行内存碎片的整理,尝试去找到一块空间来分配新对象,但是这个过程需要移动对象,所以会停顿用户线程.","categories":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"}],"author":"John Doe"},{"title":"count(*),count(1),count(column name)的区别","slug":"count-count-1-count-column-name-的区别","date":"2021-06-23T08:56:00.000Z","updated":"2021-06-23T16:12:37.419Z","comments":true,"path":"2021/06/23/count-count-1-count-column-name-的区别/","link":"","permalink":"http://example.com/2021/06/23/count-count-1-count-column-name-%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"count(*),count(1),count(column name)的区别count(*) vs count(1)count(*)和count(1)的功能还有性能是差不多的,这里面括号里面的参数只是一个标识符,如果发现有一行数据,那么就分配一个标识符,最后看有多少标识符而已.count(1)并不是说统计第一行的数据有多少个,count(*)也不是说要把一行的所有列都扫描一遍.如果你不信,你可以试试count(-13),难道它统计的是-13列的个数? 那么它们具体是怎么去计算有多少条数据的呢?在只有主键索引的情况下,通过主键索引中索引的个数来计算有多少条数据.在有二级索引的情况下,通过二级索引中索引的个数来计算有多少条数据,如果有多个二级索引,用索引空间占用量小的二级索引来进行计算. count(*) vs count(column name)count(*)和count(column name)的区别在于count(column name)统计的是某一列字段的个数,其中字段值为NULL的不进行统计. 那么count(column name)是如何计算有多少条数据的呢?如果有column name对应的二级索引,那么就计算该二级索引的个数(字段值不为NULL).如果没有对应的二级索引,则扫描主键索引,通过判断行数据对应的column name是否为空来进行统计.","categories":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}],"author":"John Doe"},{"title":"RocketMQ消息重投和消息重试","slug":"RocketMQ消息重投和消息重试","date":"2021-06-22T17:10:00.000Z","updated":"2021-06-23T16:34:38.991Z","comments":true,"path":"2021/06/23/RocketMQ消息重投和消息重试/","link":"","permalink":"http://example.com/2021/06/23/RocketMQ%E6%B6%88%E6%81%AF%E9%87%8D%E6%8A%95%E5%92%8C%E6%B6%88%E6%81%AF%E9%87%8D%E8%AF%95/","excerpt":"","text":"RocketMQ消息重投和消息重试消息重投消息重投指的是生产者发送消息到Broker没有成功,然后进行重新发送.使用同步发送方式,当消息发送失败时,默认最多会重试两次,并且重试时会选择不同的Broker来进行消息的发送. 消息重试消息重试指的是消费者在消费消息时失败,然后重新消费消息.每个消费组都有一个Topic名称为“%RETRY%+consumerGroup”的重试队列,当消息消费失败时,将根据具体的重试级别来进行重试,默认是重试16次,并且每次的重试时间间隔逐步递增.","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"怎么保证RocketMQ消息不丢失","slug":"怎么保证RocketMQ消息不丢失","date":"2021-06-21T05:47:00.000Z","updated":"2021-06-21T06:02:29.451Z","comments":true,"path":"2021/06/21/怎么保证RocketMQ消息不丢失/","link":"","permalink":"http://example.com/2021/06/21/%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81RocketMQ%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/","excerpt":"","text":"怎么保证RocketMQ消息不丢失要保证RocketMQ消息不丢失,需要生产者、Broker以及消费者的配合. 生产者生产者在发送消息时,如果没有收到服务器的成功响应,那么就要重试发送消息,直到消息发送成功. BrokerBroker要做的事情是保证消息不丢失,当接收到生产者发送的消息时,Broker为了保证消息不丢失,会将数据同步到磁盘中,为了做到真正意义上的不丢失,需要Broker设置同步刷盘模式,必须要等到数据真的同步到磁盘上之后,再向客户端返回消息发送成功状态. 除此之外,为了提高服务的可用性,Broker通常会采用主从模式,这个时候还要保证消息真正同步到了从服务器上,需要设置主从服务器复制策略为同步复制模式,等到所有的从服务器都真正接收到数据并存储到磁盘时,再向客户端返回消息发送成功状态. 总结起来就是两点: Master保证数据同步到磁盘 Slava保证数据同步到磁盘 消费者对于消费者而言,消息不丢失意味着能够成功消费到所有消息.正常情况下,消费者成功消费之后,再向Broker返回成功消费信息,否则返回消费失败信息,使得下次还能够重新消费那些没有消费成功的消息. 效率问题为了保证消息不丢失,需要使用大量的同步策略,这样可能导致效率的低下,所以在实际生产环境中还需要结合实际业务进行权衡.","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"Kafka和RocketMQ的区别","slug":"Kafka和RocketMQ的区别","date":"2021-06-21T04:56:00.000Z","updated":"2021-06-21T05:40:02.471Z","comments":true,"path":"2021/06/21/Kafka和RocketMQ的区别/","link":"","permalink":"http://example.com/2021/06/21/Kafka%E5%92%8CRocketMQ%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Kafka和RocketMQ的区别 RocketMQ Kafka 协议和规范 拉取模式,支持TCP、JMS、OpenMessage 拉取模式,支持TCP 消息顺序 保证严格的消息顺序,并且可以优雅扩展 保证分区消息顺序 批量消息 支持,使用同步模式来避免消息丢失 支持,使用异步生产者 广播消息 支持 不支持 消息过滤 支持,使用基于SQL92的属性过滤表达式 支持,使用Kafka Stream过滤消息 服务端触发重新发送消息 支持 不支持 消息追溯 支持偏移量和时间戳指定 支持偏移量指定 高可用 使用主从模式 使用Zookeeper 消息追踪 支持 不支持 服务端重新发送消息 支持 不支持","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"Redis分布式锁","slug":"Redis分布式锁","date":"2021-06-21T03:16:00.000Z","updated":"2021-06-21T04:23:12.634Z","comments":true,"path":"2021/06/21/Redis分布式锁/","link":"","permalink":"http://example.com/2021/06/21/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"Redis分布式锁分布式锁主要用来解决分布式环境下资源的互斥访问,这篇文章就来介绍一下分布式锁. 基本思路首先要保证获取锁和释放锁的正确性,通常来说就是保证获取锁和释放锁的原子性,一般会使用SETNX key 唯一标识命令来获取锁,在Lua脚本中通过唯一标识来释放锁,这样能保证获取锁和释放锁的客户端是同一个. 考虑宕机当客户端服务获取到锁之后挂掉了,那么锁可能永远都没办法被另外的客户端获取到,所以此时要考虑给锁加一个过期时间,保证客户端服务挂掉之后锁也能释放,通常使用SETNXEXPIRE key 唯一标识 过期时间来实现. 考虑可用性假如我们的Redis实例只有一个,那么有可能Redis实例挂了,那么分布式锁服务就不可用了,所以我们现在要想办法提高可用性.一个比较可靠的方式是使用RedLock算法. RedLock算法假设我们有5个Redis实例,彼此是独立的,那么RedLock的算法的运作过程如下: 向各个Redis实例获取锁(SETNXEXPIRE key 唯一标识 过期时间),并且需要设置一个获取锁的超时时间(通常远远小于锁的过期时间),如果在这个超时时间之内没有获取到锁,那么则向另外的Redis实例获取锁. 当获取到锁的时间小于锁的失效时间,并且有超过半数的Redis实例上成功获取到锁时,锁才算获取成功. 如果锁没有获取成功(在锁过期前没有获取到锁或者没有在超过半数的实例上获取到锁),那么向所有的Redis实例进行一个解锁操作,即使某个Redis实例上并没有成功获取到锁. 持久化对分布式锁的影响假如我们没有使用持久化,那么实例重启之后key就会消失,其它服务就有可能获取到锁.比如本来在5台机器中的3台中成功获取到锁,此时挂掉一台,那么重启后没有设置key的实例就变成了3台(半数以上),其他的请求就可能获取到锁.为了保证可靠性,需要开启持久化模式,并设置fsync=always来保证数据不丢失. 锁失效了怎么办还有这样一个问题,假如我们获取到锁之后,做一个业务操作,这个业务操作在超过了锁的失效时间前还没有完成,那么其它的请求就有可能获取到分布式锁,这样破坏了锁的安全性.针对这种情况,可以在业务操作没有完成之前对锁的失效时间做一个延长,主要的步骤如下: 获取到分布式锁 在获取到锁的机器上开启定时任务,根据唯一标识对获取到的key延长过期时间(使用lua脚本). 等到操作完成之后,我们就会释放该锁,这样这个key不可能再延长过期时间. 通过这种方式,可以保证在业务操作完成之前,锁不会被释放.另外由于这个定时任务是在获取锁的机器上开启的,那么当这台机器挂了之后,也不会有定时任务对这个key进行一个过期时间的延长,还是可以满足机器挂掉之后key能够在过期时间到来时释放.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"John Doe"},{"title":"Redis缓存穿透、雪崩、击穿","slug":"Redis缓存穿透、雪崩、击穿","date":"2021-06-20T17:00:00.000Z","updated":"2021-06-20T18:09:31.382Z","comments":true,"path":"2021/06/21/Redis缓存穿透、雪崩、击穿/","link":"","permalink":"http://example.com/2021/06/21/Redis%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E9%9B%AA%E5%B4%A9%E3%80%81%E5%87%BB%E7%A9%BF/","excerpt":"","text":"Redis缓存穿透、雪崩、击穿这篇文章介绍Redis缓存穿透、雪崩、击穿问题及解决方案. 缓存穿透缓存穿透指的是存储层(如mysql)不存在相应的数据,此时如果根据key进行查询,那么在缓存层查找不到数据,所有的这类查询最终都会访问存储层,相当于缓存层是没有用的.出现缓存穿透的原因主要有以下两点: 恶意攻击 业务代码问题 解决方案对于缓存穿透主要有两种解决方案. 缓存空值,并设置一定的过期时间.在存储层查询不到数据后,在缓存层设置key对应的值为空,当下一次访问时,那么将直接从缓存层返回空对象,设置一定的过期时间是为了能够在存储层数据更新时,缓存层数据也可以得到更新. 使用布隆过滤器.通过对存储层数据对应的key进行扫描,将其放置在布隆过滤器中,因为布隆过滤器判断key是否存在的效率很高.在获取数据时,先判断布隆过滤器中是否存在该key,存在的话则访问缓存层,缓存层找不到的话再去访问存储层. 缓存击穿和热点Key问题缓存击穿指的是缓存层的key过期之后,将会访问存储层进行缓存层数据的重建.这是一个正常的现象,只不过如果一个key属于热点key,对于这个key的请求量有几百上千万,那么当key过期了,还没来得及重建缓存层数据时,大量请求都会访问存储层来重建缓存,那么这样就会给存储层带来很大压力,我们主要讨论的就是缓存击穿中的热点Key问题. 解决方案对于热点key问题,主要有两种解决方案. 分布式锁更新缓存.当缓存失效时,使用分布式锁让其中一个请求去重建缓存,其它请求进行等待,直到缓存数据重建完成.这种方式的优点是能保证缓存层和数据层的数据一致性,缺点是如果重建缓存时间太长,则会使服务阻塞. 不给key设置过期时间.当一个请求到来时,由于key没有设置过期时间,那么总能从缓存层获取到数据.然后为了保证数据能及时得到更新,我们会设置一个逻辑过期时间,当请求到来时,如果发现时间已经到达逻辑更新时间,那么我们会异步的对缓存进行重建.这样做的优点是够最大程度的解决热点key问题,缺点是数据一致性无法得到保证. 缓存雪崩缓存雪崩指的是缓存层服务不可用,这种情况就相当于没有缓存层,此时对数据的请求直接就涌向了存储层,可能导致存储层也不可用. 解决方案解决缓存雪崩的方案主要有以下几种: 保证缓存层的高可用性.对于Redis来说,比如可以使用主从、哨兵等方式. 使用降级组件来使得服务不可用时,及时响应服务请求,比如使用Java的Hystrix.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"John Doe"},{"title":"Java Class文件分析","slug":"JVM类文件分析","date":"2021-06-18T07:03:00.000Z","updated":"2021-06-18T08:41:49.175Z","comments":true,"path":"2021/06/18/JVM类文件分析/","link":"","permalink":"http://example.com/2021/06/18/JVM%E7%B1%BB%E6%96%87%E4%BB%B6%E5%88%86%E6%9E%90/","excerpt":"","text":"Java Class文件分析这篇文章的主要目的是要分析Java编译后的Class文件的组成结构,以此来加深对Class文件的理解. 将Class文件转换成十六进制如果是使用Idea进行开发,可以安装插件HexView来进行分析,直接在Class文件上点击右键使用HexView来展示即可.比如有这么一个java类: 12345678910public class TestClass &#123; private int m; public TestClass() &#123; &#125; public int inc() &#123; return this.m + 1; &#125;&#125; 使用HexView之后,解析的十六进制格式如下： Class文件分析我们将通过分析上面的这张图片来分析类文件的组成. 魔数可以看到,图片的前四个字节为ca fe ba be(咖啡宝贝),是设计者用来表示Class文件格式的方式,因为通过文件后缀来表示Class文件格式不够安全,所以使用了这种方式. 版本号第五个字节和第六个字节为00 00,这个表示次版本号,这个版本号因为用处不大,所以不关注. 第七个字节和第八个字节00 34表示主版本号,对应JDK的不同版本,比如我的是JDK1.8,对应的主版本号为52.0(34的十进制),如果Class文件的主版本号大于JDK对应的主版本号,那么虚拟机将拒绝执行. 常量池这部分比较复杂,留到后面再继续写 访问标志在常量池后面,有两个字节代表访问标志,识别类或接口等一系列访问信息,对应上图中000000e0行的00 21.目前有9个访问标志: 这里重点在于理解其思想,所以只讲解21是如何得出来的.我们该类的哪些访问标志为真,然后把这些标志对应的标志值取出来,做一个|操作,即为最终的访问标志值.用上面的这个类来说,首先定义的这个类是public的,那么要将ACC_PUBLIC对应的标志值0x0001取出来,另外ACC_SUPER这个标志固定为真,所以也要取出来,其他的访问标志为假,所以最终结果为0x0001 | 0x0020 = 0x0021.","categories":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"}],"author":"John Doe"},{"title":"Spring Cloud Ribbon负载均衡","slug":"Spring-Cloud-Ribbon负载均衡","date":"2021-06-16T07:33:00.000Z","updated":"2021-06-18T08:41:35.052Z","comments":true,"path":"2021/06/16/Spring-Cloud-Ribbon负载均衡/","link":"","permalink":"http://example.com/2021/06/16/Spring-Cloud-Ribbon%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","excerpt":"","text":"Spring Cloud Ribbon负载均衡12@LoadbalancedRestTemplate restTemplate; 当我们在RestTemplate上面加上@Loadbalanced注解时,就轻而易举的指定了默认的负载均衡策略,那么还有哪些负载均衡策略呢?具体的实现原理是什么样子的?这篇文章就来分析一下Ribbon有哪些负载均衡策略以及负载均衡策略是如何实现的. 负载均衡的大体过程负载均衡的过程主要做以下几件事: 负载均衡器根据服务名称获取实例列表 负载均衡器根据负载均衡策略选中一个实例,获取该实例信息 根据实例信息的逻辑服务名重构URI,形成真实的地址 根据真实的访问服务器 接下来,将对关键的方法进行叙述,代码很长,重点看注释部分. 当发送一个请求时,将会被LoadBalancerInterceptor类的intercept函数拦截,之后将会调用LoadBalanceClient的execute方法. 1234567891011public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); //调用loadBalancer的execute方法 return (ClientHttpResponse)this.loadBalancer.execute(serviceName, new LoadBalancerRequest&lt;ClientHttpResponse&gt;() &#123; public ClientHttpResponse apply(ServiceInstance instance) throws Exception &#123; HttpRequest serviceRequest = LoadBalancerInterceptor.this.new ServiceRequestWrapper(request, instance); return execution.execute(serviceRequest, body); &#125; &#125;); &#125; 而LoadBalancerClient的具体实现类为RibbonLoadBalancerClient,在execute方法中,将根据serviceId使用负载均衡策略从对应的实例列表中选出一台实例. 12345678910111213141516171819202122232425262728293031public &lt;T&gt; T execute(String serviceId, LoadBalancerRequest&lt;T&gt; request) throws IOException &#123; ILoadBalancer loadBalancer = getLoadBalancer(serviceId); //根据负载均衡器选择服务器 Server server = getServer(loadBalancer); if (server == null) &#123; throw new IllegalStateException(&quot;No instances available for &quot; + serviceId); &#125; RibbonServer ribbonServer = new RibbonServer(serviceId, server, isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server)); RibbonLoadBalancerContext context = this.clientFactory .getLoadBalancerContext(serviceId); RibbonStatsRecorder statsRecorder = new RibbonStatsRecorder(context, server); try &#123; //回调,这一步将进行URI地址的转换 T returnVal = request.apply(ribbonServer); statsRecorder.recordStats(returnVal); return returnVal; &#125; // catch IOException and rethrow so RestTemplate behaves correctly catch (IOException ex) &#123; statsRecorder.recordStats(ex); throw ex; &#125; catch (Exception ex) &#123; statsRecorder.recordStats(ex); ReflectionUtils.rethrowRuntimeException(ex); &#125; return null; &#125; 接着在经过一系列调用之后,将调用RibbonLoadBalancerClient的reconstructURI来重新构造URI,将形如ip:port的服务转换成http://ip:port/接口名称的形式,最终调用服务. 1234567891011121314//重新构造URIpublic URI reconstructURI(ServiceInstance instance, URI original) &#123; Assert.notNull(instance, &quot;instance can not be null&quot;); String serviceId = instance.getServiceId(); RibbonLoadBalancerContext context = this.clientFactory .getLoadBalancerContext(serviceId); Server server = new Server(instance.getHost(), instance.getPort()); boolean secure = isSecure(server, serviceId); URI uri = original; if (secure) &#123; uri = UriComponentsBuilder.fromUri(uri).scheme(&quot;https&quot;).build().toUri(); &#125; return context.reconstructURIWithServer(server, uri); &#125; 最终再回到LoadBalancerInterceptor类的intercept方法,调用ClientHttpRequestExecution的execute调用服务. 1234567891011public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); return (ClientHttpResponse)this.loadBalancer.execute(serviceName, new LoadBalancerRequest&lt;ClientHttpResponse&gt;() &#123; public ClientHttpResponse apply(ServiceInstance instance) throws Exception &#123; HttpRequest serviceRequest = LoadBalancerInterceptor.this.new ServiceRequestWrapper(request, instance); //最终的服务调用 return execution.execute(serviceRequest, body); &#125; &#125;); &#125; 负载均衡器从上面的介绍中,我们已经知道负载均衡器是在LoadBalancerClient的exetute方法的下列代码片段中工作: 12ILoadBalancer loadBalancer = getLoadBalancer(serviceId);Server server = getServer(loadBalancer); 所以接下来,我们来了解一下有哪些负载均衡器.","categories":[{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/categories/spring-cloud/"}],"tags":[{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/tags/spring-cloud/"}],"author":"John Doe"},{"title":"RocketMQ顺序消息","slug":"RocketMQ顺序消费","date":"2021-06-16T05:01:00.000Z","updated":"2021-06-16T05:38:10.819Z","comments":true,"path":"2021/06/16/RocketMQ顺序消费/","link":"","permalink":"http://example.com/2021/06/16/RocketMQ%E9%A1%BA%E5%BA%8F%E6%B6%88%E8%B4%B9/","excerpt":"","text":"RocketMQ顺序消息在一些应用场景,需要做到消息的有序性,即生产和消费严格按照FIFO的方式来进行,默认情况,RocketMQ发送消息不是有序的,所以这篇文章来介绍RocketMQ顺序消息. 原理首先消息是存放在主题中的,一个主题有多个分区,如果分区只有一个,生产者按照顺序发送消息到这个分区,消费者按照分区中消息存放的顺序消费消息,那么就能保证全局有序.如果分区有多个,那么生产者就要把消息按照一定的规则分配到指定的某个分区,消费者还是按照分区中消息存放的顺序消费消息,虽然这不能保证全局有序,但是可以保证分区有序. 例子这里的例子来自于RocketMQ官方文档.业务场景是用户下一个订单的各个步骤,因为这些步骤的顺序是至关重要的,所以必须使用顺序消息来实现. 完整代码请参考https://github.com/CodeShowZz/code-repository/tree/master/rocketmq-demo/src/main/java/com/rocketmq/order 生产者关键代码12345678SendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Long id = (Long) arg; //根据订单id选择发送queue long index = id % mqs.size(); return mqs.get((int) index); &#125; &#125;, orderList.get(i).getOrderId());//订单id 生产者在发送消息时,需要实现一个MessageQueueSelector接口,并根据业务id(比如订单id)将消息发送到指定分区中. 消费者关键代码12345678consumer.registerMessageListener(new MessageListenerConcurrently() &#123; public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; for (MessageExt messageExt : msgs) &#123; System.out.println(&quot;consumer:&quot; + new String(messageExt.getBody())); &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); 使用MessageListenerConcurrently接口方式来消费消息,在我的测试中,可以观察到消息是按照分区有序来消费的. 12345678910consumer.registerMessageListener(new MessageListenerOrderly() &#123; public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; context.setAutoCommit(true); for(MessageExt msg : msgs) &#123; System.out.println(&quot;consumeThread=&quot; + Thread.currentThread().getName() + &quot;queueId=&quot; + msg.getQueueId() + &quot;, content:&quot; + new String(msg.getBody())); &#125; return ConsumeOrderlyStatus.SUCCESS; &#125; &#125;); 使用MessageListenerOrderly接口,在我的测试中,是按照全局有序来消费的,也就是生产者如何生产的,消费者就是如何消费的,这个和官方文档描述的只有一个分区时的全局有序有所出入.","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"运营系统介绍","slug":"运营系统介绍","date":"2021-06-13T16:26:00.000Z","updated":"2021-06-13T23:45:47.126Z","comments":true,"path":"2021/06/14/运营系统介绍/","link":"","permalink":"http://example.com/2021/06/14/%E8%BF%90%E8%90%A5%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"运营系统介绍每个公司可能会有这样一个系统,它负责给公司的用户发布一些营销信息,比如通过短信的方式或者通过APP消息的方式来触达用户.那么首先应该要有一个用户标签的概念,用户标签指的是用户的特征,比如用户在最近30天内购买过一个理财产品,通过一个或多个特征,我们可以从一个用户库里面筛选出一部分人出来,我们称它为客群.有了客群,我们还需要营销的信息,比如给用户发短信,发优惠券等各种各样能触达到用户的方式,这类信息叫做触点.最后一点是我们将在什么时候触达用户,这个叫做执行时间.总结起来,运营系统最主要的职责便是在某个时间,向特定的用户发布特定的信息.除此之外,在某一次任务执行完成之后,我们还需要追踪任务的执行情况,以获取最终的运营效果. 功能点当然,每个公司的业务不一样,可能运营系统还会承担其它的一些职责,现在介绍一下我之前参与开发的运营系统的功能.如图: 基础功能这一部分主要是建立用户-角色体系,运营系统主要针对的用户不是大众用户,而是服务于公司内部的运营人员.主要由以下几点组成: 用户 角色 权限 菜单 定时任务 在运营系统中,首先需要有用户来操作,接着会给予用户某个角色,通过角色控制用户的权限,比如用户可以看到什么菜单,做什么操作(菜单权限和接口权限).另外,定时任务主要给开发人员配置定时任务,在某个时间点执行某个任务. 基础配置这一部分主要是服务于任务执行的信息,比如客群和标签,这一部分主要对接公司的大数据部门.当然短信配置和消息配置其实也算是基础配置,但从功能点来分的话,就没把它们放到基础配置里面了.另外运营系统还服务于另外一个广告系统,所以这部分还有其它一些配置功能.主要功能如下: 任务执行相关: 标签:一个表达式,标识用户的某个特征. 客群:标签的组合,最终筛选出一批用户. 广告系统相关: 运营位:运营位对应APP上的某个广告位. 素材:素材也就是广告的内容,一般为图片或者文案. 推广计划:也就是广告,其中会指定广告位置、素材、客群,同时还有其它一些配置,比如推广的时间段,比如指定用户的操作系统类型(只给安卓用户下发某个广告). ABTest:主要用户建立实验,进行广告策略的分流. 账户:创建推广计划时,还需要指定账户,账户会充值一定的金额.当一条广告下发时,将消耗这个账户的一部分金额. 核心功能短信任务和息任务是系统的核心,以这两个触点触达用户.素材自动化也是一个比较核心的功能,以自动化的方式来生成素材,能提升素材的生成效率. 短信任务:在指定时间,向客群发送短信. 消息任务:在指定时间,向客群发送消息. 素材自动化:主要用于自动化生成素材.运营人员自己创建一张图片的成本比较高,如果可以根据用户上传图片的模板信息来组合成一系列的素材,那么素材创建将会更加的高效. 扩展功能这一部分是由其他部分产生的扩展功能. 任务执行信息:任务执行过程的信息:比如调用短信接口的次数,成功数以及失败数. 任务监控:主要用于监控任务的执行情况,当任务执行满足一定的条件时(比如任务开始5分钟之内没有执行成功的记录),将会进行任务预警. 任务预警:主要通过短信和邮件的方式告知开发人员任务执行异常. 定时清理任务:将超过一定时间点的数据清除(比如半年). 报表订阅:这个主要和报表功能相关,当用户订阅报表时,将通过邮件发送给用户. 效果数据这一部分主要用于追踪短信和消息以及广告的执行情况,但是这类数据有个很明显的特点,它们不是实时的产生效果数据的,比如一条短信下发之后,可能由于短信运营商或者用户原因(比如停机)导致无法接收到短信,那么就需要追踪一个时间段发送出去的短信,那么就有个问题:通过什么方式来追踪呢? 在我们的系统中,创建一个短信任务时,将同时为每个短信内容创建一个模板id(通过对接短信平台),也就是短信的唯一标识.发送短信时,只要通过这个标识调用短信平台接口就可以了.除此之外,短信的发送状态一般由短信平台同步到大数据的Hive库中,运营平台需要每天去拉取某个时间段的短信状态信息(比如截止到短信发送后一周,仍然要拉取短信状态信息). 消息任务的效果追踪和短信是类似的,至于广告,主要对Hive库的广告下发表、广告展示表还有广告点击表的数据进行统计. 短信任务报表:展示短信最终的效果信息,比如到达量,发送成功量,失败量. 消息任务报表:展示消息最终的效果信息,比如到达量,发送成功量,失败量. 广告报表.展示广告下发量、浏览量、点击量等信息. 形成闭环从用户创建任务,到任务执行,再到查看营销结果,整个系统形成了一个闭环,从上面的图中自上而下也能很明显得看出这一点. 技术栈在我参与开发的运营系统中,主要用到的技术如下图: 数据库系统主要分为四个模块: web模块 配置模块 数据模块 任务执行模块 根据模块的职责,各个模块的表情况如下图: 流程图运营系统其实大部分流程并不复杂,这里简单的描述一下短信任务的一个流程,复杂性在于要把系统的各个模块串联起来.主要流程描述如下: 短信任务执行之前,会有短信模板id的生成(在创建任务的时候会生成,还会有定时任务每天重新生成新的模板id) 拉取任务对应的客群,调用短信接口发送短信 在任务执行过程中,会记录任务执行的情况(任务发送了多少条短信,调用短信发送接口成功数和失败数). 每日的短信任务报表的拉取,以此来查看短信最终的触达情况. 短信任务流程图: 再详细一点,短信任务的执行流程图: 总结这篇文章大致介绍了之前参与开发的运营系统的主要功能,不同的运营系统的设计可能多种多样,需要开发人员发挥想象力,创造更有价值的功能.","categories":[],"tags":[],"author":"John Doe"},{"title":"基于注解的动态数据源实现","slug":"基于注解的动态数据源实现","date":"2021-06-10T10:45:00.000Z","updated":"2021-06-16T05:34:11.795Z","comments":true,"path":"2021/06/10/基于注解的动态数据源实现/","link":"","permalink":"http://example.com/2021/06/10/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E7%9A%84%E5%8A%A8%E6%80%81%E6%95%B0%E6%8D%AE%E6%BA%90%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"基于注解的动态数据源实现需求有些项目不只访问一个数据库,可能需要访问多个数据库,那么就会有一个问题,怎么进行数据源的切换. 动态数据源解决这个需求的一个常见解决方案是使用动态数据源.下面将按部就班的来介绍一下如何实现基于注解的动态数据源.完整的代码请参考https://github.com/CodeShowZz/data-source/tree/master/dynamic-data-source. 第一步:配置数据源将项目中需要使用的数据源放到一个配置文件中,比如叫做jdbc.properties,在我的例子中,我有两个数据源,一个是learning库,另外一个是test库. 数据库配置文件: 123456789spring.datasource.test.driver-class-name=com.mysql.jdbc.Driverspring.datasource.test.jdbc-url=jdbc:mysql://localhost:3306/test?useSSL=false&amp;serverTimezone=GMT%2B8&amp;characterEncoding=UTF-8spring.datasource.test.username=rootspring.datasource.test.password=123456spring.datasource.learning.driver-class-name=com.mysql.jdbc.Driverspring.datasource.learning.jdbc-url=jdbc:mysql://localhost:3306/learning?useSSL=false&amp;serverTimezone=GMT%2B8&amp;characterEncoding=UTF-8spring.datasource.learning.username=rootspring.datasource.learning.password=123456 数据源常量类: 123456public class DataSourceConstants &#123; public static final String DB_LEARNING = &quot;learning&quot;; public static final String DB_TEST= &quot;test&quot;;&#125; 动态数据源类: 1234567public class DynamicDataSource extends AbstractRoutingDataSource &#123; @Override protected Object determineCurrentLookupKey() &#123; return DynamicDataSourceContextHolder.getContextKey(); &#125;&#125; 这里使用了一个DynamicDataSourceContextHolder类,将在下面进行讲解. 数据源配置: 123456789101112131415161718192021222324252627282930@EnableAutoConfiguration(exclude = &#123;DataSourceAutoConfiguration.class&#125;)@Configuration@PropertySource(&quot;classpath:jdbc.properties&quot;)@MapperScan(basePackages = &quot;com.dynamic.datasource.dao&quot;)public class DynamicDataSourceConfig &#123; @Bean(DataSourceConstants.DB_LEARNING) @ConfigurationProperties(prefix = &quot;spring.datasource.learning&quot;) public DataSource learningDataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(DataSourceConstants.DB_TEST) @ConfigurationProperties(prefix = &quot;spring.datasource.test&quot;) public DataSource testDataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean @Primary public DataSource dynamicDataSource() &#123; Map&lt;Object, Object&gt; dataSourceMap = new HashMap(2); dataSourceMap.put(DataSourceConstants.DB_LEARNING, learningDataSource()); dataSourceMap.put(DataSourceConstants.DB_TEST, testDataSource()); DynamicDataSource dynamicDataSource = new DynamicDataSource(); dynamicDataSource.setTargetDataSources(dataSourceMap); dynamicDataSource.setDefaultTargetDataSource(testDataSource()); return dynamicDataSource; &#125;&#125; 在这里讲一下具体的原理,首先我们定义了两个数据源,然后在dynamicDataSource方法中定义了一个Map,将两个数据源以(名称,数据源)的形式放入.接着调用setTargetDataSources将Map设置进去,并通过setDefaultTargetDataSource设置了默认数据源.在每次执行sql语句时,将通过DynamicDataSource类实现的determineCurrentLookupKey方法返回的key从Map中找到对应的数据源,如果没有找到,将使用默认数据源. 了解了这个原理,那么改变determineCurrentLookupKey方法返回的key就可以实现数据源的切换,那如何改造这个方法使得可以动态切换数据源呢?通常来说,会将它放在ThreadLocal中. 第二步:引入ThreadLocal定义ThreadLocal对象: 123456789101112131415161718192021222324252627public class DynamicDataSourceContextHolder &#123; /** * 动态数据源名称上下文 */ private static final ThreadLocal&lt;String&gt; DATASOURCE_CONTEXT_KEY_HOLDER = new ThreadLocal&lt;&gt;(); /** * 设置/切换数据源 */ public static void setContextKey(String key)&#123; DATASOURCE_CONTEXT_KEY_HOLDER.set(key); &#125; /** * 获取数据源名称 */ public static String getContextKey()&#123; String key = DATASOURCE_CONTEXT_KEY_HOLDER.get(); return key == null? DataSourceConstants.DB_TEST:key; &#125; /** * 删除当前数据源 */ public static void removeContextKey()&#123; DATASOURCE_CONTEXT_KEY_HOLDER.remove(); &#125;&#125; 很清晰可以看到上面通过ThreadLocal来动态的修改数据源对应的key值,以此来决定某次数据库操作使用的是哪个数据源.至此,一个简单的动态数据源实现就搞定了,接下来可以测试一下. 第三步:测试1234567891011@Testpublic void testDynamicDataSource() &#123; Student student = studentDao.queryById(1); System.out.println(student); DynamicDataSourceContextHolder.setContextKey(DataSourceConstants.DB_LEARNING); System.out.println(userDao.selectById(1)); DynamicDataSourceContextHolder.removeContextKey(); DynamicDataSourceContextHolder.setContextKey(DataSourceConstants.DB_TEST); System.out.println(studentDao.queryById(1)); DynamicDataSourceContextHolder.removeContextKey();&#125; 这样,就可以实现动态数据源了,但是可以很清楚的看到,我们需要在做数据库操作时设置ThreadLocal的值,使用后还要清除值,如果能够尽可能消除这种样板代码就更好了.我们可以引入AOP,并自定义注解来做这件事. 第四步:引入AOP注解: 12345678@Target(&#123;ElementType.METHOD,ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface DS &#123; /** * 数据源名称 */ String value() default DataSourceConstants.DB_TEST;&#125; AOP: 12345678910111213141516171819202122232425262728293031323334@Aspect@Componentpublic class DynamicDataSourceAspect &#123; @Pointcut(&quot;@annotation(com.dynamic.datasource.annotation.DS)&quot;) public void dataSourcePointCut() &#123; &#125; @Around(&quot;dataSourcePointCut()&quot;) public Object around(ProceedingJoinPoint joinPoint) throws Throwable&#123; String dsKey = getDSAnnotation(joinPoint).value(); DynamicDataSourceContextHolder.setContextKey(dsKey); try&#123; return joinPoint.proceed(); &#125;finally &#123; DynamicDataSourceContextHolder.removeContextKey(); &#125; &#125; /** * 根据类或方法获取数据源注解指定的值 */ private DS getDSAnnotation(ProceedingJoinPoint joinPoint) &#123; Class&lt;?&gt; targetClass = joinPoint.getTarget().getClass(); DS classAnnotation = targetClass.getAnnotation(DS.class); if (classAnnotation != null) &#123; return classAnnotation; &#125; MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature(); return methodSignature.getMethod().getAnnotation(DS.class); &#125;&#125; 在Dao层接口的类或方法上添加注解: 12345@Mapperpublic interface StudentDao &#123; @DS(DataSourceConstants.DB_TEST) Student queryById(Integer id);&#125; 12345@Mapper@DS(DataSourceConstants.DB_LEARNING)public interface UserDao &#123; User selectById(Integer id);&#125; 第五步:再次测试1234567@Test public void testDynamicDataSourceUseAnnotation() &#123; Student student = studentDao.queryById(1); System.out.println(student); System.out.println(userDao.selectById(1)); System.out.println(studentDao.queryById(1)); &#125; 这样基于注解的动态数据源就实现完成了.","categories":[{"name":"数据源","slug":"数据源","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%BA%90/"}],"tags":[{"name":"数据源","slug":"数据源","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%BA%90/"}],"author":"John Doe"},{"title":"对数据权限实现的思考","slug":"对数据权限实现的思考","date":"2021-06-09T18:58:00.000Z","updated":"2021-06-10T19:27:28.730Z","comments":true,"path":"2021/06/10/对数据权限实现的思考/","link":"","permalink":"http://example.com/2021/06/10/%E5%AF%B9%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%80%9D%E8%80%83/","excerpt":"","text":"对数据权限实现的思考背景之前在公司做项目的时候,有这么一个需求,需要在现有的系统的某些表上加上数据权限功能.每个用户都有一个角色id,现在要使得相同的角色id可以查询相同的数据,超级管理员的id为1,这个角色可以查询所有的数据. 从简单做起对数据库的操作无非就是CRUD,既然要实现数据权限,那么最简单的方式就是在表里面新增一个roleId字段,然后把它当成一个业务字段来做CRUD,这样非常简单.但是这样会使人有点烦躁,为了实现这个功能,就要将该字段不断的在系统间传递,这样是不是挺麻烦的? 使roleId传递更方便我们的系统有一个和前端打交道的web模块,web模块将调用其余业务模块来进行业务处理.我之前实现这个功能的时候,在Web层实现了一个切面,这个切面是切在对业务模块的调用上,也就是web模块的出口处,因为之前接口方法都满足一定的规范,所以切点就比较通用,然后切面所做的事情就是从Session中取出roleId,然后判断调用的方法参数中是否有roleId字段(通过反射),有则将roleId设置进去,这样roleId就可以自动的进行传递了. 在业务模块中,也实现一个切面,切点为业务模块的入口处,因为web模块已经设置了roleId,那么我们现在取roleId的目的是为了将roleId保存起来,到Dao层再对sql执行进行拦截,同样检测一下是否字段有roleId,有则将roleId设置进去.在我的实现中,我是使用ThreadLocal来保存入口处的roleId,在拦截器中从ThreadLocal中获取roleId,如果方法参数中有roleId这个字段,则设置进去. 整个系统其实有三个切面,一个是web模块的切面,一个是业务模块入口的切面,最后一个是sql拦截器(这个也当成一个切面). 说了这么多,其实无非就是解决roleId如何在系统间传递的问题.但是现在想起来,之前这么做是有点武断的.因为需要做数据权限的表也只是少数,不是所有的调用方法都应该拦截,这样可能会对系统的响应时间造成影响.所以现在想想,如果要改进这个问题,可以自定义一个注解,然后在需要获取或者设置roleId的方法上加上注解,把切点变成基于注解的拦截.这样和直接在系统中直接传递roleId比起来,我也不知道是不是把问题搞复杂了,本来在系统间就是要传递各种数据的,大费周章搞这些切面干什么.如果再让我实现一次,我觉得还是直接在系统间传递roleId好了,除非代码太复杂,直接传递roleId不大可行,我觉得通过切面来传递才有好处. SQL改造在我的实现中,我还对查询语句做了一个sql改造,现在想起来感觉是有点画蛇添足了,但是还是想表达表达.假如有这么一条sql:select id,name from student where id = #&#123;id&#125;,假如在不修改这句sql的前提下,如何将其改成是带有roleId的查询呢?思路是这样的,用拦截器拦截原sql,在外面再嵌套一层sql,形如select * from (原sql) where roleId = #&#123;roleId&#125;,当然原sql的查询字段上要包含roleId字段,这样的话对于大多数sql都是通用的.但是为什么不直接在sql语句中(用的是mybatis)直接加上roleId = #&#123;roleId&#125;呢?因为之前一个表里面可能有多个sql查询语句,且查询的字段使用了mybatis的&lt;sql&gt;&lt;/sql&gt;和&lt;include&gt;&lt;/include&gt;来复用,所以只要在&lt;sql&gt;&lt;/sql&gt;间再加入一个roleId字段,就可以通过上面的方式来简单的改造SQL语句了.现在想想,除非对某个表的查询语句太多(一般不会),否则直接在原sql上加入查询条件或许更好,首先第一点改造的只是查询,其它的数据库操作还是要对原sql进行改造,并且改造查询给系统也带来了复杂性,可能这种改造对复杂sql还不一定适用. 流程在这里总结一下整个流程: 从web层的session中获取roleId,设置到web层调用业务层的对象上(如果有roleId) 获取业务层入口方法的方法参数,将roleId设置在ThreadLocal中. 数据库操作时,使用拦截器将ThreadLocal中的roleId设置到方法参数上. 如果是查询操作,拦截器改造原sql. 这里还要强调一下,在我的实现中,查询在原sql的查询字段上加入roleId即可,其余操作仍然需要修改sql语句. 另外,在三个切面中,可以选择通过规范的方法名称来实现切点,也可以使用基于注解的方式来实现切点,前者比较通用但多了很多不必要的判断,后者需要我们指定拦截的地方,少了不必要的判断却增加了一些开发量. 形成组件在我的实现中,因为关于roleId的切面、注解、拦截器是通用的,所以我将它们全部都放在了一个新的项目中,形成一个组件,然后让业务模块依赖它. Talk is cheap,show me the code俗话说得好,没有代码你说个XX.之前在公司做这个功能的时候,可能一股脑就想着简化,现在想起来觉得不能为了简化而简化,现在我觉得这个数据权限的思路可以记录下来,但是要让我再实现它一次觉得已经没有动力了,所以就不再想写代码了.(好吧,我承认再实现一遍有点折磨人😭)","categories":[{"name":"数据权限","slug":"数据权限","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"}],"tags":[{"name":" 数据权限","slug":"数据权限","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"}],"author":"John Doe"},{"title":"ABTest分流算法设计与实现","slug":"ABTest分流算法设计","date":"2021-06-08T14:14:00.000Z","updated":"2021-06-16T05:35:12.528Z","comments":true,"path":"2021/06/08/ABTest分流算法设计/","link":"","permalink":"http://example.com/2021/06/08/ABTest%E5%88%86%E6%B5%81%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"ABTest分流算法设计与实现需求有这么一个需求,我们需要建立一个实验,实验有一个实验名称.实验下面有多个分组,每个分组也有个分组名称.当我们创建一个实验的时候,需要同时建立一个或多个分组,并且每个分组都有一个百分比的属性,代表当我们进入一个实验的时候,选择某个分组的可能性有多大,所有的分组的百分比之和为100%. 目的通过实现实验和分组,我们可以把它应用在分流策略中,比如ABTest.考虑这样一个场景,我们要对比三个推荐算法带来的效益,此时我们会给这三个推荐算法分配一定的比例,然后让每次推荐都按这个比例来执行不同的算法,最终再根据一定的换算来统计算法带来的效益. 实现思路我们可以将分组的百分比看成是几条线段,假如总共有100米长,每条线段有一定的长度,我们根据标识(比如hashCode)来对100取模,最终这个数字一定会落在某一条线段上,也就是某个分组上.在我的实现中,将以1000作为模.在这里,我将使用Java语言实现一个简单的分流算法. 完整代码地址为:https://github.com/CodeShowZz/abtest,所有的接口都进行了初步的测试. 步骤一:定义模型123456789public class Lab &#123; private String key; private String name; private List&lt;Group&gt; groupList;&#125; 12345678910111213141516171819202122public class Group &#123; private String key; private String name; /** * 百分比 **/ private BigDecimal ratio; /** * 分组的开始位置 */ private int start; /** * 分组的结束位置 */ private int end;&#125; 步骤二:实现分流工具类这里要实现一个工具类,能够将百分比转换成区间. 12345678910111213public static void assignRangeByRatio(Lab lab) &#123; List&lt;Group&gt; groupList = lab.getGroupList(); int current = 0; for (Group group : groupList) &#123; BigDecimal ratio = group.getRatio(); int count = ratio.multiply(range).setScale(0, RoundingMode.HALF_UP).intValue(); int start = current; int end = current + count - 1; group.setStart(start); group.setEnd(end); current = end + 1; &#125; &#125; 步骤三:获取标识取Hash,将其分配到某个分组中.在我的实现中,如果两次传入的标识key是一样的,那么计算出来的分组位置也是一样的.所以使用这个Hash算法时,可能要根据具体的应用场景来取一个具体的key,比如对于一个用户来说,取值如果要和上次相同,那么可以使用用户id来作为key,如果取值要随机,那么可以取时间戳或者其它属性作为key. 这里我的Hash算法借鉴(可以说是照抄)了HashMap中的Hash算法. 123public static final int hashCode(String key, String value) &#123; return Math.abs(Objects.hashCode(key) ^ Objects.hashCode(value)); &#125; 接着,使用上面的模型并且结合Hash算法来实现分组.分区函数的返回结果就是某一个分组. 1234567891011public static Group partition(String key, Lab lab) &#123; int hashCode = hashCode(key, lab.getName()); int position = hashCode % range.intValue(); List&lt;Group&gt; groupList = lab.getGroupList(); for (Group group : groupList) &#123; if (group.getStart() &lt;= position &amp;&amp; group.getEnd() &gt;= position) &#123; return group; &#125; &#125; return null;&#125; 步骤四:测试通过上面的三个步骤,一个简单的分流算法实现完成.现在我们来假设这样一个场景:据统计,50%的人喜欢数学,30%的人喜欢语文,20%的人喜欢英语,那么现在我们随便找一个人,来猜测它喜欢哪个科目,那么我们就可以使用上面的程序,测试程序如下. 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; Lab subject = new Lab(); Group math = new Group(); math.setRatio(new BigDecimal(0.5)); math.setKey(&quot;math&quot;); math.setName(&quot;数学&quot;); Group chinese = new Group(); chinese.setRatio(new BigDecimal(0.3)); chinese.setKey(&quot;chinese&quot;); chinese.setName(&quot;语文&quot;); Group english = new Group(); english.setRatio(new BigDecimal(0.2)); english.setKey(&quot;english&quot;); english.setName(&quot;英语&quot;); List&lt;Group&gt; groupList = Arrays.asList(math, chinese, english); subject.setGroupList(groupList); subject.setKey(&quot;subject&quot;); subject.setName(&quot;学科&quot;); SplitFlowUtil.splitFlow(subject); Group res = partition(&quot;the boy maybe like math&quot;, subject); System.out.println(res); res = partition(&quot;i am a programmer&quot;, subject); System.out.println(res); &#125; 继续思考很明显,上面的这个程序其实算是一个通用程序,如果设计的算法更加的快捷,API接口更加易用,它完全可以作为一个公司内部的服务来提供给别人调用.所以现在我们要思考如何将它改进成一个公司内部可以使用的程序. 改进一:建立微服务提供添加、更新、删除、查询、分流五个接口来对实验进行操作,在这里使用Restful接口来提供这项服务. 改进二:将模型数据存储到Mysql上面的测试程序只是在本地构造程序,我们可以将模型数据映射成表,然后存储到数据库中,然后通过UI界面来进行CRUD,这一点很容易就可以做到,不再赘述. 改进三:引入Zookeeper很明显,既然要在公司内部使用,那么要保证每个实验都是隶属于某个项目的,首先要保证实验的唯一性,而这又能看出有很明显的层级结构,所以可以引入Zookeeper来存储这些模型数据,而上面的Mysql则用于冗余模型数据. 开始第一次改进先列一下实现上述改进所要引入的一些技术,其中改进二不在本次实现考虑范围.另外在下文中可能不会提供所有的代码,完整的代码将在最后给出Github仓库地址. 序列化框架:Kryo,用于序列化模型数据并将其存储到Zookeeper上 微服务框架:Spring Boot,用于实现一个微服务并提供Restful接口 分布式协调框架:Zookeeper及其API,用于实现模型数据的保存,并形成目录结构. Zookeeper在工程中实现对zookeeper api的调用,主要考虑的操作有4种 节点增加 节点更新 节点删除 节点数据查询 出于更新的复杂性,调用方可能修改实验名称、分组名称以及分组的属性,所以在真正实现中,将使用节点删除加上节点增加来实现节点更新.在这里将不会讨论Zookeeper API的细节,假设读者已经对此有一定的了解和经验. Spring Boot构建一个Spring Boot服务是非常简单的,和上述的Zookeeper类似,我们将对外提供几个api供外部接口调用. 实验创建 实验更新 实验删除 实验查询 根据实验名称进行分流 API如下所示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 创建实验 * @param lab * @return */ @RequestMapping(value = &quot;/create&quot;, method = RequestMethod.POST) public boolean create(@RequestBody Lab lab) &#123; return labService.create(lab); &#125; /** * 根据projectKey和labKey删除实验 * @param lab * @return */ @RequestMapping(value = &quot;/delete&quot;, method = RequestMethod.POST) public boolean delete(@RequestBody Lab lab) &#123; return labService.delete(lab); &#125; /** * 根据projectKey和labKey查询实验下的分组 * @param projectKey * @param labKey * @return */ @RequestMapping(value = &quot;/query&quot;, method = RequestMethod.GET) public List&lt;Group&gt; query(@RequestParam String projectKey, @RequestParam String labKey) &#123; return labService.query(projectKey,labKey); &#125; /** * 根据projectKey和labKey还有identify来进行分流 得到某个分组 * @param projectKey * @param labKey * @param identify * @return */ @RequestMapping(value = &quot;/partition&quot;, method = RequestMethod.GET) public Group partition(@RequestParam String projectKey, @RequestParam String labKey,@RequestParam String identify) &#123; return labService.partition(projectKey,labKey,identify); &#125; /** * 更新实验 * @param lab * @return */ @RequestMapping(value = &quot;update&quot;,method = RequestMethod.POST) public boolean update(@RequestBody Lab lab) &#123; return labService.update(lab); &#125; 我在模型数据中又引入了几个参数: 1234567891011121314/** * 分流需要的参数,由调用方传入,调用方决定分流所使用的标识 */ private String identify; /** * 某个项目的标识,在ZK中是第一级目录,以来区分各个项目的实验 */ private String projectKey; /** * 在进行更新时,需要传入变更前的实验分组key,以便于删除原来的实验 */ private String oldKey; 这几个参数的作用已经通过注释来表达,这样可以使得服务更加通用和简单. 测试实现完上述的两个改进之后,我们就可以打开PostMan或者其它Http请求工具来对我们提供的接口进行测试了,这里对如何测试不进行展开. 总结在这篇文章中,介绍了一个简单的分流算法的设计以及实现,当然程序还存在很多不足之处,比如异常处理,参数校验,又或者是无法实现多层的分流,这都是值得改进的地方,希望以后有机会再进行改进(程序员经常说的一句话就是下次一定😄).","categories":[{"name":"abtest","slug":"abtest","permalink":"http://example.com/categories/abtest/"}],"tags":[{"name":"abtest","slug":"abtest","permalink":"http://example.com/tags/abtest/"}],"author":"John Doe"},{"title":"Kafka如何保证发送消息有序","slug":"Kafka如何保证发送消息有序","date":"2021-06-07T17:33:00.000Z","updated":"2021-06-07T17:46:56.766Z","comments":true,"path":"2021/06/08/Kafka如何保证发送消息有序/","link":"","permalink":"http://example.com/2021/06/08/Kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F/","excerpt":"","text":"Kafka如何保证发送消息有序生产者有一个配置项叫做max.in.flight.requests.per.connection,这个参数能够保证生产者在收到服务器响应前能发送多少个消息.举个例子,如果我们把这个参数设置为1,那么当生产者发送了一个消息给服务器之后,只能等到服务器响应生产者发送的这个消息之后,生产者才能够继续向服务器发送消息. 优点和缺点优点:将max.in.flight.requests.per.connection配置为1后,能够保证消息的有序性. 缺点:缺点也很明显,因为必须要等待服务器响应才能够发送消息,那么就会降低生产者发送消息的吞吐量,除非有严格的顺序要求,才会将这个选项配置为1.","categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"author":"John Doe"},{"title":"Kafka Producer三种使用方式","slug":"Kafka-Producer三种使用方式","date":"2021-05-31T20:32:00.000Z","updated":"2021-06-16T05:33:30.544Z","comments":true,"path":"2021/06/01/Kafka-Producer三种使用方式/","link":"","permalink":"http://example.com/2021/06/01/Kafka-Producer%E4%B8%89%E7%A7%8D%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F/","excerpt":"","text":"Kafka Producer三种发送方式Kafka生产者使用起来非常简单,它主要有三种发送方式,分别是 Fire And Forget(发送但不等待结果) 同步发送,等待结果返回 异步发送,结果通过回调接口接收 这里,通过编写一个程序来展示上述的三种方式,如无环境,请参考文档自行搭建,这里就不阐述了. 具体的细节可以参考Kafka API,这里主要要表达的是三种使用方式,完整的代码参见Github仓库 Fire And Forget1234public void fireAndForget() &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic&quot;, &quot;key&quot;, &quot;value&quot;); producer.send(record);&#125; 同步发送123456789public void sendMsgSync() &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic&quot;, &quot;key1&quot;, &quot;value1&quot;); try &#123; RecordMetadata recordMetadata = (RecordMetadata) producer.send(record).get(); System.out.println(&quot;send sync return:&quot; + recordMetadata); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 异步发送123456789101112131415public void sendMsgAsync() &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic&quot;, &quot;key2&quot;, &quot;value2&quot;); try &#123; producer.send(record, new Callback() &#123; public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if(e != null) &#123; e.printStackTrace(); &#125; System.out.println(&quot;send async return:&quot; + recordMetadata); &#125; &#125;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;","categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"author":"huangjunlin"},{"title":"Elastic Search安装","slug":"ElasticSearch安装","date":"2021-05-30T16:56:00.000Z","updated":"2021-06-13T23:30:43.627Z","comments":true,"path":"2021/05/31/ElasticSearch安装/","link":"","permalink":"http://example.com/2021/05/31/ElasticSearch%E5%AE%89%E8%A3%85/","excerpt":"","text":"Elastic Search安装俗话说:工欲善其事,必先利其器.本篇文章就来介绍一下如何安装ES,并启动它来实现万里长征的第一步.此次演示的是在Mac系统上安装ES. 首先要保证拥有Java环境,版本为jdk8以上. 其次,找一个目录,执行如下命令curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.16.tar.gz来下载5.6.16版本的ES. 接着执行tar -xvf elasticsearch-5.6.16.tar.gz将下载的文件解压成文件夹.最终的效果如下截图所示. 接着执行cd elasticsearch-5.6.16/bin,运行./elasticsearch,可以看到启动后的效果,如下截图 到此,大功告成!","categories":[{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/categories/elastic-search/"}],"tags":[{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/tags/elastic-search/"}],"author":"huangjunlin"},{"title":"一个key值如何在redis集群中找到存储在哪里","slug":"一个key值如何在redis集群中找到存储在哪里","date":"2021-05-18T02:34:00.000Z","updated":"2021-05-31T20:50:17.294Z","comments":true,"path":"2021/05/18/一个key值如何在redis集群中找到存储在哪里/","link":"","permalink":"http://example.com/2021/05/18/%E4%B8%80%E4%B8%AAkey%E5%80%BC%E5%A6%82%E4%BD%95%E5%9C%A8redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%89%BE%E5%88%B0%E5%AD%98%E5%82%A8%E5%9C%A8%E5%93%AA%E9%87%8C/","excerpt":"","text":"一个key值如何在redis集群中找到存储在哪里首先,一个集群只有在16384个槽全部都分配给了集群下的节点时,集群才能处于正常运行状态.现在假设我们集群中有三台机器,代号分别是7001,7002,7003,并且假设槽的分配关系如下: 7001负责处理0-10000 7002负责处理10001-13000 7003负责处理13001-16383 比如我们向7001执行一条set msg hello命令,这个时候7001所在的节点会调用函数CRC16(key) &amp; 16383,这个函数用于计算key的CRC-16校验和,然后对槽号取模,最终计算出分配给哪个槽.这个时候有两种情况,要么计算出来的槽号由自己负责,要么由其它节点负责.如果是由自己负责,那么直接执行命令,返回结果就完事了.但是如果是由别的节点负责,那么会向客户端返回MOVED错误,形式为MOVED 槽号 实际处理槽节点的ip:实际处理槽的节点port,指引客户端往真正处理槽的节点去发送命令. 另外还要注意的一点是,除了上述的两种情况外,其实还会有另外一种情况.考虑这样一种场景,当集群里面加入了新节点7004,并且将7001负责的0-5000分配给了7004,那么这个时候会执行重新分片操作,在这个过程中,可能存在一种情况就是槽号在分片的过程中,槽里面的键值对并没有全部分配到新的节点中,而是部分分配到新的节点中.举个例子:比如原本由7001负责的槽5000里有两个键值对,分别是(key1,value1),(key2,value2),假设(key1,value1)已经分配给了新的节点7004.这个时候,当我们向7001发送命令get key1的时候,7001是无法获取到值的,但是它计算出来的槽号确实是属于它自己管理的槽号,因为这个时候槽中的键值对并没有完全转移完成,所以槽的归属权还是自己的,只有当槽完全转移完成,槽才会属于7004.在这种情况下,7001是无法获取到值的,所以7001会再判断是否槽处于迁移状态,如果是的话,会返回一个ASK错误,形式为ASK 槽号 实际拥有键值对的节点的ip:实际拥有键值对的节点的port,让客户端向真正拥有该键值对的节点去发送请求.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"huangjunlin"}],"categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/categories/zookeeper/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"其它","slug":"其它","permalink":"http://example.com/categories/%E5%85%B6%E5%AE%83/"},{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"},{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"},{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"},{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/categories/spring-cloud/"},{"name":"数据源","slug":"数据源","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%BA%90/"},{"name":"数据权限","slug":"数据权限","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"},{"name":"abtest","slug":"abtest","permalink":"http://example.com/categories/abtest/"},{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/categories/elastic-search/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/tags/zookeeper/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"其它","slug":"其它","permalink":"http://example.com/tags/%E5%85%B6%E5%AE%83/"},{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"},{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"},{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"},{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/tags/spring-cloud/"},{"name":"数据源","slug":"数据源","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%BA%90/"},{"name":" 数据权限","slug":"数据权限","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"},{"name":"abtest","slug":"abtest","permalink":"http://example.com/tags/abtest/"},{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/tags/elastic-search/"}]}