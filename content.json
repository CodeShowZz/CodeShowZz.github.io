{"meta":{"title":"DreamLin's blog","subtitle":"","description":"","author":"DreamLin","url":"http://example.com","root":"/"},"pages":[{"title":"关于我","date":"2021-05-18T12:28:45.000Z","updated":"2023-04-23T17:36:13.637Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"我是 股市的传说 交易的王者 股民膜拜我 机构针对我 证监会调查我 2022年 我潇洒的离开了股市 只留下了9个字:巴菲特也就那么回事 联系vx:huang_04"}],"posts":[{"title":"test.md","slug":"test-md","date":"2023-04-23T18:51:31.000Z","updated":"2023-04-23T18:51:31.153Z","comments":true,"path":"2023/04/24/test-md/","link":"","permalink":"http://example.com/2023/04/24/test-md/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"test2.md","slug":"test2-md","date":"2023-04-23T18:49:48.000Z","updated":"2023-04-23T18:49:48.218Z","comments":true,"path":"2023/04/24/test2-md/","link":"","permalink":"http://example.com/2023/04/24/test2-md/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"HashMap的实现原理","slug":"HashMap实现原理","date":"2023-04-23T17:07:07.000Z","updated":"2023-04-23T17:07:07.089Z","comments":true,"path":"2023/04/24/HashMap实现原理/","link":"","permalink":"http://example.com/2023/04/24/HashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"HashMap的实现原理是什么样的?在面试中经常会遇到这个问题.那这个问题实际上是要问哪些问题呢,能不能将这个问题细化一下,表达得更精确一点.在我看来,通常要要问以下几个问题: HashMap的数据结构 HashMap的hash方法是什么样的,为什么要这么实现 HashMap的负载因子是什么,有什么用 HashMap是如何扩容的 HashMap的get方法和put方法是如何实现的 把这些问题弄懂之后,对HashMap的原理算是很大程度的理解了. HashMap的实现原理首先,我直接通过源码的方式提取出一些HashMap中一些重要的字段,在一些字段上标明了注释,在这里不关注太多的细节,只把最重要及最精华的部分拿出来讲,源码中有太多的细节实现,我觉得没有必要死抠一些可能暂时不太有用的东西,能把大体的实现思路说清楚是我的目的. 1234567891011121314151617181920212223242526272829//节点数组,在必要的时候会进行扩容,长度总是2的幂次方transient Node&lt;K,V&gt;[] table;static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; //键的hash值 final int hash; final K key; V value; //下一个节点,以链表的形式连接 Node&lt;K,V&gt; next;&#125; //当链表中的数量大于等于8时,为了提高查询性能,将会转成红黑树来进行存储static final int TREEIFY_THRESHOLD = 8;//默认的负载因子,用于判断是否扩容static final float DEFAULT_LOAD_FACTOR = 0.75f;//默认的初始容量static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//在Map中的键值对数量transient int size;//下一次扩容的大小int threshold;//负载因子final float loadFactor; HashMap由数组+链表(可能转换成红黑树)组成,数组的优点是可以通过索引直接进行访问,复杂度为O(1),那这个数组要多大才合理呢?另外索引该怎么计算出来才合理呢?很显然,数组不可能无限大,因为我们的内存有限.另外,索引是通过计算hash值来实现的,最理想的情况是我们通过key算出一个hash值,然后对数组的大小取模,恰好每次都能得出一个之前没有用到的索引位置上.但是显然这是不可能的,因为即使hash值不同,但是数组的大小是固定的,那么必然算出的索引有可能会一样,这就导致了碰撞,这样一来就必须在数组的一个索引位置上存放多个元素(如果key是一样的,那么对旧key对应的值进行替换).但是,我们一定会想尽可能的均匀地把元素放到数组的每个位置中,尽量少发生碰撞,因为如果碰撞太多,那么我们设置值和取值就需要遍历链表来完成.这个时候,如何计算hash值使得元素能更均匀分布变得至关重要. 在源码中,hash值的计算是这样的: 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 计算索引的代码是这样的: 12//n为当前数组大小index = (n - 1) &amp; hash 这里把这段代码放在一起是因为它们是有关联的.首先,计算索引的方式使用了&amp;操作,比如数b是2的幂次方的时候,将这个数减去-1,与另外一个数a做&amp;操作,计算的结果就相当于a%b,并且这样的效率会更高,例如17&amp;4=1,7&amp;4=3. hash值的计算是这样的:先计算hashCode,将其右移16位,再与本来的hashCode做异或操作,如果直接用hashCode去做&amp;运算,那么可以很明显的看出是低位的值决定了最终计算出来的值是什么,因为高位&amp;0得出的值永远是0.右移的话,可以让高位也参与到整个hash的计算过程.这样的计算方式可以使得计算出来的值的分布更加随机,这跟我们上面所叙述的不谋而合.","categories":[],"tags":[]},{"title":"五种IO模型","slug":"五种IO模型","date":"2023-04-23T17:06:00.000Z","updated":"2023-04-23T18:57:48.858Z","comments":true,"path":"2023/04/24/五种IO模型/","link":"","permalink":"http://example.com/2023/04/24/%E4%BA%94%E7%A7%8DIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"五种IO模型现在,我来介绍一下我所理解的五种IO模型.在本篇文章中,我们以Input为例来进行讲解.其中五种IO模型分别是 阻塞式IO 非阻塞式IO IO多路复用 信号驱动IO 异步式IO 我会在这篇文章尽可能的把这几个IO模型讲得通俗易懂. 阻塞式IO当我们的应用程序要进行数据的读取时,我们通过系统调用去获取数据,通常程序会调用一个recvfrom,在阻塞式IO的场景下,发出这个信号之后,应用程序就会阻塞,直到数据准备好,并且从内核空间复制到应用程序数据缓冲区之后,这个读取过程才算完成.整个过程分成两步,一是等待数据准备好,二是数据要有一个从内核空间到应用程序缓冲区的一个过程,这两个过程进行完之后,我们的应用程序才可以读取数据,在这之前,我们的应用程序都是阻塞的,所以我们称之为阻塞式IO.![阻塞式IO](截屏2023-04-23 下午11.45.16.png) 在这里引用一张Unix网络编程书中的一张图片,我们可以看到进程发出一个recvfrom在整个过程中都处于阻塞状态,第一个小过程是等待数据,第二个小过程是从内核空间复制数据到用户程序缓冲区,直到数据准备完成之后,内核返回一个OK的响应. 在这里,我将用一个生活的场景来类比这个过程,比如我们去饭馆吃饭,通常我们会有两个过程,第一个过程是点菜,第二个过程是上菜,并且你和老板约定说:”你什么也不做,就干在这等着,菜好了就端上来,并且上菜过程你也不做其他事情”.那么阻塞的场景好比是我们去点菜,然后菜品制作的过程我们一直在等,菜品制作完之后我们还等着上菜,整个过程都一直处在等待状态. 但是有时候,你不想这样干等着,你想玩会手机,你和老板约定说:”你们厨房先做菜,我会隔一段时间问一下菜准备好了没,如果菜准备好了,就帮我端上来,上菜过程我不玩手机,我就等着上菜“这样,你就可以玩会手机,然后过几分钟就问一下菜准备好了没,这样你就可以用等待做菜的时间去玩手机了,不必一直等着,但是你还是得不定时的去问一下老板.这就是我们即将介绍的非阻塞式IO. 非阻塞式IO非阻塞式IO是应用程序发出一个recvfrom指令,但是如果数据没有准备好的话,会立即得到一个响应,应用程序会隔一段时间再次发出指令,直到数据准备好的时候,数据就会从内核空间复制数据到用户程序缓冲区,这个复制数据的过程和阻塞式IO是一模一样的,应用程序同样是阻塞的.但是我们的第一个等待数据准备的过程中,应用程序是可以做其他事情的,所以非阻塞式IO实际上指的是第一阶段是非阻塞的,而第二个阶段还是阻塞的.这里继续引用Unix网络编程书中的一张图片. ![非阻塞式IO](截屏2023-04-24 上午12.19.08.png) 我们把程序不时发出recvfrom的这个过程叫做轮询,当发出recvfrom的时候,此时数据还没有准备好,内核返回一个EWOULDBLOCK的响应,表示数据还没准备好.等哪一次准备好了,就会进行数据复制的过程,复制完成后,会返回一个OK的响应.很明显,我们可以看出非阻塞式IO和阻塞式IO的区别仅在第一个过程,但是第一个过程变得更加的灵活. 通过一个类比的例子,我们应该可以很清晰的知道一个应用程序读取数据的两阶段了,接下去的几个模型中,不再进行这种类比,因为我觉得只要理解了这两个过程,直接阐述具体模型更容易理解,类比就更复杂了. IO多路复用废话不多说,先给出一张IO多路复用的图,英文叫做IO Multiplexing. ![IO多路复用](截屏2023-04-24 上午12.37.00-2276000.png) 首先,我们可以很直接的从图中可以看出程序首先调用了一个select,然后等待数据准备好,但是此时并没有直接进入数据复制的过程,而是还需要再发出一个recvfrom,才会进行数据复制的过程,第一个阶段是阻塞的,第二个阶段还是阻塞的,跟第一种阻塞式IO几乎没有太大的差别,但是很明显还是可以看出,在等待数据准备好之后,应用程序可以选择去进行数据复制,也可以选择后面再进行数据复制,相当于把阻塞式IO分成了两个阶段来做,提高了灵活性.但是除此之外,第一个阶段其实也是暗藏玄机的,在这个过程中,我们可以等待多个输入完成,比如在网络编程中,是等待多个套接字,而上面的两种模型,通常是等待一个输入准备好.在这个模型中,如果多个输入中,如果没有一个输入可以读取,那么应用程序将会一直等待下去,直到有大于或等于一个输入可以读取,那么将返回给应用程序一个readable响应,通常根据这个响应,我们会知道哪些输入已经准备好,哪些输入还没有准备好,然后我们根据这些准备好的输入来进行数据的复制过程,这个数据复制过程和上面两种模型的数据复制是一模一样的,这里不再阐述. 之所以叫做IO多路复用,我认为主要来自于第一个阶段可以等待大于一个或多个输入准备好,一旦有大于等或于一个输入准备好的时候,程序将不再处于阻塞状态,可以立即处理一个或多个准备好的输入,这种处理比阻塞式IO更加的灵活.","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[]},{"title":"mysql之MVCC原理","slug":"Mysql之MVCC原理","date":"2022-11-23T16:14:00.000Z","updated":"2022-11-23T16:32:10.738Z","comments":true,"path":"2022/11/24/Mysql之MVCC原理/","link":"","permalink":"http://example.com/2022/11/24/Mysql%E4%B9%8BMVCC%E5%8E%9F%E7%90%86/","excerpt":"","text":"众所周知,mysql的数据库默认隔离级别是可重复读,那么mysql是如何做到可重复读的呢.这篇文章,就来谈谈mysql的多版本控制机制,也就是MVCC. 从版本链开始说起假设我们现在有一张表t,里面有id,name,id是主键,这个表目前是空表.现在我们开启一个事务,往里面插入一条id=1,name=mysql的数据.mysql在聚簇索引中,除了保存数据外,还保存了一个trx_id和一个roll_pointer字段,trx_id表明数据是哪个事务操作的该条数据,roll_pointer指针指向该数据的前一个版本.假设我们刚刚进行插入语句时的事务id为100,那么聚簇索引中这条数据的trx_id为100,roll_pointer指向一条insert undo记录,可以简单理解这条记录和我们的数据是一模一样的,除了它的roll pointer是空的.可以简单用一张图来描述我们现在的数据状态: 然后,假设我们开启了一个新的事务,对id=1的记录对应的name进行修改,改成了java,事务id为200.接着,又开启一个新的事务,将name又改成了spring,事务id为300.对于mysql来说,一条记录是有保存修改历史的,就像我们用的git一样,有版本的概念.mysql同样也是这样的思想,经过两次修改,最新的name是spring,trx_id为300,但是事务id为100和200的两条历史记录也会保存下来.就我们上面的上次修改来说,最终形成的数据状态是这样的: 和insert操作不一样,update产生的历史版本记录的roll_pointer是有指向的,指向它的上一个版本.我们可以看到,新的数据已经是事务id为300的数据了,越上面的数据版本越新,这就是版本链的概念. 可重复读的实现说完了版本链,现在来讲讲可重复读是如何实现的.假设我们上面所说的这个场景的事务都已经提交了,那么我们开启一个新的事务,假设事务id是400,那么我们用普通的select语句去读取id为1的记录,我们再假设整个系统目前就我们这么一个事务,那么读出来的name就是spring.那假如此时系统里面还有其他事务也在修改id为1的记录呢?这里就要引出一个快照的概念,其实就是版本链上哪些数据可见的规则.在这个隔离级别下,快照会在事务开始时生成一次,并且只生成这一次.假如我开启事务id为400的事务时,系统中还有一个事务把id为1的name修改为linux,但处于未提交状态,事务的id为399.这个未提交状态是在我们开启事务id为400的时候知道的,如果下一秒这个事务都提交了,我们的快照也是认为没有提交的.另外,假设此时快照生成后,还有事务id比400还大的事务被创建来修改id为1的记录,即使这些事务提交了,快照也会认为它们没有提交,因为在快照看来,时间就仿佛冻结在开启事务的那一刻. 比如说,当事务id为400的事务对id进行一个select name from t where id =1的查询,那么获取到的值是spring.然后事务id为399的事务提交了,事务id为400的事务再次运行同样的查询语句,那么返回的name还是spring.因为我们这两次查询用的是同一个快照,那么即使事务id为399修改了数据之后提交,我们也认为它是没有提交的,简单的理解就是:只认为在事务开启前那一刻已提交的数据才是查得到的.这样,无论事务id为400所在的事务查询多少次,查出来的name永远都是spring.除此之外,在事务id为400的事务在查询中,可能会有新的事务也来修改这个值,但是我们知道,快照对这些事务是不可见的,所以更加读不到它们的修改.但是,这些当时还在运行中的事务和后来运行的事务对id=1的记录的修改是会加入到版本链中的(即使事务还没提交),我们只是依据快照,自版本链的顶部往下搜索,过滤掉一些不可见的记录,直到找到第一条符合的记录,也就是快照开启时已提交的最新的一个版本对应的记录.其实可以画图,将所有的这些记录都加入到版本链中,来模拟这个过程.但是画图实现太费时间了,所以我就懒得画了. 如果是读已提交隔离级别呢对于已提交隔离级别来说.也是生成快照,但是它和可重复读不一样的一点是,只要查询,就生成一个新的快照.这样一来,因为快照会变化,所以中途如果有数据提交了,在这个隔离级别下是可以读得到的.比如将上面的例子换成在这个隔离级别下进行实验,第二次查询出来的name将会是linux.因为第一次查询时,事务id为399的事务还没有提交,生成的快照认为此条数据是读不到的.但是第二次查询,又生成了新的快照,事务id为399的事务提交了,再次查询就可以读到linux这个值. 所以,这两个隔离级别下都是读已提交的数据.只是因为快照的生成时机不一样,导致最终读取的数据不一样而已.","categories":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]},{"title":"怎么保证kafka的消息不丢失","slug":"怎么保证kafka的消息不丢失","date":"2022-11-08T14:59:00.000Z","updated":"2022-11-08T16:44:27.637Z","comments":true,"path":"2022/11/08/怎么保证kafka的消息不丢失/","link":"","permalink":"http://example.com/2022/11/08/%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81kafka%E7%9A%84%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/","excerpt":"","text":"从一个场景开始这篇文章,我来谈谈怎么保证kafka的消息不丢失这个话题.首先,我来假设一个场景,现在你要处理客户往银行卡充值的事件,比如A用户向银行卡充值100元,200元等类似的事件.假设你使用一个项目来把这个事件发给kafka,另外一个项目从kafka获取这个事件,然后对事件进行处理,往数据库里客户的银行卡上累加金额.这个场景如果把事件丢失了,那就是事故了.我们的系统绝对不允许有任何的差错,一旦出错,客户会投诉,别人就不敢用这个系统了.有了这样的场景,我们必须认真的去考虑怎么保证消息不丢失,并且能正确处理这些消息.我们的整个架构可以用下图来表示: 我们来简单举个例子,什么情况会丢失消息. 假如用户发起充值,这个时候项目A要发送充值事件到kafka,如果项目A没等到kafka响应,就认为事件提交成功了,那么就会丢失消息.这是生产者丢失消息的情况. 如果kafka没有把消息存入到磁盘,突然发生宕机.这是kafka丢失消息的情况. 如果项目B读取到消息后,还没将数据存储到数据库,提前告诉kafka消息已经处理完成.但这时数据库存储数据时发生错误了,那么消息就丢失了.这是消费者丢失消息的情况. 这里只是举点简单的例子来表明消息丢失的情况,当然还会有很多场景也会造成消息的丢失,但是万变不离其宗.这里想表达的是,消息会不会丢失,不只是kafka的责任,生产者和消费者也承担着责任.要保证消息不丢失,这些环节都得考虑. 生产者怎么保证消息不丢失生产者要保证消息不丢失,一定要等到kafka响应成功了,才放弃发送消息.如果消息发出去,比如kafka目前不可用了,那我们要继续重试去发送消息.比如网络异常了,在10秒内没有收到kafka的响应,我们也要重试去发送消息.总之生产者遇到错误了,解决之道就是不断重试(当然,在实际场景要灵活变通,我在这里有点激进,主要是想传达这种思想).直到生产者收到kafka的成功响应了,我们才能认为消息已经成功发送到kafka了.但这可能还不够,生产者还可以让kafka多做点事情,生产者可以要求kafka要把消息备份多少份,才能算消息发送成功.因为一台kafka机器(准确来说是分区,但是这里假设读者对这个概念是清楚的)还不够安全,假如宕机了,这个时候整个系统就使用不了.所以一般会有好几台kafka机器,一台负责接收消息,另外几台复制这台机器的消息,接收消息的那台kafka叫做首领,复制消息的叫做副本.假如一台宕机了,可以选举一台新的机器作为新的首领来接受请求.生产者可以要求kafka集群备份了N份,才能够向生产者发出成功响应.这个配置,在kafka术语中叫ack.在生产者端,可以配置ack参数,比如ack=3代表消息要复制3份成功后,kafka才能认为消息写入成功,给生产者返回成功响应.ack=all则表示所有机器都复制成功了,才能返回成功响应.否则生产者一直重试直到成功.当然重试有可能会发生消息重复,这个不在我们的讨论范围. 总结起来,生产者要做两件事情保证消息不丢失: 如果消息发送失败,重试直到kafka返回成功响应. 设置ack=all(或者一个比较大的值N),让kafka的所有机器(或者N台)都获取到消息,才算消息写入成功,才返回成功响应. kafka怎么保证消息不丢失消息在kafka中怎么才能不丢失呢.我们假设一个完美的情况,一台kafka接收到消息,然后另外几台kafka非常迅速的从这台kafka上复制消息,整个kafka的所有机器的消息都是同步的.但是事情不可能一直这么完美,因为网络等原因,副本有可能会复制失败,那么副本就不会有首领所有的消息,直到网络恢复正常,副本把没复制到的消息从首领那里迅速复制过来,重新变成同步的.另外首领也可能宕机,那么就要重新选出一个首领来处理消息,新的首领应该具备的特点是拥有所有的消息,但是有的副本可能还没复制到最新消息,导致消息不完整.如果让这些副本成为首领,那么可能就存在最新的消息丢失的可能性,所以我们不应该让没有所有消息的副本成为首领,在kafka的世界里,术语叫做禁用不完全的首领选举,通过配置unclean.leader.election.enable=false来制定,我们令可kafka不可用,也不能让这种副本成为首领. 另外,我们可以规定有几台kafka拥有全部的消息才能提供给生产者写入服务,比如设置为3,那么当至少有3台机器拥有所有的消息才能提供写入服务.这个在kafka中通过min.insync.replicas=N这个参数配置.再结合生产者发送的ack要求,kafka可以识别ack要求,才认为消息写入成功,给生产者返回成功响应.这样可以锦上添花. 总结起来,kafka要保证三件事情保证消息不丢失: 通过配置unclean.leader.election.enable=false禁用不完全的首领选举,不让非同步副本提供服务 配置min.insync.replicas=N,保证同步副本个数,防止首领宕机后没有其它同步副本可以提供服务,没有足够的同步副本时,不对外提供写服务. 根据ack要求,向生产者返回成功响应 消费者怎么保证消息不丢失消息目前已经安全的存储到kafka上了,消费者从kafka已经可以获取到所有的消息.这个环节可能丢失的原因是,当消费者没有成功处理消息,就告诉kafka消息处理完成,这种情况就会丢失消息,术语叫做提前提交消息偏移量.一个最简单的做法是保证消息处理成功了,再提交偏移量告诉kafka消息真正处理完成了,这样消息就真正处理到了.所以在消费者这里,没真正处理完成就是丢失,而生产者和kafka的消息丢失主要表现在数据的传输和存储上.在上面的场景中,假如消费者拉取到一个充值事件,那么只有当数据存储到数据库中了,才应该拉取下一个充值事件(这里也有点激进,其实可以把处理失败的消息存储起来,后续再处理,总之要学会变通),否则消息就会丢失.消费者在处理过程中发生错误了,就不断重试去拉取发生错误的消息,直到处理成功了,才提交偏移量,这样就能保证消息不丢失. 总结起来,消费者要做一件事情保证消息不丢失 消息处理成功了,再提交偏移量,如果处理失败,重试去拉取消息,直到处理成功. 补充在上面的场景中,充值事件到消费者这里已经不会丢失了,但是可能会出现消息重复.这个消息重复在我们的场景下也是一个事故,假如用户充值10元,我们因为消息多重复了几条,给用户充值了50元,那么公司将面临极大的损失,这绝对是不允许的.一个能想到的解决方案是给消息设置一个唯一标识,如果数据库中已经有该消息,则认为已经处理成功了.其实如果学会分析kafka消息怎么可能丢失,去分析消息怎么可能重复是非常轻而易举的事情.比如Kafka已经成功写入消息,但是给生产者返回成功响应时,网络出问题了,生产者误以为没有发送成功(比如设置了超时重试),那么就会重新发送消息,kafka就重复写入了一条消息.再比如,消费者成功处理了消息,但是偏移量还没提交,消费者宕机了,那么下一次拉取的消息就是重复的.消息重复是可以避免但无法杜绝的,如果kafka没办法做幂等,那么这个重担就只能交给消费者去做了. 另外,提供什么样的消息安全级别要根据场景来定,不同的场景适合不同的方案,我们的这种方案确实安全,但是为了安全可能会牺牲很多的性能,有可能因为太安全了,导致整个系统运行得非常的缓慢,消息处理的吞吐量很低,也会给系统带来灾难.总而言之,要结合真实的情况去制定一个合适的方案,灵活变通才是王道.","categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}]},{"title":"对JMM与happen-before的理解","slug":"对JMM与happen-before的理解","date":"2022-10-29T09:59:00.000Z","updated":"2022-10-29T10:13:04.341Z","comments":true,"path":"2022/10/29/对JMM与happen-before的理解/","link":"","permalink":"http://example.com/2022/10/29/%E5%AF%B9JMM%E4%B8%8Ehappen-before%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"","text":"Java Memory Model编写一个程序,我们首先希望它是正确的,其次是快的,如果程序不正确,那么再快也没用.尤其在多线程环境下,如果没有很好处理并发问题,那么很容易导致数据的不一致性,这将产生严重的问题.所以,理解java内存模型-JMM是非常重要的. 首先来看一下java内存模型的组成: 我们需要知道这样一个非常非常重要的事实,在CPU中运行一个线程,线程所读取到的值可能存放于寄存器,可能存放于缓存中,这些是线程自身所能看到的数据.只有堆上的数据是所有线程都能看到的,也就是说线程共享本地内存. 我来举一个最简单的情况: 假如有这么一个类: 1234567891011class Number &#123; private int a; public void set(int a) &#123; this.a = a; &#125; public void get() &#123; return a; &#125;&#125; 在单线程场景下,执行以下操作: 123Number num = new Number();num.set(3); //设置3print(num.get()); //打印3 毋庸置疑,这个操作绝对是没有问题的,对num这个对象设置3这个值,即使值没有被刷新到线程共享的本地内存中,帮它存放到缓存中,也不会出现问题. 那么考虑多线程场景,如果两个线程同时更新和读取a值: 1234Number num = new Number();num.set(3); //设置3 A线程设置num.set(4)；//设置4 B线程设置print(num.get()); //A线程读取 如果程序按照这个顺序执行,那么A线程读取的值,会是3还是4呢,其实两种情况都有可能发生.如果线程B设置的值能写入本地内存,并且线程A不读取缓存中的值,也是读取本地内存中的值,那么读取的值就是3.如果线程直接读取在缓存中的值,那么就会读到3.所以这种代码是有问题的,这种叫做可见性问题.为了解决可见性问题,java提供了一个叫做volatile的修饰语,我们将类改成: 1234567891011class Number &#123; private volatile int a; public void set(int a) &#123; this.a = a; &#125; public void get() &#123; return a; &#125;&#125; 那么按照上面程序的执行顺序会输出什么呢?答案是会输出4.volatile修饰的变量能保证对变量读取和写入都会去直接和本地内存交互,也就是说不会经过缓存,读取直接从本地缓存读取,写入直接写入本地缓存,这样以来会很容易理解为什么会读取到4了.除此之外,volatile还有一个非常重要的特性,它会刷新线程所能看到的其他值到本地内存当中.我们再来举一个例子: 1234567891011121314151617181920212223class Factory &#123; private volatile boolean flag = true; private int a; //A线程执行该代码 public void consume() &#123; while(flag) &#123; &#125; a = a - 1; //C1 flag = true; //C2 &#125; //B线程执行该代码 public void produce()&#123; while(!flag) &#123; &#125; a = a + 1; //P1 flag = false; //P2 &#125;&#125; 我们假设这里对a的操作是原子操作(其实是三个操作).那么开启两个线程执行上述代码,值都会在0和1之间徘徊,因为每当读取flag的值的时候,顺带也有把a的最新值从本地内存读出来,写入的时候也是一样的.如果volatile只能保证flag被写入本地内存,不能保证a写入到本地内存,那么a一直无法读取到另一个线程的变更,那么可能会出现a=-1或者a=-2等情况,但其实会在写入flag的值的时候,顺带写入a的值,读取的时候也是同样的道理.更新一个volatile变量,顺带把线程操作的其它变量也一起刷新到本地内存去,做这样一件顺便且合理的事情,何乐而不为呢? 但是计算机为了追求速度,会引进一些优化手段,如重排序.重排序会将一些看似没有关联的操作顺序给重新排列,以追求更高的执行速度.在上面这个代码中,P1操作可能会被排序到P2之后进行,即先更新flag的值,再更新a的值,这样一来,flag被写入到本地内存中,因为a的操作还未开始,即使被写入本地内存后还是原值,还是会产生a的值被不确定的读取问题. 为了防止这种情况,java限制对volatile的这些操作是不能被重排序的,也就是C2和P2两个操作不能被重排序.这样一来,程序就能按照我们的意愿来执行了. Happen Before说到Happen Before规则,有些人可能会被它拗口的一些规则给搞得头昏脑涨.我觉得只要在理解了内存模型和可见性的基础上,就不难理解这些规则了.多线程程序要正确执行,首先要满足可见性,其实要满足原子性,可见性使得操作的数据是最新的,而原子性保证操作的过程不会因为线程对数据的同时更新而导致错误结果.在本地内存中数据对线程都是可见的,但由于操作的顺序问题,也可能导致数据不准确,比如a=a+1这个操作就不是原子操作,它实际包含读取a,设置a=a+1,写入a三个操作,这样两个线程同时执行,可能由于顺序问题而导致a的值只更新了1次.总结起来,多线程下数据的正确性=可见性+原子性.只有同时满足这两点,程序才能执行正确.原子性主要靠锁或者CAS等操作来保护(这里的主题不是这个),可见性就靠java制定的一些规则来保护,比如Happen Before. Happen Before中的一些规则,无非就是要保证可见性,在我看来,无非就是要保证数据被及时刷新到本地内存中.上面讲解的volatile其实就具备了这样的特性,比如:对volatile修饰的值的写入操作在下一次读取时总能读到正确的值.在Java并发编程实战一书中,这个规则被描述成:对volatile变量的写入操作必须在对该变量的读操作之前执行.我不知道这个是翻译问题还是什么问题,总觉得描述得很奇怪,为什么写入一定在读之前,我可以读两次再写不行吗?我的理解是说要保证读写的可见性即可,爱怎么读就怎么读! 再举一个Happen Before规则,即进入一个同步块总能从本地内存读取到线程可见的变量的最新值,退出一个同步块总能写入变量的最新值到本地内存中.这个同步块所具备的效果,不就是volatile所具备的效果吗?再加上同步代码块能保证程序是同步执行的,所以所保护的代码段就一定不会出问题,即满足了可见性和原子性两个条件.至于重排序,比如在同步代码块中,在符合逻辑的情况下(比如变量有前后依赖关系就不能乱排),爱怎么排就怎么排. Happen Before还有其他一些规则,但总结起来,感觉就是本地内存刷新和读取规则,目的就是保证数据对所有线程的可见性,不要在一些地方直接读取缓存,或者不把缓存数据写入本地内存.所以叫Happen Before总觉得很奇怪,叫可见性规则或者Java内存刷新与读取规则可能更合适.当然,这只是个人的见解.:)","categories":[{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"}]},{"title":"快速排序之精彩解说","slug":"快速排序之精彩解说","date":"2022-09-13T19:09:00.000Z","updated":"2022-09-24T16:11:45.170Z","comments":true,"path":"2022/09/14/快速排序之精彩解说/","link":"","permalink":"http://example.com/2022/09/14/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E4%B9%8B%E7%B2%BE%E5%BD%A9%E8%A7%A3%E8%AF%B4/","excerpt":"","text":"故事开始了假设有这样一个场景,有个小学生给了你10000个数字,要你在一天之内把这些数字排好序,然后交给他.身为一个程序猿,你想到了排序算法,进而又想到了快速排序.那么快速排序是怎么实现的呢? 既然10000个数字能排序,那么6个数字也能排序,只要6个数字时,运行结果是准确的,那么这个排序算法就能通用任意个数字的排序.假设我们有这么6个数字: 3 7 2 1 4 6 .首先,快速排序要找个基准点,一般是开头的数字,在我们这个例子中是3,然后想办法把3放到数列的中间位置,使得左边的数字比3小,右边的数字比3大,然后再递归3左右两边的数组,继续使用快速排序,最后就能全局排序.所以要搞清楚,有什么办法可以做到我们要的效果.一个可行的方法是这样的:在数组的头部和尾部各放置一个指针,然后让尾巴和3进行比较,如果大于等于3,那么就往左移动,如果碰到了比3大的数,那么就停下来.另外,如果碰到了左边的指针,也要停下来,这说明我们要的效果已经达到了.说这么多,可能还不够直观,我们用图的方式来模拟这个过程. 起初,整个指针的位置是这样的: 指针继续向左移动,6比3大,4也比3大,直到遇到了1,比3小,这个时候右边的指针就停下来,并且在移动过程中并未碰到左边的指针.现在指针的位置是这样的: 同理,左边的指针如果小于等于3,也要向右移动,直到遇到比3大的数,或者遇到了右指针也要停下来.3等于3,向右移动,7大于3,停下来,现在指针的位置是这样的: 此时,我们把两个指针所指的数字交换一下,为什么要交换呢?因为我们要使得左边的数比3小,右边的数比3大,然后就形成了下面这张图: 此时,两个指针还未碰到,但是我们能看到的是:右指针右边的数据全部比3大,左指针左边的数字全部比3小.这是一个很直观的现象. 接着,我们继续移动右指针,这里的7还要和3再比较一次,当然肯定是比3大,所以一定会左移.然后2比3小,右指针又停下来了: 然后左指针用1和3比,肯定要向右移动,然后移动过去时,发现碰到了右指针: 它们撞到一起去了,这个时候我们已经达到了我们的目的,这就是左右指针停下来的一个标志,我们把开头的3和此时这个位置上的2进行一个交换,那么就变成了: 卧槽!我们达到我们的终极目的:使得3左边的数字比3小,右边的数字比3大. 接下来的故事是,保持3这个位置不动,将左边的2,1当成一个数组,右边的7,4,6当成一个数组,然后再重新对每个数组搞两个指针进行移动,切记:撞到了就是成功了.那么我们可以使用递归的方式来做剩余的事情.如果数组只剩下一个元素,那么一定要结束递归,此时的结束条件是i&gt;=j.下面将展示快速排序的代码,也就是模拟我们现在的整个过程. 快速排序代码12345678910111213141516171819202122public static void quickSort(int [] nums,int i,int j) &#123; if(i&gt;=j)&#123; return; &#125; int start = i; int end = j; int basic = nums[i]; while(i &lt; j) &#123; while(i&lt;j &amp;&amp; nums[j] &gt;= basic) &#123; j--; &#125; while(i&lt; j &amp;&amp; nums[i] &lt;= basic) &#123; i++; &#125; if(i&lt;j) &#123; ArrayUtil.swap(nums, i, j); &#125; &#125; ArrayUtil.swap(nums,i,start); quickSort(nums,start,i-1); quickSort(nums,i+1,end); &#125; 首先代码的开始是判断是不是要结束递归,紧接着,搞出两个指针,开始和开头位置的数字进行比较.代码里面始终要判断i&lt;j,因为撞到了就是成功了,无需再进行下去,另外再没撞到之前,如果双方都停下来一次之后,要交换一下所指的数字,这是为了确保右边的数字比开头的数字大,左边的数字比开头的数字小. 然后,如果i和j相遇,那么一次完整的快速排序就结束了,在遇到的地方和开头的数字进行一次交换,接着再进行左右两个数组的排序,如此递归下去,直到排完. 故事的最后你轻轻松松的排完了10000个数字,写下代码的过程只用了59秒,接着你拿起了桌上那瓶95年的可乐喝了一口,顺手掏出了手机,开始刷起了你养了多年的B站号,进入了传说中的工作休息区.","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Kafka的ack机制","slug":"Kafka的ack机制","date":"2021-07-07T16:58:31.000Z","updated":"2021-07-07T17:09:18.152Z","comments":true,"path":"2021/07/08/Kafka的ack机制/","link":"","permalink":"http://example.com/2021/07/08/Kafka%E7%9A%84ack%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Kafka的ack机制在博客中有一篇关于Kafka消息丢失和消息重复的文章,已经有对ack进行了讨论,这里再把这个概念拿出来单独说说. Kafka的ack机制实际上指的是生产者的ack配置,不同的配置对消息的处理方式不同,配置得越严格消息越不容易丢失,主要有以下几种配置: ack=0,消息一旦发送出去,就认为是发送成功了,即使Broker没有接收到消息. ack=1,一旦首领接收到消息,那么会收到发送成功的响应.但是首领有可能在消息同步到其它副本前发生崩溃,其它副本成为新的首领(即使禁用了不完全的首领选举),所以这个配置还是有可能导致消息丢失的. ack=all,这里的all等于Broker端配置的min.insync.replicas的个数,如果有等于这个个数的副本接收到消息,才能收到成功响应,但是要保证消息不可能丢失,应该是要保证所有副本都能收到消息,所以要使得min.insync.replicas等于副本个数.","categories":[],"tags":[],"author":"John Doe"},{"title":"Kafka消息丢失和消息重复,解决方案","slug":"Kafka什么时候出现消息丢失和消息重复","date":"2021-06-30T07:17:00.000Z","updated":"2021-07-07T16:50:47.529Z","comments":true,"path":"2021/06/30/Kafka什么时候出现消息丢失和消息重复/","link":"","permalink":"http://example.com/2021/06/30/Kafka%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%87%BA%E7%8E%B0%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%E5%92%8C%E6%B6%88%E6%81%AF%E9%87%8D%E5%A4%8D/","excerpt":"","text":"Kafka什么时候出现消息丢失和消息重复,解决方案消息队列的参与者无非是三个:生产者、Broker以及消费者.消息丢失和消息重复在这三个参与者当中都会出现,本篇文章以这三者的角度来叙述消息丢失和消息重复问题. 生产者消息丢失如果生产者往Broker发送消息,没有等到Broker的回复,就认为是成功了,那么就可能存在生产者的消息丢失.可能由于网络原因导致消息没有到达Broker或是其它一些异常情况.生产者发送消息有一个发送确认的概念,使用ack来进行配置. 当设置ack=0时,消息一旦从生产者端发送出去就认为是成功了.这种情况就很有可能出现消息丢失. 当设置ack=1时,会等到首领收到消息,并返回结果才算是发送成功.这种情况认为生产者已经发送成功了,对于生产者端来说不会出现消息丢失,而对于Broker来说可能存在消息丢失,主要和同步副本有关系,这个将在下面讲解. 当设置ack=all时,会等到首领及首领配置的最小同步副本都接收到消息,那么才算是发送成功,Broker通过min.insync.replics来进行配置,比如这个属性配置成2,那么要至少有2个Broker接收到消息才算成功,不会出现消息丢失,除此之外更加严格的确保消息在副本之间的一致性. 所以如果要让消息不丢失,那么可以设置ack=1或者ack=all. 消息重复消息重复是无法避免的情况,即使消息真正存放到Broker之中,Broker返回的响应结果也有可能由于网络原因出现丢失或者超时的可能性.这个时候生产者误以为自己没有发送成功,那么就有可能会重试发送消息,Broker再次接收到消息,那么消息就发生了重复. 这种消息重复只能在Broker端或者消费者端做逻辑上的去重处理. Broker消息丢失在Broker端存在三个很重要的概念:复制系数,不完全的首领选举及最小同步副本. 复制系数复制系数表示一个分区有多少个副本,通过replication.factor来进行配置,很明显这个值配置得越大,消息越不容易丢失,但是复制带来的性能损耗也越大. 不完全的首领选举不完全的首领选举表示在进行首领选举时,是否允许非同步副本成为首领,一般通过unclean.leader.election进行配置,如果配置成true,那么可能出现首领宕机,但其它副本并没有同步完成时成为新的首领,那么就可能出现消息丢失.如果配置成false,那么分区在旧首领重启之前就是不可用的,这种情况不会出现消息丢失. 但是可能出现这么一种情况,消息写入到首领之后,还没有同步到其它副本中,此时首领宕机了,但是其它副本还是认为自己是同步的,还是会进行首领选举产生新首领,所以使用这个配置并不能完全防止消息丢失,只能认为它具有一定的防止消息丢失的作用. 最小同步副本最小同步副本是在Broker端对消息同步副本个数的约束,一般通过min.insync.replicas进行配置.比如配置成2,那么要保证至少有两个同步副本时分区才能对外进行写服务,否则只能提供读服务,将最小同步副本设置成分区副本的总个数,那么在Broker端就不会出现消息丢失. 所以要让消息不丢失,可以禁用不完全的首领并且或者并且最小同步副本个数为分区副本个数. 消息重复在Broker端出现消息重复的根本原因是生产者重复发送导致的.对于Broker来说,可以根据Broker本身提供的幂等功能来进行去重. 消费者消息丢失消费者一般通过轮询的方式来获取消息,消息消费成功后就提交偏移量.如果消费者没有消费成功,但是提交了偏移量,那么就存在消息丢失的可能性,下一次拉取的消息就不会有本次消费失败的消息. 对于这种情况,要确保消息处理成功再提交偏移量,或者对于没有处理成功的消息,保存到数据库或者缓存中,稍后再进行处理. 消息重复消费者如果没有成功提交偏移量或者提交了已处理成功的消息的偏移量,那么就有可能导致消息重复.对于这种情况,要确保消息处理和偏移量提交的原子性(将偏移量作为数据库表的列,通过事务处理),或者在处理消息前判断数据库中是否已经有该消息对应的记录. 除此之外,如果Broker没有对重复的消息进行去重,消费者也置之不理,那么也会出现消息重复.对于这种情况,要根据业务标识做幂等处理.","categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"author":"John Doe"},{"title":"Zookeeper节点类型及特点","slug":"Zookeeper节点类型及特点","date":"2021-06-29T17:35:00.000Z","updated":"2021-06-30T09:34:17.850Z","comments":true,"path":"2021/06/30/Zookeeper节点类型及特点/","link":"","permalink":"http://example.com/2021/06/30/Zookeeper%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B%E5%8F%8A%E7%89%B9%E7%82%B9/","excerpt":"","text":"Zookeeper节点类型及特点Zookeeper共有四种节点类型: 持久节点.持久节点指的是在Zookeeper上进行持久化的节点,除非主动进行删除,否则节点会一直存在. 持久顺序节点.在持久节点的基础上,添加了顺序.比如创建一个持久节点,那么会自动的在路径的末尾添加一个序列号.比如连续创建两次/A/B,并表示创建的是顺序节点,那么会创建/A/B0000000000和/A/B0000000001这两个持久顺序节点,如果创建的是/A/C,那么会创建/A/C0000000002,这是因为有一个父节点在维护这些顺序节点的顺序(从0开始递增),只要是创建带有顺序的子节点,都会使用这个顺序来作为节点的后缀. 临时节点.临时节点指的是会话结束后会被删除的节点.临时节点不能有子节点,所以临时节点一定是叶子节点. 临时顺序节点.在临时节点的基础上,添加了顺序,其创建出来的节点路径规律和持久顺序节点是类似的.","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/categories/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/tags/zookeeper/"}],"author":"John Doe"},{"title":"CAP定理","slug":"CAP定理","date":"2021-06-29T08:36:00.000Z","updated":"2021-06-30T09:34:05.177Z","comments":true,"path":"2021/06/29/CAP定理/","link":"","permalink":"http://example.com/2021/06/29/CAP%E5%AE%9A%E7%90%86/","excerpt":"","text":"CAP定理CAP定理指的是在分布式环境中,只可能满足一致性、可用性、分区容错性中的其中两个,不可以三个都满足. 一致性:这里的一致性指的是强一致性,如果节点有副本,那么一旦一个节点的数据进行更新之后,那么在另外一个节点能立即获取到更新后的值. 可用性:可用性指的是在有限的时间返回正确的结果.对于不同系统来说,有限的时间是根据具体的应用场景来定义的一个合理的指标,比如对于搜索引擎而言,这个有限的时间就是一个比较短暂的时间,比如100ms.对于一个离线处理任务来说,可能长达几分钟或者几个小时.正确的结果是一个对用户来说看的懂或者说有意义的结果,比如下单成功或失败,而不是NullPointer Exception这种结果. 分区容错性:网络分区指的是两个网络之间由于网络问题,导致不同节点无法进行通信,各自形成一个子网络.分区容错性指的是即使出现网络分区,系统也能提供具有一致性和可用性的服务. 因为是在分布式环境下,那么网络分区是一个必然会出现的问题,所以在设计系统时,一般考虑的是系统的一致性和可用性.","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"author":"John Doe"},{"title":"分布式环境中的问题","slug":"分布式环境中的问题","date":"2021-06-29T08:24:00.000Z","updated":"2021-06-29T08:36:11.904Z","comments":true,"path":"2021/06/29/分布式环境中的问题/","link":"","permalink":"http://example.com/2021/06/29/%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"分布式环境中的问题现在几乎所有的软件都是分布式环境,那么分布式环境有什么问题呢? 通信问题.如果是单体应用,那么所有的程序逻辑都会在一台机器上进行处理,一般不会出现网络通信问题.但是在分布式环境,会涉及到不同机器之间的通信,所以可能会出现类似网络丢包等问题. 机器故障.故障一般指的是机器宕机或者僵死.如果是单体应用,那么故障之后会造成服务不可用,重启之后又可以继续进行服务.在分布式环境下,故障问题就可能发生在所有的机器上. 网络分区.在分布式环境中,机器可能部署在不同的网络环境中,有可能出现机器本身运行正常但网络出现分区的情况,即两个网络之间的通信链路出现了问题. 三态问题.如果是单体应用,那么一般来说一个请求的处理结果要么是成功或者失败.如果是分布式环境,那么可能由于网络原因出现第三种处理结果-超时.","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"author":"John Doe"},{"title":"Zookeeper和Redis实现分布式锁的区别","slug":"Zookeeper和Kafka实现分布式锁的区别","date":"2021-06-29T03:23:00.000Z","updated":"2021-06-29T03:34:27.545Z","comments":true,"path":"2021/06/29/Zookeeper和Kafka实现分布式锁的区别/","link":"","permalink":"http://example.com/2021/06/29/Zookeeper%E5%92%8CKafka%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Zookeeper和Redis实现分布式锁的区别在网上看了关于Zookeeper和Redis实现分布式锁的区别的一些文章,感觉可能有的文章写的这两者的区别可能跟我的理解有点出入,所以这里按照自己的理解来谈谈它们的区别: Zookeeper实现分布式锁时,除了可以使用和redis类似的独占锁的思路,还可以监听节点变更事件,在锁可能可以获取到的情况下通知客户端再次获取锁. Redis在获取锁时,可能需要设置过期时间,而Zookeeper通常是设置一个临时节点,在会话过期的时候自动释放锁. 还有一些其它区别暂时还没有学习到,可能是一些算法设计或者一致性方面的内容,如果以后涉及到这部分内容,再进行补充.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"John Doe"},{"title":"观察者模式","slug":"观察者模式","date":"2021-06-25T11:49:00.000Z","updated":"2021-06-25T12:01:22.011Z","comments":true,"path":"2021/06/25/观察者模式/","link":"","permalink":"http://example.com/2021/06/25/%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"观察者模式观察者模式解决的是一种关于发布/订阅的场景,是观察者和通知者的一种交互方式.举个例子:现在很流行订阅APP的消息,那么APP就相当于通知者,我们的手机相当于观察者,当APP有消息要发布时,会遍历所有它的观察者进行消息的发送.在实际编码中,我们在描述这种关系时可能要注意将观察者抽象化,或者将通知者抽象化,如果不抽象化我们也能描述这种发布/订阅的关系,只不过代码的扩展性比较差. 代码示例https://github.com/CodeShowZz/code-repository/tree/master/design-pattern-demo/src/main/java/com/observer 这里有一个简单的代码示例,将观察者和通知者进行了一定的抽象,实际应用场景中可以是使用接口进行抽象,或者使用抽象类进行抽象,总之就是代码的具体实现可能是多种多样的,重要的事情在于要把模式体现出来和要使程序具有扩展性即可,不能拘泥于某种代码写法.","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"单例模式","slug":"单例模式","date":"2021-06-25T09:11:00.000Z","updated":"2021-06-25T10:26:32.824Z","comments":true,"path":"2021/06/25/单例模式/","link":"","permalink":"http://example.com/2021/06/25/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"单例模式单例模式是最简单的设计模式,它用来解决一个类只可以有一个唯一实例的问题,该类提供一个访问该实例的方法.单例模式需要注意的一个点是要保证在多线程的环境下,也能保证单例. 懒汉式单例如果我们在需要单例的情况再去初始化它,则叫做懒汉式单例. 通常我们会这么写一个单例模式: 123456789101112131415161718public class Singleton &#123; private static Singleton singleton; private Singleton() &#123; &#125; private Singleton getSingleton() &#123; synchronized (Singleton.class) &#123; if(singleton == null) &#123; singleton = new Singleton(); &#125; &#125; return singleton; &#125;&#125; 这么做有一个坏处,就是每次都会有锁的开销,进而有人发明了双重检查的写法: 1234567891011121314151617181920public class Singleton2 &#123; private static Singleton2 singleton; private Singleton2() &#123; &#125; private Singleton2 getSingleton() &#123; if(singleton ==null) &#123; synchronized (Singleton2.class) &#123; if (singleton == null) &#123; singleton = new Singleton2(); &#125; &#125; &#125; return singleton; &#125;&#125; 但是这种写法是有问题的,这里主要的问题在于可见性问题,有可能A线程执行了new操作,但是对象没有完全被构建,B线程获取到了这个没有完全被构建完成的对象,这样是不安全的.进而有大神发明了一个更牛逼的写法: 12345678910111213141516public class SingletonFactory &#123; public Singleton3 getSingleton() &#123; return SingletonHolder.singleton3; &#125; private static class SingletonHolder &#123; public static Singleton3 singleton3 = new Singleton3(); &#125; private static class Singleton3 &#123; private Singleton3() &#123; &#125; &#125;&#125; 这种写法能做到懒加载方式,又能保证线程安全,堪称单例模式的最佳写法. 恶汉式如果我们在程序的初始阶段就实例化单例,则叫做恶汉式单例. 写法也是简单粗暴: 123456789101112public class Singleton4 &#123; private Singleton4 singleton = new Singleton4(); private Singleton4() &#123; &#125; public Singleton4 getSingleton() &#123; return singleton; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"Mac Os Open JDK编译","slug":"Mac Os Open JDK编译","date":"2021-06-25T03:47:00.000Z","updated":"2021-06-29T07:33:34.652Z","comments":true,"path":"2021/06/25/Mac Os Open JDK编译/","link":"","permalink":"http://example.com/2021/06/25/Mac%20Os%20Open%20JDK%E7%BC%96%E8%AF%91/","excerpt":"","text":"Open JDK编译我们经常都会看一些源码,但有没有想过动手修改源代码呢?在平时的开发环境中,是无法直接修改源码的,如果要修改源码,那么就要获取源代码进行编译,想象一下我们可以修改源码,然后在看源码的时候加上一些注释,仿佛成为JDK的开发人员那样,是不是很有意思?编译JDK是一个比较繁琐的过程,这里以Mac OS系统为例. 第一步:下载Open JDK11 进入页面 https://adoptopenjdk.net/installation.html?variant=openjdk11&amp;jvmVariant=hotspot#x64_mac-jdk 下载 tar.gz 包 解压到/Library/Java/JavaVirtualMachines,如果你有其它版本的JDK则不需要设置环境变量,否则需要设置. 第二步:下载XCode编译过程需要使用到XCode.在我的系统里面,App Store的XCode版本并不兼容我的系统,所以要找到兼容本系统的历史版本进行下载.可以在https://developer.apple.com/download/all/?q=Xcode%2011.7下载. 第三步:下载Open JDK11源码在http://hg.openjdk.java.net/jdk-updates/jdk11u/下载zip包,解压到一个英文目录中. 第四步:编译进入到下载的jdk源码目录中,使用命令sh configure --with-target-bits=64 --enable-ccache --with-jvm-variants=server --with-boot-jdk-jvmargs=&quot;-Xlint:deprecation -Xlint:unchecked&quot; --disable-warnings-as-errors --with-debug-level=slowdebug 2&gt;&amp;1 | tee configure_mac_x64.log 进行编译. 在我的实际编译中,遇到了这样一个错误: 12configure: error: No xcodebuild tool and no system framework headers found, use --with-sysroot or --with-sdk-name to provide a path to a valid SDK/Users/huangjunlin/IdeaProjects/jdk11u-113c646a33d2/build/.configure-support/generated-configure.sh: line 82: 5: Bad file descriptor 此时应执行sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer后再运行上面的命令.如果编译成功,将出现类似如下代码输出: 1234Build performance summary:* Cores to use: 4* Memory limit: 8192 MB* ccache status: Active (3.7.1) 之后在这个目录使用make命令进行编译,整个过程可能会耗费一定时间,如果执行命令有问题,记得先加上sudo进行尝试,还有就是如果卡在某个地方,请耐心等待,大部分情况最终都会向下执行的,不要一开始就认为它挂了.等到出现 1Finished building target &#x27;default (exploded-image)&#x27; in configuration &#x27;macosx-x86_64-normal-server-slowdebug&#x27; 在build目录出现macosx-x86_64-normal-server-slowdebug/jdk文件夹,那么编译就完成了. 第五步:指定Idea的JDK创建一个Idea项目,在Idea的Project Structure指定SDK为刚刚编译出来的jdk,路径类似为/jdk11u-113c646a33d2/build/macosx-x86_64-normal-server-slowdebug/jdk. 第六步:下载CLionClion是一个C/C++的一个开发工具,我们要修改JDK的源码,可以借助这个工具来进行修改,这个工具跟Idea的风格很像,下载也很简单.下载完成之后,使用它打开jdk11u-113c646a33d2/src目录,我们就是在导入的文件中修改源码. 第七步:修改源码并测试找到java.c文件,添加一条输出语句,片段如下: 12345678910intJavaMain(void* _args)&#123; JavaMainArgs *args = (JavaMainArgs *)_args; printf(&quot;修改open jdk&quot;); int argc = args-&gt;argc; char **argv = args-&gt;argv; int mode = args-&gt;mode; char *what = args-&gt;what; InvocationFunctions ifn = args-&gt;ifn; 重新运行make命令,进行重新编译,这次编译速度相对来说会比较快.然后在Idea用main方法进行测试: 123456public class MyTest &#123; public static void main(String[] args) &#123; &#125;&#125; 结果输出了修改open jdk; 在网上还有一些关于在Clion打断点的文章,但好像对.java上打断点没有过多的描述,我认为对看源码参考价值不大,这部分内容后续还要再研究一下. 感觉整个过程遇到了好多坑,编译JDK还是比较麻烦的.","categories":[{"name":"其它","slug":"其它","permalink":"http://example.com/categories/%E5%85%B6%E5%AE%83/"}],"tags":[{"name":"其它","slug":"其它","permalink":"http://example.com/tags/%E5%85%B6%E5%AE%83/"}],"author":"John Doe"},{"title":"Java锁的优化","slug":"锁的优化","date":"2021-06-24T11:05:00.000Z","updated":"2021-06-25T10:24:54.402Z","comments":true,"path":"2021/06/24/锁的优化/","link":"","permalink":"http://example.com/2021/06/24/%E9%94%81%E7%9A%84%E4%BC%98%E5%8C%96/","excerpt":"","text":"Java锁的优化在JDK6,Java对synchronized进行了大量的改进,包括适应性自旋,锁膨胀,轻量级锁,偏向锁. 自适应自旋自旋指的是当线程获取不到锁的时候,不用直接进入挂起状态,而是执行一个忙循环,如果在忙循环结束之后能获取到锁,那么就可以减少线程切换的开销.自适应自旋通过统计自旋相关的一些参数信息,从而动态的调整执行忙循环的次数,甚至有可能跳过自旋过程. 锁消除如果在一个方法内部声明一个对象,并且这个对象不可能被外部的方法所访问到,但是这个对象的一些方法可能有关于锁的一些操作,这样可能会降低程序运行的性能.所以在即时编译器检查到这类操作时,会将这些关于锁操作的代码消除掉. 锁粗化通常来说,将锁的粒度控制得小一点是一个不错的做法,但是如果某些操作,频繁的用同一个对象进行加锁和解锁,那还不如将锁的范围扩大,这就是锁粗化. 锁升级这部分内容还有一些疑问没有搞清楚,留待以后补充 偏向锁轻量级锁重量级锁","categories":[{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"}],"author":"John Doe"},{"title":"ReentrantLock和Synchronized的区别","slug":"ReentrantLock和Synchronized","date":"2021-06-24T03:56:00.000Z","updated":"2021-06-25T10:24:28.976Z","comments":true,"path":"2021/06/24/ReentrantLock和Synchronized/","link":"","permalink":"http://example.com/2021/06/24/ReentrantLock%E5%92%8CSynchronized/","excerpt":"","text":"ReentrantLock和Synchronized的区别有了Synchronized,为什么还要有ReentrantLock呢?这是因为在某些特定的场景,Synchronized无法提供很好的灵活性,而ReentrantLock提供了一些更高级的功能,但是同时也有一些缺点. 两者主要的区别如下: ReentrantLock可以在获取锁失败时立即退出或者在一段时间内等待锁的获取,可以防止锁顺序死锁,而Synchronized不能. ReentrantLock可以在等待获取锁的时候响应中断,而Synchronized不能. ReentrantLock要手动释放锁,而Synchronized能自动释放锁. 在JDK5时,ReentrantLock的性能比Synchronized好.从JDK6开始,两者性能差不多. Synchronized是非公平锁,ReentrantLock既可以是公平锁,也可以是非公平锁. 如何选择应该优先考虑使用Synchronized,因为它能够自动释放锁,这样能降低危险性.只有在需要一些高级功能时,才应该考虑ReentrantLock.","categories":[{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"}],"author":"John Doe"},{"title":"JVM运行时数据区域","slug":"JVM运行时数据区域","date":"2021-06-24T03:09:00.000Z","updated":"2021-06-25T12:10:23.690Z","comments":true,"path":"2021/06/24/JVM运行时数据区域/","link":"","permalink":"http://example.com/2021/06/24/JVM%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/","excerpt":"","text":"JVM运行时数据区域JVM运行时数据区域如下图所示: 程序计数器程序计数器是线程私有的,指向当前线程执行的字节码行号,当线程挂起后恢复,就是通过这个计数器来知道下一条指令的执行位置. Java虚拟机栈Java虚拟机栈是线程私有的,在方法执行的时候,会在线程中创建一个栈帧,存放局部变量表、操作数栈、动态连接、方法出口等信息,每个方法的调用到执行完毕对应栈帧的入栈和出栈过程. 本地方法栈本地方法栈是线程私有的,和Java虚拟机栈相似,区别在于它为Native方法服务. Java堆堆是线程共享的,几乎所有的对象实例都是分配在Java堆上,另外从JDK7开始,本位于方法区的字符串常量池已经移动到了堆上. 方法区方法区是线程共享的,用来存放虚拟机加载的类型信息.运行时常量池是方法区的一部分,编译期生成的各种字面量和符号引用在类加载后会存放到运行时常量池中. 直接内存这部分区域并不属于JVM运行时数据区域,但是JAVA里的部分技术可能使用到这部分内存,如NIO,通过Native函数库直接分配堆外内存,所以在使用时要考虑本机总内存的大小.","categories":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"}],"author":"John Doe"},{"title":"JVM垃圾收集器","slug":"jvm垃圾收集器","date":"2021-06-23T16:34:00.000Z","updated":"2021-06-23T18:24:21.702Z","comments":true,"path":"2021/06/24/jvm垃圾收集器/","link":"","permalink":"http://example.com/2021/06/24/jvm%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"JVM垃圾收集器JVM垃圾收集器中比较经典的收集器如下图所示: 从图中可以看到,经典垃圾收集器目前有7种,图片米色部分的是新生代收集器,浅绿色部分是老年代收集器,横跨两种颜色的G1既是新生代收集器又是老年代收集器.除了这几个收集器之外,还有一些新版本的低延迟垃圾收集器,比如Shenandoah和ZGC收集器.接下来介绍一下各个收集器. SerialSerial是新生代垃圾收集器,具有如下特点: 采取标记复制算法进行垃圾回收 垃圾回收过程会暂停用户线程,并使用单个线程对垃圾进行回收 Serial是串行的意思,名字很好的反映了该收集器的特点. Serial OldSerial Old是老年代垃圾收集器,具有如下特点: 采取标记整理算法进行垃圾回收 垃圾回收过程会暂停用户线程,并使用单个线程对垃圾进行回收 和上面的Serial收集器对比,该收集器不同的点就是工作在老年代,垃圾回收的算法不一样. ParNew从名字中的New可以看出来这是一个新生代垃圾收集器,而名字中的Par代表的意思是并行,代表该收集器会同时开启多个线程来进行垃圾回收,所以可以把它认为是Serial收集器的并行回收版本,特点如下: 采取标记复制算法进行垃圾回收 垃圾回收过程会暂停用户线程,并使用多个线程对垃圾进行回收 Parallel Scavenge这个收集器也是新生代收集器,其基本特点与ParNew一模一样,除此之外,还有其它特别之处,可以理解为ParNew的加强版.除了ParNew的两个特点之外,特别之处在于这是一个关注吞吐量的收集器,它可以设定两个参数来控制垃圾收集器的吞吐量.首先吞吐量 = 运行用户代码时间/(运行用户代码时间+运行垃圾收集时间.)第一个参数-XX:MaxGCPauseMillis来让垃圾收集器尽可能保证垃圾收集的停顿时间小于这个指定的时间.第二个参数-XX:GCTimeRatio指定了GC时间占垃圾回收时间的比值,计算方式为GC占用时间比例=1/1+指定的值).除此之外,该收集器还提供了-XX:UseAdaptiveSizePolicy参数来根据运行情况收集性能信息来调整垃圾收集器的参数,这个叫做自适应调节策略. Parallel Old从Old可以看出这是一个老年代收集器,其特点和Parallel Scavenge是一样的,不同之处在于它采用了标记整理算法来对老年代进行收集. CMSCMS是老年代收集器,是以低停顿为目标的垃圾收集器,采用标记清除算法来清除垃圾,运行过程如下: 初始标记,标记GC Roots能直接关联到的对象,这个过程需要停顿用户线程. 并发标记,遍历整个对象图来标记能回收的对象,这个过程可以和用户线程一起并发执行 重新标记,这个过程标记的是在并发标记时产生的新的垃圾,相当于做一个修正,这个过程需要停顿用户线程 并发清除,这个过程清理标记了的对象,可以与用户线程一起并发执行. CMS被称为低停顿并发收集器,在上面耗时比较长的2和4过程中可以做到和用户线程一起执行,在耗时比较短的1和3过程需要停顿用户线程,总体来说可以认为整个过程是和用户线程一起并发执行,其思想是非常优秀的.但是由于这种设计,也有如下几个缺点: 占用CPU资源.因为与用户线程一起并发执行,那么肯定会和用户线程一起抢占CPU的执行权,这样就会导致用户线程的吞吐量下降,尤其是在CPU核心数较少的情况下. 无法处理浮动垃圾.因为在垃圾清除阶段,用户线程还在运行,这个时候老年代可能会产生新的对象,如果这个时候无法找到足够的内存空间进行对象的分配,将会出现Concurrent Mode Failure,此时将会停顿用户线程,改用Serial Old垃圾收集器来进行垃圾回收,这样停顿的时间就更长了.默认情况下,CMS会预留一部分内存空间来分配新对象,可以通过参数-XX:CMSInitiatingOccupancyFraction来指定占用内存的比例为多少时触发CMS的垃圾回收. 内存碎片导致无法分配新对象.因为CMS是基于标记清除的垃圾收集器,这会导致它产生大量的内存碎片,可能导致无法找到一块内存去分配新的对象,那么就很容易导致触发FULL GC.所以CMS提供了一个CompactAtFullCollection参数来使得进行Full GC前先进行内存碎片的整理,尝试去找到一块空间来分配新对象,但是这个过程需要移动对象,所以会停顿用户线程.","categories":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"}],"author":"John Doe"},{"title":"count(*),count(1),count(column name)的区别","slug":"count-count-1-count-column-name-的区别","date":"2021-06-23T08:56:00.000Z","updated":"2021-06-23T16:12:37.419Z","comments":true,"path":"2021/06/23/count-count-1-count-column-name-的区别/","link":"","permalink":"http://example.com/2021/06/23/count-count-1-count-column-name-%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"count(*),count(1),count(column name)的区别count(*) vs count(1)count(*)和count(1)的功能还有性能是差不多的,这里面括号里面的参数只是一个标识符,如果发现有一行数据,那么就分配一个标识符,最后看有多少标识符而已.count(1)并不是说统计第一行的数据有多少个,count(*)也不是说要把一行的所有列都扫描一遍.如果你不信,你可以试试count(-13),难道它统计的是-13列的个数? 那么它们具体是怎么去计算有多少条数据的呢?在只有主键索引的情况下,通过主键索引中索引的个数来计算有多少条数据.在有二级索引的情况下,通过二级索引中索引的个数来计算有多少条数据,如果有多个二级索引,用索引空间占用量小的二级索引来进行计算. count(*) vs count(column name)count(*)和count(column name)的区别在于count(column name)统计的是某一列字段的个数,其中字段值为NULL的不进行统计. 那么count(column name)是如何计算有多少条数据的呢?如果有column name对应的二级索引,那么就计算该二级索引的个数(字段值不为NULL).如果没有对应的二级索引,则扫描主键索引,通过判断行数据对应的column name是否为空来进行统计.","categories":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}],"author":"John Doe"},{"title":"RocketMQ消息重投和消息重试","slug":"RocketMQ消息重投和消息重试","date":"2021-06-22T17:10:00.000Z","updated":"2021-06-23T16:34:38.991Z","comments":true,"path":"2021/06/23/RocketMQ消息重投和消息重试/","link":"","permalink":"http://example.com/2021/06/23/RocketMQ%E6%B6%88%E6%81%AF%E9%87%8D%E6%8A%95%E5%92%8C%E6%B6%88%E6%81%AF%E9%87%8D%E8%AF%95/","excerpt":"","text":"RocketMQ消息重投和消息重试消息重投消息重投指的是生产者发送消息到Broker没有成功,然后进行重新发送.使用同步发送方式,当消息发送失败时,默认最多会重试两次,并且重试时会选择不同的Broker来进行消息的发送. 消息重试消息重试指的是消费者在消费消息时失败,然后重新消费消息.每个消费组都有一个Topic名称为“%RETRY%+consumerGroup”的重试队列,当消息消费失败时,将根据具体的重试级别来进行重试,默认是重试16次,并且每次的重试时间间隔逐步递增.","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"怎么保证RocketMQ消息不丢失","slug":"怎么保证RocketMQ消息不丢失","date":"2021-06-21T05:47:00.000Z","updated":"2021-06-21T06:02:29.451Z","comments":true,"path":"2021/06/21/怎么保证RocketMQ消息不丢失/","link":"","permalink":"http://example.com/2021/06/21/%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81RocketMQ%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/","excerpt":"","text":"怎么保证RocketMQ消息不丢失要保证RocketMQ消息不丢失,需要生产者、Broker以及消费者的配合. 生产者生产者在发送消息时,如果没有收到服务器的成功响应,那么就要重试发送消息,直到消息发送成功. BrokerBroker要做的事情是保证消息不丢失,当接收到生产者发送的消息时,Broker为了保证消息不丢失,会将数据同步到磁盘中,为了做到真正意义上的不丢失,需要Broker设置同步刷盘模式,必须要等到数据真的同步到磁盘上之后,再向客户端返回消息发送成功状态. 除此之外,为了提高服务的可用性,Broker通常会采用主从模式,这个时候还要保证消息真正同步到了从服务器上,需要设置主从服务器复制策略为同步复制模式,等到所有的从服务器都真正接收到数据并存储到磁盘时,再向客户端返回消息发送成功状态. 总结起来就是两点: Master保证数据同步到磁盘 Slava保证数据同步到磁盘 消费者对于消费者而言,消息不丢失意味着能够成功消费到所有消息.正常情况下,消费者成功消费之后,再向Broker返回成功消费信息,否则返回消费失败信息,使得下次还能够重新消费那些没有消费成功的消息. 效率问题为了保证消息不丢失,需要使用大量的同步策略,这样可能导致效率的低下,所以在实际生产环境中还需要结合实际业务进行权衡.","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"Kafka和RocketMQ的区别","slug":"Kafka和RocketMQ的区别","date":"2021-06-21T04:56:00.000Z","updated":"2021-06-21T05:40:02.471Z","comments":true,"path":"2021/06/21/Kafka和RocketMQ的区别/","link":"","permalink":"http://example.com/2021/06/21/Kafka%E5%92%8CRocketMQ%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Kafka和RocketMQ的区别 RocketMQ Kafka 协议和规范 拉取模式,支持TCP、JMS、OpenMessage 拉取模式,支持TCP 消息顺序 保证严格的消息顺序,并且可以优雅扩展 保证分区消息顺序 批量消息 支持,使用同步模式来避免消息丢失 支持,使用异步生产者 广播消息 支持 不支持 消息过滤 支持,使用基于SQL92的属性过滤表达式 支持,使用Kafka Stream过滤消息 服务端触发重新发送消息 支持 不支持 消息追溯 支持偏移量和时间戳指定 支持偏移量指定 高可用 使用主从模式 使用Zookeeper 消息追踪 支持 不支持 服务端重新发送消息 支持 不支持","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"Redis分布式锁","slug":"Redis分布式锁","date":"2021-06-21T03:16:00.000Z","updated":"2021-06-21T04:23:12.634Z","comments":true,"path":"2021/06/21/Redis分布式锁/","link":"","permalink":"http://example.com/2021/06/21/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"Redis分布式锁分布式锁主要用来解决分布式环境下资源的互斥访问,这篇文章就来介绍一下分布式锁. 基本思路首先要保证获取锁和释放锁的正确性,通常来说就是保证获取锁和释放锁的原子性,一般会使用SETNX key 唯一标识命令来获取锁,在Lua脚本中通过唯一标识来释放锁,这样能保证获取锁和释放锁的客户端是同一个. 考虑宕机当客户端服务获取到锁之后挂掉了,那么锁可能永远都没办法被另外的客户端获取到,所以此时要考虑给锁加一个过期时间,保证客户端服务挂掉之后锁也能释放,通常使用SETNXEXPIRE key 唯一标识 过期时间来实现. 考虑可用性假如我们的Redis实例只有一个,那么有可能Redis实例挂了,那么分布式锁服务就不可用了,所以我们现在要想办法提高可用性.一个比较可靠的方式是使用RedLock算法. RedLock算法假设我们有5个Redis实例,彼此是独立的,那么RedLock的算法的运作过程如下: 向各个Redis实例获取锁(SETNXEXPIRE key 唯一标识 过期时间),并且需要设置一个获取锁的超时时间(通常远远小于锁的过期时间),如果在这个超时时间之内没有获取到锁,那么则向另外的Redis实例获取锁. 当获取到锁的时间小于锁的失效时间,并且有超过半数的Redis实例上成功获取到锁时,锁才算获取成功. 如果锁没有获取成功(在锁过期前没有获取到锁或者没有在超过半数的实例上获取到锁),那么向所有的Redis实例进行一个解锁操作,即使某个Redis实例上并没有成功获取到锁. 持久化对分布式锁的影响假如我们没有使用持久化,那么实例重启之后key就会消失,其它服务就有可能获取到锁.比如本来在5台机器中的3台中成功获取到锁,此时挂掉一台,那么重启后没有设置key的实例就变成了3台(半数以上),其他的请求就可能获取到锁.为了保证可靠性,需要开启持久化模式,并设置fsync=always来保证数据不丢失. 锁失效了怎么办还有这样一个问题,假如我们获取到锁之后,做一个业务操作,这个业务操作在超过了锁的失效时间前还没有完成,那么其它的请求就有可能获取到分布式锁,这样破坏了锁的安全性.针对这种情况,可以在业务操作没有完成之前对锁的失效时间做一个延长,主要的步骤如下: 获取到分布式锁 在获取到锁的机器上开启定时任务,根据唯一标识对获取到的key延长过期时间(使用lua脚本). 等到操作完成之后,我们就会释放该锁,这样这个key不可能再延长过期时间. 通过这种方式,可以保证在业务操作完成之前,锁不会被释放.另外由于这个定时任务是在获取锁的机器上开启的,那么当这台机器挂了之后,也不会有定时任务对这个key进行一个过期时间的延长,还是可以满足机器挂掉之后key能够在过期时间到来时释放.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"John Doe"},{"title":"Redis缓存穿透、雪崩、击穿","slug":"Redis缓存穿透、雪崩、击穿","date":"2021-06-20T17:00:00.000Z","updated":"2021-06-20T18:09:31.382Z","comments":true,"path":"2021/06/21/Redis缓存穿透、雪崩、击穿/","link":"","permalink":"http://example.com/2021/06/21/Redis%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E9%9B%AA%E5%B4%A9%E3%80%81%E5%87%BB%E7%A9%BF/","excerpt":"","text":"Redis缓存穿透、雪崩、击穿这篇文章介绍Redis缓存穿透、雪崩、击穿问题及解决方案. 缓存穿透缓存穿透指的是存储层(如mysql)不存在相应的数据,此时如果根据key进行查询,那么在缓存层查找不到数据,所有的这类查询最终都会访问存储层,相当于缓存层是没有用的.出现缓存穿透的原因主要有以下两点: 恶意攻击 业务代码问题 解决方案对于缓存穿透主要有两种解决方案. 缓存空值,并设置一定的过期时间.在存储层查询不到数据后,在缓存层设置key对应的值为空,当下一次访问时,那么将直接从缓存层返回空对象,设置一定的过期时间是为了能够在存储层数据更新时,缓存层数据也可以得到更新. 使用布隆过滤器.通过对存储层数据对应的key进行扫描,将其放置在布隆过滤器中,因为布隆过滤器判断key是否存在的效率很高.在获取数据时,先判断布隆过滤器中是否存在该key,存在的话则访问缓存层,缓存层找不到的话再去访问存储层. 缓存击穿和热点Key问题缓存击穿指的是缓存层的key过期之后,将会访问存储层进行缓存层数据的重建.这是一个正常的现象,只不过如果一个key属于热点key,对于这个key的请求量有几百上千万,那么当key过期了,还没来得及重建缓存层数据时,大量请求都会访问存储层来重建缓存,那么这样就会给存储层带来很大压力,我们主要讨论的就是缓存击穿中的热点Key问题. 解决方案对于热点key问题,主要有两种解决方案. 分布式锁更新缓存.当缓存失效时,使用分布式锁让其中一个请求去重建缓存,其它请求进行等待,直到缓存数据重建完成.这种方式的优点是能保证缓存层和数据层的数据一致性,缺点是如果重建缓存时间太长,则会使服务阻塞. 不给key设置过期时间.当一个请求到来时,由于key没有设置过期时间,那么总能从缓存层获取到数据.然后为了保证数据能及时得到更新,我们会设置一个逻辑过期时间,当请求到来时,如果发现时间已经到达逻辑更新时间,那么我们会异步的对缓存进行重建.这样做的优点是够最大程度的解决热点key问题,缺点是数据一致性无法得到保证. 缓存雪崩缓存雪崩指的是缓存层服务不可用,这种情况就相当于没有缓存层,此时对数据的请求直接就涌向了存储层,可能导致存储层也不可用. 解决方案解决缓存雪崩的方案主要有以下几种: 保证缓存层的高可用性.对于Redis来说,比如可以使用主从、哨兵等方式. 使用降级组件来使得服务不可用时,及时响应服务请求,比如使用Java的Hystrix.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"John Doe"},{"title":"Java Class文件分析","slug":"JVM类文件分析","date":"2021-06-18T07:03:00.000Z","updated":"2021-06-18T08:41:49.175Z","comments":true,"path":"2021/06/18/JVM类文件分析/","link":"","permalink":"http://example.com/2021/06/18/JVM%E7%B1%BB%E6%96%87%E4%BB%B6%E5%88%86%E6%9E%90/","excerpt":"","text":"Java Class文件分析这篇文章的主要目的是要分析Java编译后的Class文件的组成结构,以此来加深对Class文件的理解. 将Class文件转换成十六进制如果是使用Idea进行开发,可以安装插件HexView来进行分析,直接在Class文件上点击右键使用HexView来展示即可.比如有这么一个java类: 12345678910public class TestClass &#123; private int m; public TestClass() &#123; &#125; public int inc() &#123; return this.m + 1; &#125;&#125; 使用HexView之后,解析的十六进制格式如下： Class文件分析我们将通过分析上面的这张图片来分析类文件的组成. 魔数可以看到,图片的前四个字节为ca fe ba be(咖啡宝贝),是设计者用来表示Class文件格式的方式,因为通过文件后缀来表示Class文件格式不够安全,所以使用了这种方式. 版本号第五个字节和第六个字节为00 00,这个表示次版本号,这个版本号因为用处不大,所以不关注. 第七个字节和第八个字节00 34表示主版本号,对应JDK的不同版本,比如我的是JDK1.8,对应的主版本号为52.0(34的十进制),如果Class文件的主版本号大于JDK对应的主版本号,那么虚拟机将拒绝执行. 常量池这部分比较复杂,留到后面再继续写 访问标志在常量池后面,有两个字节代表访问标志,识别类或接口等一系列访问信息,对应上图中000000e0行的00 21.目前有9个访问标志: 这里重点在于理解其思想,所以只讲解21是如何得出来的.我们该类的哪些访问标志为真,然后把这些标志对应的标志值取出来,做一个|操作,即为最终的访问标志值.用上面的这个类来说,首先定义的这个类是public的,那么要将ACC_PUBLIC对应的标志值0x0001取出来,另外ACC_SUPER这个标志固定为真,所以也要取出来,其他的访问标志为假,所以最终结果为0x0001 | 0x0020 = 0x0021.","categories":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"}],"author":"John Doe"},{"title":"Spring Cloud Ribbon负载均衡","slug":"Spring-Cloud-Ribbon负载均衡","date":"2021-06-16T07:33:00.000Z","updated":"2021-06-18T08:41:35.052Z","comments":true,"path":"2021/06/16/Spring-Cloud-Ribbon负载均衡/","link":"","permalink":"http://example.com/2021/06/16/Spring-Cloud-Ribbon%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","excerpt":"","text":"Spring Cloud Ribbon负载均衡12@LoadbalancedRestTemplate restTemplate; 当我们在RestTemplate上面加上@Loadbalanced注解时,就轻而易举的指定了默认的负载均衡策略,那么还有哪些负载均衡策略呢?具体的实现原理是什么样子的?这篇文章就来分析一下Ribbon有哪些负载均衡策略以及负载均衡策略是如何实现的. 负载均衡的大体过程负载均衡的过程主要做以下几件事: 负载均衡器根据服务名称获取实例列表 负载均衡器根据负载均衡策略选中一个实例,获取该实例信息 根据实例信息的逻辑服务名重构URI,形成真实的地址 根据真实的访问服务器 接下来,将对关键的方法进行叙述,代码很长,重点看注释部分. 当发送一个请求时,将会被LoadBalancerInterceptor类的intercept函数拦截,之后将会调用LoadBalanceClient的execute方法. 1234567891011public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); //调用loadBalancer的execute方法 return (ClientHttpResponse)this.loadBalancer.execute(serviceName, new LoadBalancerRequest&lt;ClientHttpResponse&gt;() &#123; public ClientHttpResponse apply(ServiceInstance instance) throws Exception &#123; HttpRequest serviceRequest = LoadBalancerInterceptor.this.new ServiceRequestWrapper(request, instance); return execution.execute(serviceRequest, body); &#125; &#125;); &#125; 而LoadBalancerClient的具体实现类为RibbonLoadBalancerClient,在execute方法中,将根据serviceId使用负载均衡策略从对应的实例列表中选出一台实例. 12345678910111213141516171819202122232425262728293031public &lt;T&gt; T execute(String serviceId, LoadBalancerRequest&lt;T&gt; request) throws IOException &#123; ILoadBalancer loadBalancer = getLoadBalancer(serviceId); //根据负载均衡器选择服务器 Server server = getServer(loadBalancer); if (server == null) &#123; throw new IllegalStateException(&quot;No instances available for &quot; + serviceId); &#125; RibbonServer ribbonServer = new RibbonServer(serviceId, server, isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server)); RibbonLoadBalancerContext context = this.clientFactory .getLoadBalancerContext(serviceId); RibbonStatsRecorder statsRecorder = new RibbonStatsRecorder(context, server); try &#123; //回调,这一步将进行URI地址的转换 T returnVal = request.apply(ribbonServer); statsRecorder.recordStats(returnVal); return returnVal; &#125; // catch IOException and rethrow so RestTemplate behaves correctly catch (IOException ex) &#123; statsRecorder.recordStats(ex); throw ex; &#125; catch (Exception ex) &#123; statsRecorder.recordStats(ex); ReflectionUtils.rethrowRuntimeException(ex); &#125; return null; &#125; 接着在经过一系列调用之后,将调用RibbonLoadBalancerClient的reconstructURI来重新构造URI,将形如ip:port的服务转换成http://ip:port/接口名称的形式,最终调用服务. 1234567891011121314//重新构造URIpublic URI reconstructURI(ServiceInstance instance, URI original) &#123; Assert.notNull(instance, &quot;instance can not be null&quot;); String serviceId = instance.getServiceId(); RibbonLoadBalancerContext context = this.clientFactory .getLoadBalancerContext(serviceId); Server server = new Server(instance.getHost(), instance.getPort()); boolean secure = isSecure(server, serviceId); URI uri = original; if (secure) &#123; uri = UriComponentsBuilder.fromUri(uri).scheme(&quot;https&quot;).build().toUri(); &#125; return context.reconstructURIWithServer(server, uri); &#125; 最终再回到LoadBalancerInterceptor类的intercept方法,调用ClientHttpRequestExecution的execute调用服务. 1234567891011public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); return (ClientHttpResponse)this.loadBalancer.execute(serviceName, new LoadBalancerRequest&lt;ClientHttpResponse&gt;() &#123; public ClientHttpResponse apply(ServiceInstance instance) throws Exception &#123; HttpRequest serviceRequest = LoadBalancerInterceptor.this.new ServiceRequestWrapper(request, instance); //最终的服务调用 return execution.execute(serviceRequest, body); &#125; &#125;); &#125; 负载均衡器从上面的介绍中,我们已经知道负载均衡器是在LoadBalancerClient的exetute方法的下列代码片段中工作: 12ILoadBalancer loadBalancer = getLoadBalancer(serviceId);Server server = getServer(loadBalancer); 所以接下来,我们来了解一下有哪些负载均衡器.","categories":[{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/categories/spring-cloud/"}],"tags":[{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/tags/spring-cloud/"}],"author":"John Doe"},{"title":"RocketMQ顺序消息","slug":"RocketMQ顺序消费","date":"2021-06-16T05:01:00.000Z","updated":"2021-06-16T05:38:10.819Z","comments":true,"path":"2021/06/16/RocketMQ顺序消费/","link":"","permalink":"http://example.com/2021/06/16/RocketMQ%E9%A1%BA%E5%BA%8F%E6%B6%88%E8%B4%B9/","excerpt":"","text":"RocketMQ顺序消息在一些应用场景,需要做到消息的有序性,即生产和消费严格按照FIFO的方式来进行,默认情况,RocketMQ发送消息不是有序的,所以这篇文章来介绍RocketMQ顺序消息. 原理首先消息是存放在主题中的,一个主题有多个分区,如果分区只有一个,生产者按照顺序发送消息到这个分区,消费者按照分区中消息存放的顺序消费消息,那么就能保证全局有序.如果分区有多个,那么生产者就要把消息按照一定的规则分配到指定的某个分区,消费者还是按照分区中消息存放的顺序消费消息,虽然这不能保证全局有序,但是可以保证分区有序. 例子这里的例子来自于RocketMQ官方文档.业务场景是用户下一个订单的各个步骤,因为这些步骤的顺序是至关重要的,所以必须使用顺序消息来实现. 完整代码请参考https://github.com/CodeShowZz/code-repository/tree/master/rocketmq-demo/src/main/java/com/rocketmq/order 生产者关键代码12345678SendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Long id = (Long) arg; //根据订单id选择发送queue long index = id % mqs.size(); return mqs.get((int) index); &#125; &#125;, orderList.get(i).getOrderId());//订单id 生产者在发送消息时,需要实现一个MessageQueueSelector接口,并根据业务id(比如订单id)将消息发送到指定分区中. 消费者关键代码12345678consumer.registerMessageListener(new MessageListenerConcurrently() &#123; public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; for (MessageExt messageExt : msgs) &#123; System.out.println(&quot;consumer:&quot; + new String(messageExt.getBody())); &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); 使用MessageListenerConcurrently接口方式来消费消息,在我的测试中,可以观察到消息是按照分区有序来消费的. 12345678910consumer.registerMessageListener(new MessageListenerOrderly() &#123; public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; context.setAutoCommit(true); for(MessageExt msg : msgs) &#123; System.out.println(&quot;consumeThread=&quot; + Thread.currentThread().getName() + &quot;queueId=&quot; + msg.getQueueId() + &quot;, content:&quot; + new String(msg.getBody())); &#125; return ConsumeOrderlyStatus.SUCCESS; &#125; &#125;); 使用MessageListenerOrderly接口,在我的测试中,是按照全局有序来消费的,也就是生产者如何生产的,消费者就是如何消费的,这个和官方文档描述的只有一个分区时的全局有序有所出入.","categories":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"}],"author":"John Doe"},{"title":"运营系统介绍","slug":"运营系统介绍","date":"2021-06-13T16:26:00.000Z","updated":"2021-06-13T23:45:47.126Z","comments":true,"path":"2021/06/14/运营系统介绍/","link":"","permalink":"http://example.com/2021/06/14/%E8%BF%90%E8%90%A5%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"运营系统介绍每个公司可能会有这样一个系统,它负责给公司的用户发布一些营销信息,比如通过短信的方式或者通过APP消息的方式来触达用户.那么首先应该要有一个用户标签的概念,用户标签指的是用户的特征,比如用户在最近30天内购买过一个理财产品,通过一个或多个特征,我们可以从一个用户库里面筛选出一部分人出来,我们称它为客群.有了客群,我们还需要营销的信息,比如给用户发短信,发优惠券等各种各样能触达到用户的方式,这类信息叫做触点.最后一点是我们将在什么时候触达用户,这个叫做执行时间.总结起来,运营系统最主要的职责便是在某个时间,向特定的用户发布特定的信息.除此之外,在某一次任务执行完成之后,我们还需要追踪任务的执行情况,以获取最终的运营效果. 功能点当然,每个公司的业务不一样,可能运营系统还会承担其它的一些职责,现在介绍一下我之前参与开发的运营系统的功能.如图: 基础功能这一部分主要是建立用户-角色体系,运营系统主要针对的用户不是大众用户,而是服务于公司内部的运营人员.主要由以下几点组成: 用户 角色 权限 菜单 定时任务 在运营系统中,首先需要有用户来操作,接着会给予用户某个角色,通过角色控制用户的权限,比如用户可以看到什么菜单,做什么操作(菜单权限和接口权限).另外,定时任务主要给开发人员配置定时任务,在某个时间点执行某个任务. 基础配置这一部分主要是服务于任务执行的信息,比如客群和标签,这一部分主要对接公司的大数据部门.当然短信配置和消息配置其实也算是基础配置,但从功能点来分的话,就没把它们放到基础配置里面了.另外运营系统还服务于另外一个广告系统,所以这部分还有其它一些配置功能.主要功能如下: 任务执行相关: 标签:一个表达式,标识用户的某个特征. 客群:标签的组合,最终筛选出一批用户. 广告系统相关: 运营位:运营位对应APP上的某个广告位. 素材:素材也就是广告的内容,一般为图片或者文案. 推广计划:也就是广告,其中会指定广告位置、素材、客群,同时还有其它一些配置,比如推广的时间段,比如指定用户的操作系统类型(只给安卓用户下发某个广告). ABTest:主要用户建立实验,进行广告策略的分流. 账户:创建推广计划时,还需要指定账户,账户会充值一定的金额.当一条广告下发时,将消耗这个账户的一部分金额. 核心功能短信任务和息任务是系统的核心,以这两个触点触达用户.素材自动化也是一个比较核心的功能,以自动化的方式来生成素材,能提升素材的生成效率. 短信任务:在指定时间,向客群发送短信. 消息任务:在指定时间,向客群发送消息. 素材自动化:主要用于自动化生成素材.运营人员自己创建一张图片的成本比较高,如果可以根据用户上传图片的模板信息来组合成一系列的素材,那么素材创建将会更加的高效. 扩展功能这一部分是由其他部分产生的扩展功能. 任务执行信息:任务执行过程的信息:比如调用短信接口的次数,成功数以及失败数. 任务监控:主要用于监控任务的执行情况,当任务执行满足一定的条件时(比如任务开始5分钟之内没有执行成功的记录),将会进行任务预警. 任务预警:主要通过短信和邮件的方式告知开发人员任务执行异常. 定时清理任务:将超过一定时间点的数据清除(比如半年). 报表订阅:这个主要和报表功能相关,当用户订阅报表时,将通过邮件发送给用户. 效果数据这一部分主要用于追踪短信和消息以及广告的执行情况,但是这类数据有个很明显的特点,它们不是实时的产生效果数据的,比如一条短信下发之后,可能由于短信运营商或者用户原因(比如停机)导致无法接收到短信,那么就需要追踪一个时间段发送出去的短信,那么就有个问题:通过什么方式来追踪呢? 在我们的系统中,创建一个短信任务时,将同时为每个短信内容创建一个模板id(通过对接短信平台),也就是短信的唯一标识.发送短信时,只要通过这个标识调用短信平台接口就可以了.除此之外,短信的发送状态一般由短信平台同步到大数据的Hive库中,运营平台需要每天去拉取某个时间段的短信状态信息(比如截止到短信发送后一周,仍然要拉取短信状态信息). 消息任务的效果追踪和短信是类似的,至于广告,主要对Hive库的广告下发表、广告展示表还有广告点击表的数据进行统计. 短信任务报表:展示短信最终的效果信息,比如到达量,发送成功量,失败量. 消息任务报表:展示消息最终的效果信息,比如到达量,发送成功量,失败量. 广告报表.展示广告下发量、浏览量、点击量等信息. 形成闭环从用户创建任务,到任务执行,再到查看营销结果,整个系统形成了一个闭环,从上面的图中自上而下也能很明显得看出这一点. 技术栈在我参与开发的运营系统中,主要用到的技术如下图: 数据库系统主要分为四个模块: web模块 配置模块 数据模块 任务执行模块 根据模块的职责,各个模块的表情况如下图: 流程图运营系统其实大部分流程并不复杂,这里简单的描述一下短信任务的一个流程,复杂性在于要把系统的各个模块串联起来.主要流程描述如下: 短信任务执行之前,会有短信模板id的生成(在创建任务的时候会生成,还会有定时任务每天重新生成新的模板id) 拉取任务对应的客群,调用短信接口发送短信 在任务执行过程中,会记录任务执行的情况(任务发送了多少条短信,调用短信发送接口成功数和失败数). 每日的短信任务报表的拉取,以此来查看短信最终的触达情况. 短信任务流程图: 再详细一点,短信任务的执行流程图: 总结这篇文章大致介绍了之前参与开发的运营系统的主要功能,不同的运营系统的设计可能多种多样,需要开发人员发挥想象力,创造更有价值的功能.","categories":[],"tags":[],"author":"John Doe"},{"title":"基于注解的动态数据源实现","slug":"基于注解的动态数据源实现","date":"2021-06-10T10:45:00.000Z","updated":"2021-06-16T05:34:11.795Z","comments":true,"path":"2021/06/10/基于注解的动态数据源实现/","link":"","permalink":"http://example.com/2021/06/10/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E7%9A%84%E5%8A%A8%E6%80%81%E6%95%B0%E6%8D%AE%E6%BA%90%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"基于注解的动态数据源实现需求有些项目不只访问一个数据库,可能需要访问多个数据库,那么就会有一个问题,怎么进行数据源的切换. 动态数据源解决这个需求的一个常见解决方案是使用动态数据源.下面将按部就班的来介绍一下如何实现基于注解的动态数据源.完整的代码请参考https://github.com/CodeShowZz/data-source/tree/master/dynamic-data-source. 第一步:配置数据源将项目中需要使用的数据源放到一个配置文件中,比如叫做jdbc.properties,在我的例子中,我有两个数据源,一个是learning库,另外一个是test库. 数据库配置文件: 123456789spring.datasource.test.driver-class-name=com.mysql.jdbc.Driverspring.datasource.test.jdbc-url=jdbc:mysql://localhost:3306/test?useSSL=false&amp;serverTimezone=GMT%2B8&amp;characterEncoding=UTF-8spring.datasource.test.username=rootspring.datasource.test.password=123456spring.datasource.learning.driver-class-name=com.mysql.jdbc.Driverspring.datasource.learning.jdbc-url=jdbc:mysql://localhost:3306/learning?useSSL=false&amp;serverTimezone=GMT%2B8&amp;characterEncoding=UTF-8spring.datasource.learning.username=rootspring.datasource.learning.password=123456 数据源常量类: 123456public class DataSourceConstants &#123; public static final String DB_LEARNING = &quot;learning&quot;; public static final String DB_TEST= &quot;test&quot;;&#125; 动态数据源类: 1234567public class DynamicDataSource extends AbstractRoutingDataSource &#123; @Override protected Object determineCurrentLookupKey() &#123; return DynamicDataSourceContextHolder.getContextKey(); &#125;&#125; 这里使用了一个DynamicDataSourceContextHolder类,将在下面进行讲解. 数据源配置: 123456789101112131415161718192021222324252627282930@EnableAutoConfiguration(exclude = &#123;DataSourceAutoConfiguration.class&#125;)@Configuration@PropertySource(&quot;classpath:jdbc.properties&quot;)@MapperScan(basePackages = &quot;com.dynamic.datasource.dao&quot;)public class DynamicDataSourceConfig &#123; @Bean(DataSourceConstants.DB_LEARNING) @ConfigurationProperties(prefix = &quot;spring.datasource.learning&quot;) public DataSource learningDataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(DataSourceConstants.DB_TEST) @ConfigurationProperties(prefix = &quot;spring.datasource.test&quot;) public DataSource testDataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean @Primary public DataSource dynamicDataSource() &#123; Map&lt;Object, Object&gt; dataSourceMap = new HashMap(2); dataSourceMap.put(DataSourceConstants.DB_LEARNING, learningDataSource()); dataSourceMap.put(DataSourceConstants.DB_TEST, testDataSource()); DynamicDataSource dynamicDataSource = new DynamicDataSource(); dynamicDataSource.setTargetDataSources(dataSourceMap); dynamicDataSource.setDefaultTargetDataSource(testDataSource()); return dynamicDataSource; &#125;&#125; 在这里讲一下具体的原理,首先我们定义了两个数据源,然后在dynamicDataSource方法中定义了一个Map,将两个数据源以(名称,数据源)的形式放入.接着调用setTargetDataSources将Map设置进去,并通过setDefaultTargetDataSource设置了默认数据源.在每次执行sql语句时,将通过DynamicDataSource类实现的determineCurrentLookupKey方法返回的key从Map中找到对应的数据源,如果没有找到,将使用默认数据源. 了解了这个原理,那么改变determineCurrentLookupKey方法返回的key就可以实现数据源的切换,那如何改造这个方法使得可以动态切换数据源呢?通常来说,会将它放在ThreadLocal中. 第二步:引入ThreadLocal定义ThreadLocal对象: 123456789101112131415161718192021222324252627public class DynamicDataSourceContextHolder &#123; /** * 动态数据源名称上下文 */ private static final ThreadLocal&lt;String&gt; DATASOURCE_CONTEXT_KEY_HOLDER = new ThreadLocal&lt;&gt;(); /** * 设置/切换数据源 */ public static void setContextKey(String key)&#123; DATASOURCE_CONTEXT_KEY_HOLDER.set(key); &#125; /** * 获取数据源名称 */ public static String getContextKey()&#123; String key = DATASOURCE_CONTEXT_KEY_HOLDER.get(); return key == null? DataSourceConstants.DB_TEST:key; &#125; /** * 删除当前数据源 */ public static void removeContextKey()&#123; DATASOURCE_CONTEXT_KEY_HOLDER.remove(); &#125;&#125; 很清晰可以看到上面通过ThreadLocal来动态的修改数据源对应的key值,以此来决定某次数据库操作使用的是哪个数据源.至此,一个简单的动态数据源实现就搞定了,接下来可以测试一下. 第三步:测试1234567891011@Testpublic void testDynamicDataSource() &#123; Student student = studentDao.queryById(1); System.out.println(student); DynamicDataSourceContextHolder.setContextKey(DataSourceConstants.DB_LEARNING); System.out.println(userDao.selectById(1)); DynamicDataSourceContextHolder.removeContextKey(); DynamicDataSourceContextHolder.setContextKey(DataSourceConstants.DB_TEST); System.out.println(studentDao.queryById(1)); DynamicDataSourceContextHolder.removeContextKey();&#125; 这样,就可以实现动态数据源了,但是可以很清楚的看到,我们需要在做数据库操作时设置ThreadLocal的值,使用后还要清除值,如果能够尽可能消除这种样板代码就更好了.我们可以引入AOP,并自定义注解来做这件事. 第四步:引入AOP注解: 12345678@Target(&#123;ElementType.METHOD,ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface DS &#123; /** * 数据源名称 */ String value() default DataSourceConstants.DB_TEST;&#125; AOP: 12345678910111213141516171819202122232425262728293031323334@Aspect@Componentpublic class DynamicDataSourceAspect &#123; @Pointcut(&quot;@annotation(com.dynamic.datasource.annotation.DS)&quot;) public void dataSourcePointCut() &#123; &#125; @Around(&quot;dataSourcePointCut()&quot;) public Object around(ProceedingJoinPoint joinPoint) throws Throwable&#123; String dsKey = getDSAnnotation(joinPoint).value(); DynamicDataSourceContextHolder.setContextKey(dsKey); try&#123; return joinPoint.proceed(); &#125;finally &#123; DynamicDataSourceContextHolder.removeContextKey(); &#125; &#125; /** * 根据类或方法获取数据源注解指定的值 */ private DS getDSAnnotation(ProceedingJoinPoint joinPoint) &#123; Class&lt;?&gt; targetClass = joinPoint.getTarget().getClass(); DS classAnnotation = targetClass.getAnnotation(DS.class); if (classAnnotation != null) &#123; return classAnnotation; &#125; MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature(); return methodSignature.getMethod().getAnnotation(DS.class); &#125;&#125; 在Dao层接口的类或方法上添加注解: 12345@Mapperpublic interface StudentDao &#123; @DS(DataSourceConstants.DB_TEST) Student queryById(Integer id);&#125; 12345@Mapper@DS(DataSourceConstants.DB_LEARNING)public interface UserDao &#123; User selectById(Integer id);&#125; 第五步:再次测试1234567@Test public void testDynamicDataSourceUseAnnotation() &#123; Student student = studentDao.queryById(1); System.out.println(student); System.out.println(userDao.selectById(1)); System.out.println(studentDao.queryById(1)); &#125; 这样基于注解的动态数据源就实现完成了.","categories":[{"name":"数据源","slug":"数据源","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%BA%90/"}],"tags":[{"name":"数据源","slug":"数据源","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%BA%90/"}],"author":"John Doe"},{"title":"对数据权限实现的思考","slug":"对数据权限实现的思考","date":"2021-06-09T18:58:00.000Z","updated":"2021-06-10T19:27:28.730Z","comments":true,"path":"2021/06/10/对数据权限实现的思考/","link":"","permalink":"http://example.com/2021/06/10/%E5%AF%B9%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%80%9D%E8%80%83/","excerpt":"","text":"对数据权限实现的思考背景之前在公司做项目的时候,有这么一个需求,需要在现有的系统的某些表上加上数据权限功能.每个用户都有一个角色id,现在要使得相同的角色id可以查询相同的数据,超级管理员的id为1,这个角色可以查询所有的数据. 从简单做起对数据库的操作无非就是CRUD,既然要实现数据权限,那么最简单的方式就是在表里面新增一个roleId字段,然后把它当成一个业务字段来做CRUD,这样非常简单.但是这样会使人有点烦躁,为了实现这个功能,就要将该字段不断的在系统间传递,这样是不是挺麻烦的? 使roleId传递更方便我们的系统有一个和前端打交道的web模块,web模块将调用其余业务模块来进行业务处理.我之前实现这个功能的时候,在Web层实现了一个切面,这个切面是切在对业务模块的调用上,也就是web模块的出口处,因为之前接口方法都满足一定的规范,所以切点就比较通用,然后切面所做的事情就是从Session中取出roleId,然后判断调用的方法参数中是否有roleId字段(通过反射),有则将roleId设置进去,这样roleId就可以自动的进行传递了. 在业务模块中,也实现一个切面,切点为业务模块的入口处,因为web模块已经设置了roleId,那么我们现在取roleId的目的是为了将roleId保存起来,到Dao层再对sql执行进行拦截,同样检测一下是否字段有roleId,有则将roleId设置进去.在我的实现中,我是使用ThreadLocal来保存入口处的roleId,在拦截器中从ThreadLocal中获取roleId,如果方法参数中有roleId这个字段,则设置进去. 整个系统其实有三个切面,一个是web模块的切面,一个是业务模块入口的切面,最后一个是sql拦截器(这个也当成一个切面). 说了这么多,其实无非就是解决roleId如何在系统间传递的问题.但是现在想起来,之前这么做是有点武断的.因为需要做数据权限的表也只是少数,不是所有的调用方法都应该拦截,这样可能会对系统的响应时间造成影响.所以现在想想,如果要改进这个问题,可以自定义一个注解,然后在需要获取或者设置roleId的方法上加上注解,把切点变成基于注解的拦截.这样和直接在系统中直接传递roleId比起来,我也不知道是不是把问题搞复杂了,本来在系统间就是要传递各种数据的,大费周章搞这些切面干什么.如果再让我实现一次,我觉得还是直接在系统间传递roleId好了,除非代码太复杂,直接传递roleId不大可行,我觉得通过切面来传递才有好处. SQL改造在我的实现中,我还对查询语句做了一个sql改造,现在想起来感觉是有点画蛇添足了,但是还是想表达表达.假如有这么一条sql:select id,name from student where id = #&#123;id&#125;,假如在不修改这句sql的前提下,如何将其改成是带有roleId的查询呢?思路是这样的,用拦截器拦截原sql,在外面再嵌套一层sql,形如select * from (原sql) where roleId = #&#123;roleId&#125;,当然原sql的查询字段上要包含roleId字段,这样的话对于大多数sql都是通用的.但是为什么不直接在sql语句中(用的是mybatis)直接加上roleId = #&#123;roleId&#125;呢?因为之前一个表里面可能有多个sql查询语句,且查询的字段使用了mybatis的&lt;sql&gt;&lt;/sql&gt;和&lt;include&gt;&lt;/include&gt;来复用,所以只要在&lt;sql&gt;&lt;/sql&gt;间再加入一个roleId字段,就可以通过上面的方式来简单的改造SQL语句了.现在想想,除非对某个表的查询语句太多(一般不会),否则直接在原sql上加入查询条件或许更好,首先第一点改造的只是查询,其它的数据库操作还是要对原sql进行改造,并且改造查询给系统也带来了复杂性,可能这种改造对复杂sql还不一定适用. 流程在这里总结一下整个流程: 从web层的session中获取roleId,设置到web层调用业务层的对象上(如果有roleId) 获取业务层入口方法的方法参数,将roleId设置在ThreadLocal中. 数据库操作时,使用拦截器将ThreadLocal中的roleId设置到方法参数上. 如果是查询操作,拦截器改造原sql. 这里还要强调一下,在我的实现中,查询在原sql的查询字段上加入roleId即可,其余操作仍然需要修改sql语句. 另外,在三个切面中,可以选择通过规范的方法名称来实现切点,也可以使用基于注解的方式来实现切点,前者比较通用但多了很多不必要的判断,后者需要我们指定拦截的地方,少了不必要的判断却增加了一些开发量. 形成组件在我的实现中,因为关于roleId的切面、注解、拦截器是通用的,所以我将它们全部都放在了一个新的项目中,形成一个组件,然后让业务模块依赖它. Talk is cheap,show me the code俗话说得好,没有代码你说个XX.之前在公司做这个功能的时候,可能一股脑就想着简化,现在想起来觉得不能为了简化而简化,现在我觉得这个数据权限的思路可以记录下来,但是要让我再实现它一次觉得已经没有动力了,所以就不再想写代码了.(好吧,我承认再实现一遍有点折磨人😭)","categories":[{"name":"数据权限","slug":"数据权限","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"}],"tags":[{"name":" 数据权限","slug":"数据权限","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"}],"author":"John Doe"},{"title":"ABTest分流算法设计与实现","slug":"ABTest分流算法设计","date":"2021-06-08T14:14:00.000Z","updated":"2021-06-16T05:35:12.528Z","comments":true,"path":"2021/06/08/ABTest分流算法设计/","link":"","permalink":"http://example.com/2021/06/08/ABTest%E5%88%86%E6%B5%81%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"ABTest分流算法设计与实现需求有这么一个需求,我们需要建立一个实验,实验有一个实验名称.实验下面有多个分组,每个分组也有个分组名称.当我们创建一个实验的时候,需要同时建立一个或多个分组,并且每个分组都有一个百分比的属性,代表当我们进入一个实验的时候,选择某个分组的可能性有多大,所有的分组的百分比之和为100%. 目的通过实现实验和分组,我们可以把它应用在分流策略中,比如ABTest.考虑这样一个场景,我们要对比三个推荐算法带来的效益,此时我们会给这三个推荐算法分配一定的比例,然后让每次推荐都按这个比例来执行不同的算法,最终再根据一定的换算来统计算法带来的效益. 实现思路我们可以将分组的百分比看成是几条线段,假如总共有100米长,每条线段有一定的长度,我们根据标识(比如hashCode)来对100取模,最终这个数字一定会落在某一条线段上,也就是某个分组上.在我的实现中,将以1000作为模.在这里,我将使用Java语言实现一个简单的分流算法. 完整代码地址为:https://github.com/CodeShowZz/abtest,所有的接口都进行了初步的测试. 步骤一:定义模型123456789public class Lab &#123; private String key; private String name; private List&lt;Group&gt; groupList;&#125; 12345678910111213141516171819202122public class Group &#123; private String key; private String name; /** * 百分比 **/ private BigDecimal ratio; /** * 分组的开始位置 */ private int start; /** * 分组的结束位置 */ private int end;&#125; 步骤二:实现分流工具类这里要实现一个工具类,能够将百分比转换成区间. 12345678910111213public static void assignRangeByRatio(Lab lab) &#123; List&lt;Group&gt; groupList = lab.getGroupList(); int current = 0; for (Group group : groupList) &#123; BigDecimal ratio = group.getRatio(); int count = ratio.multiply(range).setScale(0, RoundingMode.HALF_UP).intValue(); int start = current; int end = current + count - 1; group.setStart(start); group.setEnd(end); current = end + 1; &#125; &#125; 步骤三:获取标识取Hash,将其分配到某个分组中.在我的实现中,如果两次传入的标识key是一样的,那么计算出来的分组位置也是一样的.所以使用这个Hash算法时,可能要根据具体的应用场景来取一个具体的key,比如对于一个用户来说,取值如果要和上次相同,那么可以使用用户id来作为key,如果取值要随机,那么可以取时间戳或者其它属性作为key. 这里我的Hash算法借鉴(可以说是照抄)了HashMap中的Hash算法. 123public static final int hashCode(String key, String value) &#123; return Math.abs(Objects.hashCode(key) ^ Objects.hashCode(value)); &#125; 接着,使用上面的模型并且结合Hash算法来实现分组.分区函数的返回结果就是某一个分组. 1234567891011public static Group partition(String key, Lab lab) &#123; int hashCode = hashCode(key, lab.getName()); int position = hashCode % range.intValue(); List&lt;Group&gt; groupList = lab.getGroupList(); for (Group group : groupList) &#123; if (group.getStart() &lt;= position &amp;&amp; group.getEnd() &gt;= position) &#123; return group; &#125; &#125; return null;&#125; 步骤四:测试通过上面的三个步骤,一个简单的分流算法实现完成.现在我们来假设这样一个场景:据统计,50%的人喜欢数学,30%的人喜欢语文,20%的人喜欢英语,那么现在我们随便找一个人,来猜测它喜欢哪个科目,那么我们就可以使用上面的程序,测试程序如下. 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; Lab subject = new Lab(); Group math = new Group(); math.setRatio(new BigDecimal(0.5)); math.setKey(&quot;math&quot;); math.setName(&quot;数学&quot;); Group chinese = new Group(); chinese.setRatio(new BigDecimal(0.3)); chinese.setKey(&quot;chinese&quot;); chinese.setName(&quot;语文&quot;); Group english = new Group(); english.setRatio(new BigDecimal(0.2)); english.setKey(&quot;english&quot;); english.setName(&quot;英语&quot;); List&lt;Group&gt; groupList = Arrays.asList(math, chinese, english); subject.setGroupList(groupList); subject.setKey(&quot;subject&quot;); subject.setName(&quot;学科&quot;); SplitFlowUtil.splitFlow(subject); Group res = partition(&quot;the boy maybe like math&quot;, subject); System.out.println(res); res = partition(&quot;i am a programmer&quot;, subject); System.out.println(res); &#125; 继续思考很明显,上面的这个程序其实算是一个通用程序,如果设计的算法更加的快捷,API接口更加易用,它完全可以作为一个公司内部的服务来提供给别人调用.所以现在我们要思考如何将它改进成一个公司内部可以使用的程序. 改进一:建立微服务提供添加、更新、删除、查询、分流五个接口来对实验进行操作,在这里使用Restful接口来提供这项服务. 改进二:将模型数据存储到Mysql上面的测试程序只是在本地构造程序,我们可以将模型数据映射成表,然后存储到数据库中,然后通过UI界面来进行CRUD,这一点很容易就可以做到,不再赘述. 改进三:引入Zookeeper很明显,既然要在公司内部使用,那么要保证每个实验都是隶属于某个项目的,首先要保证实验的唯一性,而这又能看出有很明显的层级结构,所以可以引入Zookeeper来存储这些模型数据,而上面的Mysql则用于冗余模型数据. 开始第一次改进先列一下实现上述改进所要引入的一些技术,其中改进二不在本次实现考虑范围.另外在下文中可能不会提供所有的代码,完整的代码将在最后给出Github仓库地址. 序列化框架:Kryo,用于序列化模型数据并将其存储到Zookeeper上 微服务框架:Spring Boot,用于实现一个微服务并提供Restful接口 分布式协调框架:Zookeeper及其API,用于实现模型数据的保存,并形成目录结构. Zookeeper在工程中实现对zookeeper api的调用,主要考虑的操作有4种 节点增加 节点更新 节点删除 节点数据查询 出于更新的复杂性,调用方可能修改实验名称、分组名称以及分组的属性,所以在真正实现中,将使用节点删除加上节点增加来实现节点更新.在这里将不会讨论Zookeeper API的细节,假设读者已经对此有一定的了解和经验. Spring Boot构建一个Spring Boot服务是非常简单的,和上述的Zookeeper类似,我们将对外提供几个api供外部接口调用. 实验创建 实验更新 实验删除 实验查询 根据实验名称进行分流 API如下所示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 创建实验 * @param lab * @return */ @RequestMapping(value = &quot;/create&quot;, method = RequestMethod.POST) public boolean create(@RequestBody Lab lab) &#123; return labService.create(lab); &#125; /** * 根据projectKey和labKey删除实验 * @param lab * @return */ @RequestMapping(value = &quot;/delete&quot;, method = RequestMethod.POST) public boolean delete(@RequestBody Lab lab) &#123; return labService.delete(lab); &#125; /** * 根据projectKey和labKey查询实验下的分组 * @param projectKey * @param labKey * @return */ @RequestMapping(value = &quot;/query&quot;, method = RequestMethod.GET) public List&lt;Group&gt; query(@RequestParam String projectKey, @RequestParam String labKey) &#123; return labService.query(projectKey,labKey); &#125; /** * 根据projectKey和labKey还有identify来进行分流 得到某个分组 * @param projectKey * @param labKey * @param identify * @return */ @RequestMapping(value = &quot;/partition&quot;, method = RequestMethod.GET) public Group partition(@RequestParam String projectKey, @RequestParam String labKey,@RequestParam String identify) &#123; return labService.partition(projectKey,labKey,identify); &#125; /** * 更新实验 * @param lab * @return */ @RequestMapping(value = &quot;update&quot;,method = RequestMethod.POST) public boolean update(@RequestBody Lab lab) &#123; return labService.update(lab); &#125; 我在模型数据中又引入了几个参数: 1234567891011121314/** * 分流需要的参数,由调用方传入,调用方决定分流所使用的标识 */ private String identify; /** * 某个项目的标识,在ZK中是第一级目录,以来区分各个项目的实验 */ private String projectKey; /** * 在进行更新时,需要传入变更前的实验分组key,以便于删除原来的实验 */ private String oldKey; 这几个参数的作用已经通过注释来表达,这样可以使得服务更加通用和简单. 测试实现完上述的两个改进之后,我们就可以打开PostMan或者其它Http请求工具来对我们提供的接口进行测试了,这里对如何测试不进行展开. 总结在这篇文章中,介绍了一个简单的分流算法的设计以及实现,当然程序还存在很多不足之处,比如异常处理,参数校验,又或者是无法实现多层的分流,这都是值得改进的地方,希望以后有机会再进行改进(程序员经常说的一句话就是下次一定😄).","categories":[{"name":"abtest","slug":"abtest","permalink":"http://example.com/categories/abtest/"}],"tags":[{"name":"abtest","slug":"abtest","permalink":"http://example.com/tags/abtest/"}],"author":"John Doe"},{"title":"Kafka如何保证发送消息有序","slug":"Kafka如何保证发送消息有序","date":"2021-06-07T17:33:00.000Z","updated":"2021-06-07T17:46:56.766Z","comments":true,"path":"2021/06/08/Kafka如何保证发送消息有序/","link":"","permalink":"http://example.com/2021/06/08/Kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F/","excerpt":"","text":"Kafka如何保证发送消息有序生产者有一个配置项叫做max.in.flight.requests.per.connection,这个参数能够保证生产者在收到服务器响应前能发送多少个消息.举个例子,如果我们把这个参数设置为1,那么当生产者发送了一个消息给服务器之后,只能等到服务器响应生产者发送的这个消息之后,生产者才能够继续向服务器发送消息. 优点和缺点优点:将max.in.flight.requests.per.connection配置为1后,能够保证消息的有序性. 缺点:缺点也很明显,因为必须要等待服务器响应才能够发送消息,那么就会降低生产者发送消息的吞吐量,除非有严格的顺序要求,才会将这个选项配置为1.","categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"author":"John Doe"},{"title":"Kafka Producer三种使用方式","slug":"Kafka-Producer三种使用方式","date":"2021-05-31T20:32:00.000Z","updated":"2021-06-16T05:33:30.544Z","comments":true,"path":"2021/06/01/Kafka-Producer三种使用方式/","link":"","permalink":"http://example.com/2021/06/01/Kafka-Producer%E4%B8%89%E7%A7%8D%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F/","excerpt":"","text":"Kafka Producer三种发送方式Kafka生产者使用起来非常简单,它主要有三种发送方式,分别是 Fire And Forget(发送但不等待结果) 同步发送,等待结果返回 异步发送,结果通过回调接口接收 这里,通过编写一个程序来展示上述的三种方式,如无环境,请参考文档自行搭建,这里就不阐述了. 具体的细节可以参考Kafka API,这里主要要表达的是三种使用方式,完整的代码参见Github仓库 Fire And Forget1234public void fireAndForget() &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic&quot;, &quot;key&quot;, &quot;value&quot;); producer.send(record);&#125; 同步发送123456789public void sendMsgSync() &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic&quot;, &quot;key1&quot;, &quot;value1&quot;); try &#123; RecordMetadata recordMetadata = (RecordMetadata) producer.send(record).get(); System.out.println(&quot;send sync return:&quot; + recordMetadata); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 异步发送123456789101112131415public void sendMsgAsync() &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic&quot;, &quot;key2&quot;, &quot;value2&quot;); try &#123; producer.send(record, new Callback() &#123; public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if(e != null) &#123; e.printStackTrace(); &#125; System.out.println(&quot;send async return:&quot; + recordMetadata); &#125; &#125;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;","categories":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"author":"huangjunlin"},{"title":"Elastic Search安装","slug":"ElasticSearch安装","date":"2021-05-30T16:56:00.000Z","updated":"2021-06-13T23:30:43.627Z","comments":true,"path":"2021/05/31/ElasticSearch安装/","link":"","permalink":"http://example.com/2021/05/31/ElasticSearch%E5%AE%89%E8%A3%85/","excerpt":"","text":"Elastic Search安装俗话说:工欲善其事,必先利其器.本篇文章就来介绍一下如何安装ES,并启动它来实现万里长征的第一步.此次演示的是在Mac系统上安装ES. 首先要保证拥有Java环境,版本为jdk8以上. 其次,找一个目录,执行如下命令curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.16.tar.gz来下载5.6.16版本的ES. 接着执行tar -xvf elasticsearch-5.6.16.tar.gz将下载的文件解压成文件夹.最终的效果如下截图所示. 接着执行cd elasticsearch-5.6.16/bin,运行./elasticsearch,可以看到启动后的效果,如下截图 到此,大功告成!","categories":[{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/categories/elastic-search/"}],"tags":[{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/tags/elastic-search/"}],"author":"huangjunlin"},{"title":"一个key值如何在redis集群中找到存储在哪里","slug":"一个key值如何在redis集群中找到存储在哪里","date":"2021-05-18T02:34:00.000Z","updated":"2021-05-31T20:50:17.294Z","comments":true,"path":"2021/05/18/一个key值如何在redis集群中找到存储在哪里/","link":"","permalink":"http://example.com/2021/05/18/%E4%B8%80%E4%B8%AAkey%E5%80%BC%E5%A6%82%E4%BD%95%E5%9C%A8redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%89%BE%E5%88%B0%E5%AD%98%E5%82%A8%E5%9C%A8%E5%93%AA%E9%87%8C/","excerpt":"","text":"一个key值如何在redis集群中找到存储在哪里首先,一个集群只有在16384个槽全部都分配给了集群下的节点时,集群才能处于正常运行状态.现在假设我们集群中有三台机器,代号分别是7001,7002,7003,并且假设槽的分配关系如下: 7001负责处理0-10000 7002负责处理10001-13000 7003负责处理13001-16383 比如我们向7001执行一条set msg hello命令,这个时候7001所在的节点会调用函数CRC16(key) &amp; 16383,这个函数用于计算key的CRC-16校验和,然后对槽号取模,最终计算出分配给哪个槽.这个时候有两种情况,要么计算出来的槽号由自己负责,要么由其它节点负责.如果是由自己负责,那么直接执行命令,返回结果就完事了.但是如果是由别的节点负责,那么会向客户端返回MOVED错误,形式为MOVED 槽号 实际处理槽节点的ip:实际处理槽的节点port,指引客户端往真正处理槽的节点去发送命令. 另外还要注意的一点是,除了上述的两种情况外,其实还会有另外一种情况.考虑这样一种场景,当集群里面加入了新节点7004,并且将7001负责的0-5000分配给了7004,那么这个时候会执行重新分片操作,在这个过程中,可能存在一种情况就是槽号在分片的过程中,槽里面的键值对并没有全部分配到新的节点中,而是部分分配到新的节点中.举个例子:比如原本由7001负责的槽5000里有两个键值对,分别是(key1,value1),(key2,value2),假设(key1,value1)已经分配给了新的节点7004.这个时候,当我们向7001发送命令get key1的时候,7001是无法获取到值的,但是它计算出来的槽号确实是属于它自己管理的槽号,因为这个时候槽中的键值对并没有完全转移完成,所以槽的归属权还是自己的,只有当槽完全转移完成,槽才会属于7004.在这种情况下,7001是无法获取到值的,所以7001会再判断是否槽处于迁移状态,如果是的话,会返回一个ASK错误,形式为ASK 槽号 实际拥有键值对的节点的ip:实际拥有键值对的节点的port,让客户端向真正拥有该键值对的节点去发送请求.","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}],"author":"huangjunlin"}],"categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/categories/kafka/"},{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"},{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/categories/zookeeper/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"其它","slug":"其它","permalink":"http://example.com/categories/%E5%85%B6%E5%AE%83/"},{"name":"jvm","slug":"jvm","permalink":"http://example.com/categories/jvm/"},{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/categories/rocketmq/"},{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/categories/spring-cloud/"},{"name":"数据源","slug":"数据源","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%BA%90/"},{"name":"数据权限","slug":"数据权限","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"},{"name":"abtest","slug":"abtest","permalink":"http://example.com/categories/abtest/"},{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/categories/elastic-search/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"并发","slug":"并发","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91/"},{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://example.com/tags/zookeeper/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"其它","slug":"其它","permalink":"http://example.com/tags/%E5%85%B6%E5%AE%83/"},{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"},{"name":"rocketmq","slug":"rocketmq","permalink":"http://example.com/tags/rocketmq/"},{"name":"spring cloud","slug":"spring-cloud","permalink":"http://example.com/tags/spring-cloud/"},{"name":"数据源","slug":"数据源","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%BA%90/"},{"name":" 数据权限","slug":"数据权限","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/"},{"name":"abtest","slug":"abtest","permalink":"http://example.com/tags/abtest/"},{"name":"elastic search","slug":"elastic-search","permalink":"http://example.com/tags/elastic-search/"}]}